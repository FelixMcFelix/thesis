\chapter{Conclusion}\label{chap:conclusion}
This thesis has made online \gls{acr:rl} in \gls{acr:pdp} hardware possible, while also meaningfully advancing the state of the art in data-driven, programmable networks.
%?? collected literature
%
%?? Say all we've shown
%?? This thesis has... (keep short)
%?? then subsections to discuss broad themes
Through these advances and thorough review of the literature, I have given substantial evidence for the value and viability of \gls{acr:pdp} networks empowered by data-driven methods, satisfying claims \stmtno{\numrange{0}{3}}.
Recalling the initial thesis statement:
\begin{quotation}
	\noindent
	\recalltext{stmt}
\end{quotation}
The value of each of these tools on their own for network management (\stmtno{0}) has been shown mostly by the use cases considered throughout \cref{chap:nets,chap:ddn}, but is also implicitly shown through the novel techniques presented in the thesis's main developments.
\gls{acr:rl}'s value in particular was shown by its use to learn to mitigate \gls{acr:ddos} attacks in an online way (\stmtno{1}).
By adapting classical \gls{acr:rl} policy formats and algorithm choices to suit the design, threading model, and \glspl{acr:fu} of manycore SmartNIC hardware, online in-\gls{acr:nic} \gls{acr:rl} was made possible.
Eliminating \gls{acr:pcie} and host stack overheads offered substantial latency benefits, and the parallelised Sarsa algorithm brought higher-throughput online learning (\stmtno{2}).
In-network aggregation of per-packet statistics to histograms made the handling of high-rate timestamps feasible, and enabled a flow classification task with clear operational benefits (\stmtno{3}).

What is far more interesting, however, are the wider takeaways and lessons learnt in the development of this work, and by collecting together a wide family of solutions falling under the \gls{acr:ddn} or \gls{acr:pdp} umbrella.
Each has its own impact on the design and deployment of the other.
Equally, it's worth mulling over the horizon of networking capabilities and form factors in the short- and long-term.
%Synthesis of implications, takeaways on design, deployment...
%?? which I will try to opine on here.

\section{The need for co-design}
%?? notes from cycle in
Making use of \gls{acr:ddn} and \gls{acr:pdp}-accelerated solutions is, as this thesis has likely demonstrated, an involved process.
In the \gls{acr:ddn} case, `zero-touch' deployment and development are likely impossible.
While we can train successful policies, \gls{acr:ddn} cannot itself derive the \emph{mechanisms} of control: action models, reward functions, and the state which they should operate on.
Learnt policies and parameters operate as well as they can \emph{within the framework we give them}, and generally succeed at so doing.
Yet as we've seen already, by designing \gls{acr:ddn} solutions without deep, cross-disciplinary human expertise on the controlled system we can easily introduce catastrophic impact in critical scenarios.
This extends even to testing and training environments; capturing every real interaction is crucial if one has any hope of generalising to production networks.
At the compute scales of interest e.g., small and fast models with lower sample complexity, factoring in human expertise ahead of time can be useful in accelerating inference and training as opposed to completely `clean-slate' approaches.
There is an argument to be made about to what degree we \emph{should} be integrating our own intuition---biasing models away from potentially better solutions, i.e., that reward is all you need~\parencite{DBLP:journals/ai/SilverSPS21}---but we must often perform our own feature engineering regardless.
Temporal properties of state are one such example: we \emph{could} make use of \glspl{acr:lstm} and similar constructs to capture them automatically, but the price paid in complexity is less than appealing.
%Moreover, temporal properties can't be derived by fully-connected layers, requiring the 
%
%?? feature engineering needed: while nns can do it themselves in first layers (and outdo humans, again), this is wasted time to repeat and ?? 
%
%and at scales of interest (i.e., small model capacity) need to integrate human expertise -- even if ``reward is all you need''~\parencite{DBLP:journals/ai/SilverSPS21}, still need to design actions.
%?? argument to be made about to what degree we \emph{should} be integrating our own intuition (biasing models away from potentially better solns), i.e., that ``reward is all you need''~\parencite{DBLP:journals/ai/SilverSPS21}

Easily taking advantage of \gls{acr:pdp} hardware requires less thought.
Automatic offload tools are already very promising for (mostly) cutting host machines out of the packet processing loop, and extracting data and pipeline parallelism when they cannot.
I expect this tooling will only improve further.
For novel in-network compute uses or latency-optimal solutions, I have less confidence.
Today's---and likely tomorrow's---diversity of device designs and programming models is a blessing and a curse.
It is necessary that we have such variation to achieve a balance of performance, price, and capability across market silicon.
The downside is that this lack of unity demands insight and expertise on the devices themselves, rather than a single network-compute model, but making best use of them is intensely rewarding.
This is not the only field where integrating device parallelism is difficult.
Quoting an observation on combinatorics that aligns with the development of \approachshort:
%?? no such thing as zero-touch ddn, more likely zero-touch for pdp with offload texh. INC still needs all the thought (probably can't auto-derive many data structure transforms)
%?? ddn cannot derive mechanisms, and at scales of interest (i.e., small model capacity) need to integrate human expertise -- even if ``reward is all you need''~\parencite{DBLP:journals/ai/SilverSPS21}, still need to design actions.
%
%?? in ddn...
%?? and in INC in particular for PDPs: need to know 
%
%?? Downside? still need deep knowledge, from where I stand (though may be proven wrong) completely `clean-slate' unlikely to do great -- but we can still do better in params etc.
%
%?? Diversity of device designs and programming models requires insight, expertise... but rewarding.
%
%?? Downside:
\begin{quotation}
	\noindent
	Our experience from this thesis has been that implementing parallelism properly is hard. It requires deep and intrusive coupling with the algorithm, and can increase the amount of code needed by as much as an order of magnitude.
	
	\hfill\parencite[p.~214]{ciaran-phd}
\end{quotation}
%?? Accouing for exec model similarly intrusive: pipeline YES, data parallel YES (barring shared state), but latency is HARD
This refers to commodity multicore \gls{acr:cpu} programming, but as we've explored SmartNICs impose a similar blowup in complexity.
The intrusiveness extends in our case to the memory model, specialised capabilities like \gls{acr:tcam}-backed \glspl{acr:mat}, and missing \glspl{acr:fu} we might otherwise have taken for granted such as \glspl{acr:fpu}.
Many of the required data structure transformations cannot be automatically derived, and the best algorithms for in-network compute take advantage of tightly coupling these elements together.
One might also argue that this makes in-network solutions fragile against future hardware.
%?? This is made worse by the ... little unification? between what? Compute models?
%
%?? Heterogeneity great and useful, (necessary, some might say  to hit perf, if not price points) but also a curse: see above.
%?? Even so, needs great awareness.

%?? the ghost of hardware yet to come.
%Ooooo
%What will this mean for programmability? Abstractions?
%?? increasing fallback to IRs from commodity languages the `right move'... probably.

\section{A difficult security context}
%?? Uncomfortable security context
%?? Try to establish when good/bad, wrt opsec?
At present, \gls{acr:ml} and \glspl{acr:dnn} have a wide variety of viable attacks at runtime and during training, which raises questions about whether their use in \gls{acr:ddn} is safe---either online or offline.
Attacks and defences still appear to be rapidly iterating against one another, and as such it's not clear that we should be focussing on integrating specific defences while they appear to have such a short shelf-life.
I argue that \gls{acr:ddn} deployment can be safe so long as there is a reasonable degree of isolation between a hypothetical attacker (or self-serving client) and the model.
%?? spec. attacks: evasion, poisoning, data extraction.
What isolation means then depends on the attacks we concern ourselves with.
For instance, destructive steps in processing, true isolation as in many resource placement problems (i.e., \gls{acr:ddn} applied at design-time), or aggregation of input data may aid against evasion, poisoning and adversarial behaviour.
The network itself offers some degree of isolation of many outputs, which might make model stealing and evasion more difficult.
Transient network conditions make flow statistics noisier, pushing them away from the intended perturbation, while routing and \gls{acr:qos} decisions might only be inferred indirectly with added noise and delay.
%Hiding outputs as appropriate to prevent model stealing and similar attacks [probably can't even see most of the time, but might be able to infer approx later?]).
In closed loop circumstances, isolation is less clear.
This is purely intuitive reasoning, and likely unsatisfactory, but they are the best I can offer barring further research tailored to the network problem-space.
What the field needs instead is a set of quantitative bounds on how an attacker's input can affect learnt models in reasonable scenarios.

%\section{The ghost of hardware yet to come}
%Ooooo
%
%What will this mean for programmability? Abstractions?
%
%?? increasing fallback to IRs from commodity languages the `right move'... probably.

\section{Future directions}
%?? Drop the headings once written

%A problem we raised (without a clear solution) was the design of reward functions which do not rely upon heuristic estimates or a priori knowledge of benign traffic content.
%If true online learning is desired (i.e., coping with a non-stationary environment), then such reward functions are sorely needed.
%While $\bload{\cdot}$ is likely to be a good candidate for many deployments, we believe that finding an effective metric derived from the individual statistics we suggested serves as an interesting research problem.

%?? Benefit of the more realistic emulation environment is that it is far closer in behaviour and architecture (i.e. viable) to a real SDN-enabled deployment, captures some dynamics which were otherwise hidden/lost by human ignorance. It also allows me to develop the system towards evolving traffic models where it is expected that RL should shine over and above standard ML techniques. THEN: Room to introduce/roll-in dynamic changepoint detection or adaptive exploration \cite{DBLP:conf/ki/Tokic10, DBLP:conf/ki/TokicP11, DBLP:conf/annpr/TokicP12}?

Given that one of the advantages of \gls{acr:rl} methods is their ability to dynamically learn by trading off exploration and exploitation, precisely how well-suited they are to handling the evolution of networks is an interesting research question.
Handling non-stationary problems is \emph{possible}, but rarely recommended, particularly with \gls{acr:dnn}-based policies.
To respond to such change, we simply need to either scale up gradient contributions, or increase the strength of exploration parameters like $\epsilon$.
Yet we are left with two key challenges.
The first is that we need robust means of detecting the kinds of problem-space evolution we're interested in.
There are tricky tuning factors to consider: chief among them are handling seasonality, and the timescales and magnitudes of evolution worth adapting to.
Suppose traffic varies diurnally, in which case choosing the wrong timescales would likely cause an online learner to oscillate between policies---meanwhile, the expected behaviour would be to learn to handle these modes in the \emph{same policy}.
The techniques discussed in \cref{sec:ddn-rl-design-considerations} may be useful to this end, such as adaptive exploration, changepoint detection, or signal processing methods (whose intersection with \gls{acr:rl} seems as-yet unexplored).
The second challenge is that we must understand and model what problem-space evolution \emph{really} looks like.
While it is known that \gls{acr:ddos} attack strategies evolve in real time~\parencite{DBLP:conf/spw/KangGS16}, to my knowledge no works detail what patterns such evolution might take.
In the wider Internet, aggregate changes in bandwidth and usage are likely easy enough to model~\parencite{DBLP:conf/anrw/BauerJHBC21}.
But, barring historic case studies, estimating the effects of new protocols and \glspl{acr:cca} before their deployment is unlikely to be feasible.

%?? problems: policy-space variation, and timescales, detection. i.e., seasonality in diurnal traffic probably counts as two `learnt behaviours' to handle in the \emph{same policy}.
%?? need to understand what problem-space variation really looks like: we can do it w/ traffic scales, attack patterns and protocol evolution less so.
%?? need to quantify.
%?? scaling up of gradient contributions, or exploration params like $\epsilon$.
%it is important to propose and test sensible simulations or captures of evolving networks.
%While it is known that DDoS attack strategies evolve in real time~\parencite{DBLP:conf/spw/KangGS16}, evaluation is difficult at present since, to my knowledge, no works detail what patterns such evolution might take.
%Regardless, these scenarios present ideal circumstances to apply some of the techniques discussed in \cref{sec:ddn-rl-design-considerations}: adaptive exploration, changepoint detection, or intelligent sampling methods to judge which flows are most worthy of consideration.
%For estimating \emph{when} to explore, we believe that the intersection of  and RL is as-yet unexplored.
%?? broaden to networks in general?

%\paragraph{OPaL}

%OPaL itself... In future, we aim to examine the performance of individual applications driven by \approachshort---both classical and \gls{acr:drl}-based---and how a NetFPGA implementation can offer further latency and throughput improvements.
%?? Different problems cover different proportions of tile-coding, distribs of state accesses...

Online \gls{acr:rl} via \approachshort{} is limited to devices in a SmartNIC or \gls{acr:npu}-style form-factor.
This is less than ideal for larger deployments, yet achieving this level of flexibility at switch scale is unlikely to be possible without heavy concessions.
Register access limits and a fixed number of pipeline stages would make a purely P4-\gls{acr:pdp} variant impossible to express, let alone a \gls{acr:mat}-accelerated approach (which would be dependent on the controller for policy updates).
A promising avenue here would be to investigate ongoing transfer learning between online \approachshort{} agents and high-throughput offline function approximators such as \glspl{acr:bnn}.
This might allow, for instance, having a canonical `known good' policy in the majority of the network installed to Tofino switches, while a smaller proportion of flows or packets are routed through actively learning bump-in-the-wire nodes.
The control plane is then responsible for collating their local policy modifications and generating a set of \gls{acr:bnn} parameters which expresses the same decision boundaries as the aggregated tile-coded policy.

%\paragraph{\seidr}
%In the future, we aim to examine the use of \seidr{} towards microburst detection and diagnosis~\parencite{DBLP:conf/sigcomm/ChenFKRR18} and for the identification of \emph{BBR}-like temporal properties of emerging UDP-based congestion-aware protocols, such as \emph{QUIC}.%~\cite{DBLP:conf/sigcomm/LangleyRWVKZYKS17}.

%\paragraph{general}
While this thesis achieves online \gls{acr:rl} in \gls{acr:pdp} hardware, it does so by choosing a function approximation scheme with lower model capacity than more common alternatives such as \glspl{acr:nn}.
How could we enable online learning for these more complex approximators?
Practically speaking, minibatches and replay buffers are necessities and will require storage in high-speed memory.
This is somewhat counter to the design of \gls{acr:pdp} hardware, but it wouldn't be too onerous a requirement in bespoke designs.
Computing gradients themselves in a way which is scalable and tailored to the execution model (many weaker cores or \glspl{acr:fu}) remains a challenge.
We might find value in combining insights from the field of distributed model-training (\cref{sec:distributed-training}), such as wait-free backpropagation, to achieve low-latency forward passes and parallelised updates to the policy when using model-parallel inference.
Here though, we would constrain the scope of such algorithms to a single device, which might enable some shortcuts and further optimisations.
This continues to make use of the many cores or \glspl{acr:fu} that we might expect on SmartNICs or \gls{acr:fpga} devices---\emph{N3IC}~\parencite{DBLP:journals/corr/abs-2009-02353} offers a model-parallel \gls{acr:nfp} implementation of the \gls{acr:nn} forward pass which might be compatible in theory.
\glspl{acr:bnn} are not, however, suited for this purpose, given that training requires incremental real-valued adjustments to the model parameters.
As such, online \gls{acr:nn} training in the \gls{acr:pdp} would likely mandate \emph{at least} fixed-point arithmetic, ruling out the strong performance benefits of \glspl{acr:bnn}.

%\emph{N3IC}~\parencite{DBLP:journals/corr/abs-2009-02353} offers a model-parallel \gls{acr:nfp} implementation of the \gls{acr:nn} forward pass---this execution model 
%?? Insights from distributed model-training? Gradient calculation still tricky. Wait-free backpropagation as in \cref{sec:distributed-training} might work with non-\glspl{acr:bnn} to divide gradient compute across agents (again, exploiting multi-core or many \glspl{acr:fu} that we might get out of \glspl{acr:fpga})