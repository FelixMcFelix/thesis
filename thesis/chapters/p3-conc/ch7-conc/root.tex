\chapter{Conclusion}\label{chap:conclusion}
This thesis has shown how to use the advanced capabilities of modern \gls{acr:pdp} hardware to make online learning---particularly classical \gls{acr:rl}---and other \gls{acr:ddn} use cases feasible in the dataplane.
In spite of how far this hardware has come, this has required many design caveats---we have examined various \gls{acr:ml} and \gls{acr:rl} primitives alongside the architectures of modern \gls{acr:pdp} hardware to pick out the considerations of running in these resource-limited yet high-speed environments.
At the same time, I have also argued the case for the unity of \gls{acr:ddn} and dataplane programmability: by mitigating amplification \gls{acr:ddos} attacks using an online multi-agent \gls{acr:rl} system, and by showing one way in which \gls{acr:pdp} hardware can support data-driven classifications for network management.
%made online \gls{acr:rl} in \gls{acr:pdp} hardware possible, while also meaningfully advancing the state of the art in data-driven, programmable networks.
%?? focus learning -- in particular rl B)
%?? how have we supported online learning at large?
%?? not made possible, shown how the advanced capabilities of modern \glspl{acr:pdp} made feasible and considerations of running in high-speed env
%?? collected literature
%
%?? Say all we've shown
%?? This thesis has... (keep short)
%?? then subsections to discuss broad themes
Through these advances and thorough review of the literature, I have given substantial evidence for the value and viability of \gls{acr:pdp} networks empowered by data-driven methods, satisfying claims \stmtno{\numrange{0}{3}}.
Recalling the initial thesis statement:
\begin{quotation}
	\noindent
	\recalltext{stmt}
\end{quotation}
The value of each of these tools on their own for network management (\stmtno{0}) has been shown mostly by the use cases considered throughout \cref{chap:nets,chap:ddn}, but is also implicitly shown through the novel techniques presented in the thesis's main developments.
\gls{acr:rl}'s value in particular was shown by its use to learn to mitigate \gls{acr:ddos} attacks in an online way (\stmtno{1}).
By adapting classical \gls{acr:rl} policy formats and algorithm choices to suit the design, threading model, and \glspl{acr:fu} of manycore SmartNIC hardware, online in-\gls{acr:nic} \gls{acr:rl} was made possible.
Eliminating \gls{acr:pcie} and host stack overheads offered substantial latency benefits, and the parallelised Sarsa algorithm brought higher-throughput online learning (\stmtno{2}).
In-network aggregation of per-packet statistics to histograms made the handling of high-rate timestamps feasible, and enabled a flow classification task with clear operational benefits (\stmtno{3}).

What is far more interesting, however, are the wider takeaways and lessons learnt in the development of this work, and by collecting together a wide family of solutions falling under the \gls{acr:ddn} or \gls{acr:pdp} umbrella.
Each has its own impact on the design and deployment of the other.
Equally, it's worth mulling over the horizon of networking capabilities and form factors in the short- and long-term.
%Synthesis of implications, takeaways on design, deployment...
%?? which I will try to opine on here.

\section{The need for co-design}
%?? notes from cycle in
Making use of \gls{acr:ddn} and \gls{acr:pdp}-accelerated solutions is, as this thesis has likely demonstrated, an involved process.
In the \gls{acr:ddn} case, `zero-touch' deployment and development are likely impossible.
While we can train successful policies, \gls{acr:ddn} cannot itself derive the \emph{mechanisms} of control: action models, reward functions, and the state which they should operate on.
Learnt policies and parameters operate as well as they can \emph{within the framework we give them}, and generally succeed at so doing.
Yet as we've seen already, by designing \gls{acr:ddn} solutions without deep, cross-disciplinary human expertise on the controlled system we can easily introduce catastrophic impact in critical scenarios.
This extends even to testing and training environments; capturing every real interaction is crucial if one has any hope of generalising to production networks.
At the compute scales of interest, namely small and fast models with lower sample complexity, factoring in human expertise ahead of time can be useful in accelerating inference and training as opposed to completely `clean-slate' approaches.
There is an argument to be made about to what degree we \emph{should} be integrating our own intuition---biasing models away from potentially better solutions, i.e., that reward is all you need~\parencite{DBLP:journals/ai/SilverSPS21}---but we must often perform our own feature engineering regardless.
Temporal properties of state are one such example: we \emph{could} make use of \glspl{acr:lstm} and similar constructs to capture them automatically, but the price paid in complexity is less than appealing.
%Moreover, temporal properties can't be derived by fully-connected layers, requiring the 
%
%?? feature engineering needed: while nns can do it themselves in first layers (and outdo humans, again), this is wasted time to repeat and ?? 
%
%and at scales of interest (i.e., small model capacity) need to integrate human expertise -- even if ``reward is all you need''~\parencite{DBLP:journals/ai/SilverSPS21}, still need to design actions.
%?? argument to be made about to what degree we \emph{should} be integrating our own intuition (biasing models away from potentially better solns), i.e., that ``reward is all you need''~\parencite{DBLP:journals/ai/SilverSPS21}

Easily taking advantage of \gls{acr:pdp} hardware requires less thought.
Automatic offload tools are already very promising for (mostly) cutting host machines out of the packet processing loop, and extracting data and pipeline parallelism when they cannot.
I expect this tooling will only improve further.
For novel in-network compute uses or latency-optimal solutions, we should not expect any automatic wins.
More than anywhere else, these demand dedicated co-design, deep integration with hardware-specific communication primitives, and retooling to account for parallelism and heterogeneous compute models.
Today's---and likely tomorrow's---diversity of device designs and programming models is a blessing and a curse.
It is necessary that we have such variation to achieve a balance of performance, price, and capability across market silicon.
The downside is that this lack of unity demands insight and expertise on the devices themselves, rather than a single network-compute model, but making best use of them is intensely rewarding.
%This is not the only field where integrating device parallelism is difficult.
This is no surprise---taking advantage of parallelism alone is difficult~\parencite{sutter-freelunch}, but is itself necessary nowadays.
In combinatorics for instance, automated fork-join frameworks produced subpar results, necessitating similarly ``intrusive'' co-design to \approachshort{} which ``[increased] the amount of code needed by as much as an order of magnitude''~\parencite[p.~214]{ciaran-phd}.
%that aligns with the development of \approachshort,
%?? no such thing as zero-touch ddn, more likely zero-touch for pdp with offload texh. INC still needs all the thought (probably can't auto-derive many data structure transforms)
%?? ddn cannot derive mechanisms, and at scales of interest (i.e., small model capacity) need to integrate human expertise -- even if ``reward is all you need''~\parencite{DBLP:journals/ai/SilverSPS21}, still need to design actions.
%
%?? in ddn...
%?? and in INC in particular for PDPs: need to know 
%
%?? Downside? still need deep knowledge, from where I stand (though may be proven wrong) completely `clean-slate' unlikely to do great -- but we can still do better in params etc.
%
%?? Diversity of device designs and programming models requires insight, expertise... but rewarding.
%
%?? Downside:
%\begin{quotation}
%	\noindent
%	Our experience from this thesis has been that implementing parallelism properly is hard. It requires deep and intrusive coupling with the algorithm, and can increase the amount of code needed by as much as an order of magnitude.
%	
%	\hfill\parencite[p.~214]{ciaran-phd}
%\end{quotation}
%?? Accouing for exec model similarly intrusive: pipeline YES, data parallel YES (barring shared state), but latency is HARD
SmartNICs and \gls{acr:pdp} hardware impose a stronger blowup in complexity.
The intrusiveness extends in our case beyond parallelism: to the memory model, to specialised capabilities like \gls{acr:tcam}-backed \glspl{acr:mat}, and to missing \glspl{acr:fu} we might otherwise have taken for granted such as \glspl{acr:fpu}.
Many of the required data structure transformations cannot be automatically derived, and the best algorithms for in-network compute take advantage of tightly coupling these elements together.
Weighing these against one another requires real trade-offs to be consciously made at all levels of our design---one might also argue that this makes in-network solutions fragile against future hardware.
%?? This is made worse by the ... little unification? between what? Compute models?
%
%?? Heterogeneity great and useful, (necessary, some might say  to hit perf, if not price points) but also a curse: see above.
%?? Even so, needs great awareness.
This is not to say however that we won't (or shouldn't) have further developments here.
The ingenuity of engineers, novel \gls{acr:pdp} architectures, and economic drivers in data centre-scale applications will see to it that this is a long-lived wellspring of research.
It is simply the case that, as in parallel computing, there is no free lunch~\parencite{sutter-freelunch}.

%?? Test~\parencite{sutter-freelunch}
%?? Our problem is like parallelism, but stronger still due to MATs, NPUs, ... Must make tradeoffs at all levels

%?? the ghost of hardware yet to come.
%Ooooo
%What will this mean for programmability? Abstractions?
%?? increasing fallback to IRs from commodity languages the `right move'... probably.

\section{A difficult security context}
%?? Uncomfortable security context
%?? Try to establish when good/bad, wrt opsec?
At present, \gls{acr:ml} and \glspl{acr:dnn} have a wide variety of viable attacks at runtime and during training, which raises questions about whether their use in \gls{acr:ddn} is safe---either online or offline.
Attacks and defences still appear to be rapidly iterating against one another, and as such it's not clear that we should be focussing on integrating specific defences while they appear to have such a short shelf-life.
I argue that \gls{acr:ddn} deployment can be safe so long as there is a reasonable degree of isolation between a hypothetical attacker (or self-serving client) and the model.
%?? spec. attacks: evasion, poisoning, data extraction.
What isolation means then depends on the attacks we concern ourselves with.
For instance, destructive steps in processing, true isolation as in many resource placement problems (i.e., \gls{acr:ddn} applied at design-time), or aggregation of input data may aid against evasion, poisoning, and adversarial behaviour.
The network itself offers some degree of isolation of many outputs, which might make model stealing and evasion more difficult.
Transient network conditions make flow statistics noisier, pushing them away from the intended perturbation, while routing and \gls{acr:qos} decisions might only be inferred indirectly with added noise and delay.
%Hiding outputs as appropriate to prevent model stealing and similar attacks [probably can't even see most of the time, but might be able to infer approx later?]).
In closed loop circumstances, isolation is less clear.
This is purely intuitive reasoning, and likely unsatisfactory, but they are the best I can offer barring further research tailored to the network problem-space.
What the field needs instead is a set of quantitative bounds on how an attacker's input can affect learnt models in reasonable scenarios, measured specifically on network tasks.

%\section{The ghost of hardware yet to come}
%Ooooo
%
%What will this mean for programmability? Abstractions?
%
%?? increasing fallback to IRs from commodity languages the `right move'... probably.

\section{Future directions}
%?? Drop the headings once written

%A problem we raised (without a clear solution) was the design of reward functions which do not rely upon heuristic estimates or a priori knowledge of benign traffic content.
%If true online learning is desired (i.e., coping with a non-stationary environment), then such reward functions are sorely needed.
%While $\bload{\cdot}$ is likely to be a good candidate for many deployments, we believe that finding an effective metric derived from the individual statistics we suggested serves as an interesting research problem.

%?? Benefit of the more realistic emulation environment is that it is far closer in behaviour and architecture (i.e. viable) to a real SDN-enabled deployment, captures some dynamics which were otherwise hidden/lost by human ignorance. It also allows me to develop the system towards evolving traffic models where it is expected that RL should shine over and above standard ML techniques. THEN: Room to introduce/roll-in dynamic changepoint detection or adaptive exploration \cite{DBLP:conf/ki/Tokic10, DBLP:conf/ki/TokicP11, DBLP:conf/annpr/TokicP12}?

Given that one of the advantages of \gls{acr:rl} methods is their ability to dynamically learn by trading off exploration and exploitation, precisely how well-suited they are to handling the evolution of networks is an interesting research question.
Handling non-stationary problems is \emph{possible}, but rarely recommended, particularly with \gls{acr:dnn}-based policies.
To respond to such change, we simply need to either scale up gradient contributions, or increase the strength of exploration parameters like $\epsilon$.
Yet we are left with two key challenges.
The first is that we need robust means of detecting the kinds of problem-space evolution we're interested in.
There are tricky tuning factors to consider: chief among them are handling seasonality, and the timescales and magnitudes of evolution worth adapting to.
Suppose traffic varies diurnally, in which case choosing the wrong timescales would likely cause an online learner to oscillate between policies---meanwhile, the desired behaviour would be to learn to handle these modes in the \emph{same policy}.
The techniques discussed in \cref{sec:ddn-rl-design-considerations} may be useful to this end, such as adaptive exploration, changepoint detection, or signal processing methods (whose intersection with \gls{acr:rl} seems as-yet unexplored).
The second challenge is that we must understand and model what problem-space evolution \emph{really} looks like.
While it is known that \gls{acr:ddos} attack strategies evolve in real time~\parencite{DBLP:conf/spw/KangGS16}, to my knowledge no works detail what patterns such evolution might take.
In the wider Internet, aggregate changes in bandwidth and usage are likely easy enough to model~\parencite{DBLP:conf/anrw/BauerJHBC21}.
But, barring historic case studies, estimating the effects of new protocols and \glspl{acr:cca} before their deployment is unlikely to be feasible.

%?? problems: policy-space variation, and timescales, detection. i.e., seasonality in diurnal traffic probably counts as two `learnt behaviours' to handle in the \emph{same policy}.
%?? need to understand what problem-space variation really looks like: we can do it w/ traffic scales, attack patterns and protocol evolution less so.
%?? need to quantify.
%?? scaling up of gradient contributions, or exploration params like $\epsilon$.
%it is important to propose and test sensible simulations or captures of evolving networks.
%While it is known that DDoS attack strategies evolve in real time~\parencite{DBLP:conf/spw/KangGS16}, evaluation is difficult at present since, to my knowledge, no works detail what patterns such evolution might take.
%Regardless, these scenarios present ideal circumstances to apply some of the techniques discussed in \cref{sec:ddn-rl-design-considerations}: adaptive exploration, changepoint detection, or intelligent sampling methods to judge which flows are most worthy of consideration.
%For estimating \emph{when} to explore, we believe that the intersection of  and RL is as-yet unexplored.
%?? broaden to networks in general?

%\paragraph{OPaL}

%OPaL itself... In future, we aim to examine the performance of individual applications driven by \approachshort---both classical and \gls{acr:drl}-based---and how a NetFPGA implementation can offer further latency and throughput improvements.
%?? Different problems cover different proportions of tile-coding, distribs of state accesses...

Online \gls{acr:rl} via \approachshort{} is limited to devices in a SmartNIC or \gls{acr:npu}-style form-factor.
This is less than ideal for larger deployments, yet achieving this level of flexibility at switch scale is unlikely to be possible without heavy concessions.
Register access limits and a fixed number of pipeline stages would make a purely P4-\gls{acr:pdp} variant impossible to express, let alone a \gls{acr:mat}-accelerated approach (which would be dependent on the controller for policy updates).
A promising avenue here would be to investigate ongoing transfer learning between online \approachshort{} agents and high-throughput offline function approximators such as \glspl{acr:bnn}.
This might allow, for instance, having a canonical `known good' policy in the majority of the network installed to Tofino switches, while a smaller proportion of flows or packets are routed through actively learning bump-in-the-wire nodes.
The control plane is then responsible for collating their local policy modifications and generating a set of \gls{acr:bnn} parameters which expresses the same decision boundaries as the aggregated tile-coded policy.

%\paragraph{\seidr}
%In the future, we aim to examine the use of \seidr{} towards microburst detection and diagnosis~\parencite{DBLP:conf/sigcomm/ChenFKRR18} and for the identification of \emph{BBR}-like temporal properties of emerging UDP-based congestion-aware protocols, such as \emph{QUIC}.%~\cite{DBLP:conf/sigcomm/LangleyRWVKZYKS17}.

%\paragraph{general}
While this thesis achieves online \gls{acr:rl} in \gls{acr:pdp} hardware, it does so by choosing a function approximation scheme with lower model capacity than more common alternatives such as \glspl{acr:nn}.
How could we enable online learning for these more complex approximators?
Practically speaking, minibatches and replay buffers are necessities and will require storage in high-speed memory.
This is somewhat counter to the design of \gls{acr:pdp} hardware, but it wouldn't be too onerous a requirement in bespoke designs.
Computing gradients themselves in a way which is scalable and tailored to the execution model (many weaker cores or \glspl{acr:fu}) remains a challenge.
We might find value in combining insights from the field of distributed model-training (\cref{sec:distributed-training}), such as wait-free backpropagation, to achieve low-latency forward passes and parallelised updates to the policy when using model-parallel inference.
Here though, we would constrain the scope of such algorithms to a single device, which might enable some shortcuts and further optimisations.
This continues to make use of the many cores or \glspl{acr:fu} that we might expect on SmartNICs or \gls{acr:fpga} devices---\emph{N3IC}~\parencite{DBLP:journals/corr/abs-2009-02353} offers a model-parallel \gls{acr:nfp} implementation of the \gls{acr:nn} forward pass which might be compatible in theory.
\glspl{acr:bnn} are not, however, suited for this purpose, given that training requires incremental real-valued adjustments to the model parameters.
As such, online \gls{acr:nn} training in the \gls{acr:pdp} would likely mandate \emph{at least} fixed-point arithmetic, ruling out the strong performance benefits of \glspl{acr:bnn}.
\gls{acr:es} methods may be an exciting avenue here---devoid of \emph{any} gradient computation, they instead add uniform noise over the entire parameter set $\wvec{}$, making it computationally cheaper to train a policy than the use of backpropagation.

%\emph{N3IC}~\parencite{DBLP:journals/corr/abs-2009-02353} offers a model-parallel \gls{acr:nfp} implementation of the \gls{acr:nn} forward pass---this execution model 
%?? Insights from distributed model-training? Gradient calculation still tricky. Wait-free backpropagation as in \cref{sec:distributed-training} might work with non-\glspl{acr:bnn} to divide gradient compute across agents (again, exploiting multi-core or many \glspl{acr:fu} that we might get out of \glspl{acr:fpga})

%?? Future of networks??
%Future networks themselves...
%?? blurring the lines between compute classes (see cpu-nic co-designs)
%?? hybridisation, 

Future networks and \gls{acr:pdp} hardware designs will become only more varied and vibrant as time goes on.
Hearkening back to my earlier thoughts in \cref{sec:pdp-summary}, the hardware advances we've examined the wider impact of are only the first wave of truly programmable solutions.
I expect that we will see many more points along the capacity-capability Pareto front---blurring the lines between compute classes as \gls{acr:cpu}-\gls{acr:nic} co-designs are beginning to do.
Currently, the solution is to mix and match devices as a hybrid \gls{acr:soc} board (as in Intel's IPUs), but we should hope for a radical shake up in much the same vein as \gls{acr:rmt} sooner or later.
%?? Faster adaptability and installation?
Perhaps this will bowl over some of the roadblocks which make online (and otherwise in-\gls{acr:pdp}) \gls{acr:ml} difficult today---such as on-device state modification, or even piece-wise replacement of tables and logic (e.g., encoded as smaller P4 of \gls{acr:ebpf} subprograms).
%?? blurring the lines between compute classes (see cpu-nic co-designs)
%?? hybridisation,
%?? OS level changes?
Beyond hardware-based dataplane programmability, the rearchitecture and accelerated packet processing stacks we've seen from \glspl{acr:os} in the last few years alone bode well for software dataplanes.
While there will of course be iterative improvements to \gls{acr:xdp} and the like to make them more capable, user-friendly, or better exposed to applications, more specialised kernel and network stack designs will likely arise.
After all, the commodity \glspl{acr:cpu} which will inevitably be co-hosted with our SmartNICs and \glspl{acr:fpga} will need an accelerated and predictable stack for processing packets and flows.
%?? Proliferation throughout the wider Internet: maybe we'll never get to where active networks wanted to be, but .
%?? COntingent on lower cost and or more energy efficient? Maybe this couldbe a good thing?
In time I expect that while `dumb' routing hardware will still make up much of the Internet's backbone and capacity, if \gls{acr:pdp} becomes lower cost and more energy-efficient then we will see greater proliferation of programmable network infrastructure from the core to the edge.
It may be the case that we'll never reach the original \emph{active networking} vision of a fully cooperative and user-controlled routing fabric---perhaps this shouldn't be the case---but I think that this will enable a new kind of evolution in vendors, \glspl{acr:isp}, and host networking stacks.
%?? And new use cases in turn
Networking, as a field, is on the cusp of some truly interesting developments---and I'm excited to see where it all leads.

%My concluding thought is that \gls{acr:pdp} and its use cases present a vibrant, ongoing line of research---particularly when we consider how it can be combined with \gls{acr:ddn}.
%It offers today a source of raw device data which we would have never been able to feasibly use or export, as well as the necessary tools to perform complex data analysis at line-rate and switches' scale.
%At the same time it provides ways of helping host machines scale further via aggregation, and a promising location to perform complex, data-driven logic and reaction.
%I am of the opinion, however, that it is still a field in its adolescence.
%The hardware solutions and designs which have arisen and entered widespread market adoption (e.g., \gls{acr:rmt}$\rightarrow$Tofino) are impressive, powerful and capable---but they are the first wave of fully programmable devices at this scale and this form-factor, and we should expect even more innovation in hardware and languages as the field evolves.
%This may well take the form of incoming heterogeneity, as device manufacturers produce \gls{acr:pdp} devices tailored to different use cases in much the same way as \emph{Taurus}.
%Projects like \emph{BrainWave} reiterate that network hypergiants already hold the means and motivation to do just this.