Having introduced function approximation schemes of relevance to this thesis (and the most prominent in \gls{acr:ddn} at present), we now turn our attention to how these approximators are trained in practice.
Due to its relevance in \gls{acr:dnn} and \gls{acr:drl}, this comprises a brief discussion of gradient descent techniques and stochastic optimisers, followed by a more in-depth introduction to and overview of the field of \gls{acr:rl}.

\subsection{Gradient descent}
Most supervised \gls{acr:ml} problems are defined in terms of a \emph{loss function}: assuming that we're trying to learn a mapping from an input space to an output space, we can generally output a numeric score in $\mathbb{R}$ for our current policy parameter set $\wvec{}$.
However, knowing a simple quality metric is insufficient for determining how we should change our many-dimensional policy parameter vector

Regression: (LIST), Classification: (LIST)
?? Examples??

Moreover, these common loss functions are differentiable in input data and params...
Output is a gradient vector which represents a direction in parameter-space which would \emph{increase} the value of the loss function for the given input data.

\emph{Gradient descent} is the iterative process of optimising our parameter vector $\wvec{}$, using all input data at each iteration:
\begin{equation}
	\wvec{t+1} = \wvec{t} - \alpha\nabla{L\left(\cdot,\wvec{t}\right)}
\end{equation}
By the above intuition, this moves the parameter vector a small step (learning rate $\alpha\in\left(0,1\right)$) in the direction which would optimise its overall performance.
However, re-evaluating the loss function over all input data becomes intractable in the case that either the function is expensive to compute on a moderately-sized dataset, or the dataset itself is monstrously large---both of are often true in \gls{acr:dnn} training.

It is for this reason that \gls{acr:sgd} and related algorithms are typically employed in \gls{acr:ml} model training, particularly for \glspl{acr:dnn}.
\gls{acr:sgd} modifies the above such that individual, randomly chosen samples (or larger minibatches) are used as the input for the loss function rather than the entire dataset.
This \emph{approximates} the loss gradient at $\wvec{}$ on the input data, but in practice this is key for the training of modern \gls{acr:ml} techniques---$\alpha$ may be reduced over time and the dataset reshuffled as necessary until convergence.

\emph{Adam}~\parencite{DBLP:journals/corr/KingmaB14} from RMSProp~\parencite{rmsprop}

It should be noted that the above discussion is a very cursory treatment of the subject, primarily intended to contrast the in-depth discussion of \gls{acr:rl} methods in the sequel.
For more details, readers should refer to more specialised texts such as \Textcite{DBLP:journals/siamrev/BottouCN18, DBLP:books/daglib/0040158}.

\begin{marginfigure}
	\centering
	\resizebox{0.9\linewidth}{!}{\input{diagrams/rl/toy-forward-plan}}
	\caption[A motivating example for MDPs in handling delayed rewards.]{A toy example showing how the \emph{delayed rewards} which an \gls{acr:rl} agent learns to maximise. Consider a traveller journeying towards a castle in the south, with $R$ being some aggregate of food, water and rest. Learning only to optimise immediate rewards attached to actions would lead our traveller to choose an overall worse path---going thirsty in the desert!}\label{fig:toy-rl-motiv}
\end{marginfigure}

\subsection{Reinforcement learning}
Supervised and unsupervised methods alike typically offer clear ways to map from the model parameters we have at this moment, and our performance on a training dataset, into a \emph{loss function}.
Naturally, when we're trying to optimise for decisions and estimations in classification, clustering, and regression, it suffices to apply gradient descent and similar optimisations to minimise this loss function alone.
How might things differ when we want to design some \emph{agent}---an actor who sees and acts on a system for many timesteps---to interact with the world?
Consider a toy example in \cref{fig:toy-rl-motiv}: an oracle with global knowledge knows that a hypothetical traveller would best be served by going through the swamp, even if it appears the worse of two choices.
Yet if we applied simple input classification to choose our path at each turn, we would always act poorly without some way of propagating knowledge of later choices.
It becomes more complex to collect training data once we realise that our actions move the world forward and change it (thus we may not be able to rewind a simulation to explore alternative choices), and that we might need to explore apparently suboptimal choices for some time to be better off.

\gls{acr:rl} algorithms are methods of training such an agent to choose an optimal sequence of actions in pursuit of a given task~\parencite{RL2E}, and neatly encode these training- and run-time notions of \emph{exploration} and \emph{exploitation}.
An agent is typically defined by its \emph{policy} $\pi$, which chooses an action in response to an input state.
These powerful techniques effectively use a \emph{reward} metric to learn this state-action mapping without any explicit model of the system they're learning to control---even when reward signals are sparse or come with some delay after an initial choice.

To achieve the goals described above, we first make the assumption that the task we're attempting to solve is structured as an \gls{acr:mdp}.
In an \gls{acr:mdp}, we break the world or target problem down into a set of \emph{states} ($\mathcal{S}$), \emph{actions} ($\mathcal{A}$), and \emph{reward measurements} ($\mathbb{R}$).
To relate this to computer networks, an example state space would be a vector of buffer occupancies in a switch, an action space would be the priority to assign some new flow which has arrived, and the reward might be the proportion of finished flows which achieved comfortably low \glspl{acr:fct}.
We then consider our sequence of interactions in discrete \emph{timesteps}---at the present time $t$, an agent observes $s_t \in \mathcal{S}$, chooses some $a_t \in \mathcal{A}$, and then observes their change in the world as a new state $s_{t+1} \in \mathcal{S}$ and a reward measure $r_{t+1} \in \mathbb{R}$.
As required, we can qualify these further, i.e. the set of actions we can take from a state $s$ may be limited to $A_s \subseteq \mathcal{A}$.
Acting optimally then consists of maximising the sum of rewards over some time horizon.
This is captured up to an infinite horizon by the concept of the \emph{expected discounted reward}:
\begin{equation}
	\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}R_t\right]
\end{equation}
where the \emph{discount} $\gamma \in \left[0,1\right]$ controls how ``forward-planning'' an agent may be\sidenote{Setting $\gamma=0$ defines a `myopic' agent, where no intermediate loss of reward is allowed. Choosing $\gamma=1$ will prioritise future rewards, with no concern over how long it may take to \emph{achieve} those rewards. This hyperparameter features in many \gls{acr:rl} algorithms; in practice, we choose values closer to \num{1}.}, always choosing $a_t \sim \pi\left(\cdot | s_t\right)$.
While dense, this formalism essentially captures and describes the performance of our agent over all possible run-lengths from all starting states (i.e., over all \emph{episodes}), and in general this is what \gls{acr:rl} algorithms are designed to maximise.

\glspl{acr:mdp} assume that there are stationary, well-defined (though stochastic) transitions between these states.
For any $s, s' \in \mathcal{S}, a \in A_s$, an \gls{acr:mdp} is defined by a state transition function, $\operatorname{P_a}\left(s,s'\right)$ which returns the probability that we will land in $s'$ after taking action $a$ in state $s$, and a function returning the \emph{expected reward} $\operatorname{R_a}\left(s,s'\right)$ for each valid transition.
If we have all this information, then we can simply apply the Bellman equations (??), a dynamic programming algorithm, to solve for an optimal policy.
In real-world scenarios however, we usually lack this knowledge; this either requires involved modelling, or is rendered intractable by a continuous or combinatorially large state space.

Like most \gls{acr:ml} methods, modern \gls{acr:rl} algorithms use gradient information to update the parameters used to approximate a function.
How \gls{acr:rl} differs is that it aims to learn an optimal policy without any knowledge of the \gls{acr:mdp} apart from its state and action space---\emph{model-free}.
Knowledge of the \emph{trajectories} followed by agents (i.e., state-action-reward tuples at all timesteps) is then used to compute target values and adjustments for the parameter set $\wvec{}$ for any function approximator.
By requiring that our policy approximation, i.e. $\pi\left(a | s, \wvec{} \right)$, is differentiable \gls{acr:rl} works in tandem with the other mathematical training approaches described earlier.\sidenote{Note the policy $\pi$ does not require any knowledge of rewards, except when training. This allows for \gls{acr:rl} agents to be trained using simulated systems or data, and then deployed in an environment where they cannot access this knowledge due to sampling cost. If these measures are present in deployment, then \gls{acr:rl} can learn online.}

%\gls{acr:rl} works in tandem with other mathematical training approaches: the key insight is that the structure of an MDP allows external information and model(-free) observation to strengthen or weaken different function responses.

\paragraph{Demonstrating RL: Sarsa}
While many \gls{acr:rl} algorithms have been developed (each of which making quite different choices on how to apply trajectory data), it is likely most helpful to start with a concrete point in the design space \emph{before} listing their full variety.
Doubly so here, as this particular algorithm---single-step, semi-gradient Sarsa~\parencite[pp. \numrange{217}{221}]{RL2E}---underpins \cref{chap:ddos-rl,chap:in-net-rl}.
The Sarsa algorithm considers only one transition at a time: a pair of states $s_t,s_{t+1}$, their accompanying actions $a_t,a_{t+1}$, and the reward $r_{t+1}$ accompanying $s_{t+1}$.
This is why it is defined as \emph{single-step}, and as such it does not require a completed trajectory for learning.

Assume first that $\wvec{}$ is a large block of real numbers residing in \gls{acr:ram}, and that we update this block through time.
Next, as Sarsa is a \emph{value-based} method, an agent operates by defining an approximate \emph{value} function $\acval{s}{a}{\wvec{}}$ to each action $a$ it can take from $s$, typically choosing the highest-value.
We can then update $\wvec{}$ over time as follows:
\begin{subequations}
	\begin{gather}
		\delta_t = r_{t+1} + \gamma \acval{s_{t+1}}{a_{t+1}}{\wvec{t}} - \acval{s_t}{a_t}{\wvec{t}},\\
		\wvec{t+1} = \wvec{t} + \alpha \delta_t \nabla{\acval{s_t}{a_t}{\wvec{t}}},
	\end{gather}%
	\label{eqn:sg-sarsa}%
	where $\delta_t$ is known as the \gls{acr:td} error, and the vector gradient $\nabla$ is taken with respect to $\wvec{}$. $\alpha\in[0,1]$ defines the learning rate (governing the policy stability), with $\gamma$ defined as in the \gls{acr:mdp} formulation.
\end{subequations}

In essence, at each timestep the policy parameters ($\wvec{}$) are increased along the gradient ($\nabla{\acvalblank}$) using a fixed learning rate ($\alpha$) and a computed adjustment ($\delta_t$).
This adjustment is equal to the difference between the chosen action $a$'s value in state $s$ and the reward received ($r_{t+1} - \acval{s_t}{a_t}{\wvec{}}$), \emph{plus} some part of the \emph{next} action's value ($\gamma\acval{s_{t+1}}{a_{t+1}}{\wvec{}}$).
Naturally, as state transitions are visited and revisited during training, the value of later choices can propagate down the tree of states, as shown by \cref{fig:sarsa-intuition}.
To see how this combines with a simple linear function approximation, consider \cref{fig:sarsa-tilecode,fig:sarsa-tilecode-update}.

\begin{marginfigure}
	\resizebox{0.9\linewidth}{!}{\input{diagrams/rl/td-learn}}
	\caption[An illustration of how value adjustments in single-step RL propagate backwards through a state trajectory.]{A simplified view of how action values propagate back through visited states: because a state's value includes some portion $\gamma$ of its children's values, at convergence it includes $\gamma^2$ of its grandchildren, and so on. A limitation of the single-step family is that they must visit the same transitions several times for earlier states to learn about their successors.}\label{fig:sarsa-intuition}
\end{marginfigure}

\begin{figure}
	\centering
	\begin{subfigure}{0.6\linewidth}
		\resizebox{\linewidth}{!}{\input{diagrams/rl/tile-coding-worked}}
		\caption{Tile Coding\label{fig:tilecode-code}}
	\end{subfigure}

	\begin{subfigure}{0.6\linewidth}
		\resizebox{\linewidth}{!}{\input{diagrams/rl/tile-coding-acsel}}
		\caption{\centering Value Estimation and Action Selection\label{fig:tilecode-select}}
	\end{subfigure}
	
	\caption[An end-to-end example of Sarsa selecting an action using a tile-coded policy.]{
		An example of tile coding for 2-dimensional state and 4 actions, using 2 tilings, 3 tiles per dimension, and a bias tile.
		All components of $s$ are clamped to $[0,1]$.
		For simplicity, we denote $\symbfit{x}(s,\cdot)$ as a list of indices and represent the values of all actions at each tile with a vector.
		(a) The state $s$ activates the bias tile and exactly one tile in each tiling.
		(b) The action values of active tiles are summed to produce the current value estimate for each action available in $s$---for this state, local knowledge ensures that action 4 is chosen by the greedy policy despite typically being a poor choice elsewhere.
		\label{fig:sarsa-tilecode}
	}
	\vspace{-0.6cm}
\end{figure}

\begin{figure}
	\centering
	\resizebox{0.6\linewidth}{!}{\input{diagrams/rl/tile-coding-update}}
	\caption[An end-to-end example of Sarsa updating the values for a chosen action using a tile-coded policy.]{
		The update step for \cref{fig:sarsa-tilecode}, given an observed \gls{acr:td} error $\delta_t=-0.2$ (indicating a lower observed reward than the expected long-term value of \num{0.7}) and $\alpha=0.5$.
		Action 4's value is thus reduced in the tiles associated with state $s$, but remains the most likely choice; the negative $\delta_t$ may have arisen from noise in the reward signal.
		For illustrative purposes, we choose an unrealistically high $\alpha$ (typically, $\alpha\le0.05$ is a more reasonable choice).
		\label{fig:sarsa-tilecode-update}
	}
	\vspace{-0.6cm}
\end{figure}

\paragraph{The algorithm design space}
While the above introduction to Sarsa is a clear way to intuit the key ideas underpinning \gls{acr:rl}, it is a very specific point within a remarkable design space.
For context, other algorithms may employ separate state-value approximations, use the entirety of an agent's trace, or be tailored to characteristics of the policy approximator (e.g., how neural networks benefit from batching).
Algorithms tend to differ in some key ways:

\begin{description}
	\item[Trace length.] Single-step methods like the above can be generalised to \emph{$n$-step methods} by including further discounted reward measurements (at increased runtime memory cost), as in \emph{A3C}~\parencite{DBLP:conf/icml/MnihBMGLHSK16}. \emph{Monte Carlo methods} such as \emph{REINFORCE}~\parencite{DBLP:journals/ml/Williams92} carry this to its logical extreme, updating every transition in a trace using the exactly known return for an episode. This solves the issue of `repeat visits' hinted at in \cref{fig:sarsa-intuition}, at the cost of storing entire trajectories (which can be tricky to delineate into clear episodes in some online tasks). Consider also \emph{eligibility trace} methods such as TD($\lambda$)~\parencite{DBLP:journals/cacm/Tesauro95}, which propagate value backwards through the \gls{acr:mdp} by including some portion $\lambda\in\left[0,1\right]$ of the last timestep's gradient alongside the current (i.e., having some decaying part of \emph{every} prior state's gradient).
	\item[The role of values in a policy.] Sarsa (and many other algorithms) follow a \emph{value-based} approach: the role of the function approximator is to compute and learn the value for each action, and then action selection chooses based on these values. This design, however, prevents continuous control (i.e., $A\in\mathbb{R}$). The dominating paradigm of late has been \emph{policy gradient methods}, which impose no requirement on the output---given a differentiable performance metric in $\wvec{}$ is known. This allows easier expression of many system designs, such as having mixed real-valued and discrete action components. The development of \emph{DPG}~\parencite{DBLP:conf/icml/SilverLHDWR14} has been a key driver in ensuring their suitability for continuous problems. \emph{Actor-critic} algorithms are a considerable subset of this which also learn separate a value estimate for the current state to provide this performance measure. Going further still, \emph{direct policy search} approaches such as those inspired by or using \gls{acr:es}.
\end{description}

\paragraph{Modern algorithms}
\emph{Q-learning}~\parencite{WatkinsThesisQLearning}, similar to Sarsa (though \emph{off-policy}), has had a long and continuing influence on the design of \gls{acr:rl} algorithms.
\emph{DQN}~\parencite{DBLP:journals/corr/MnihKSGAWR13} introduces the additional modifications required to make Q-learning stable and feasible using \glspl{acr:dnn}---introducing \gls{acr:drl}---primarily by including an \emph{experience replay buffer}.
This buffer contains a history of individual state transitions, and once this is full enough the model is updated using a randomly chosen subset (minibatch) of such transitions at each timestep, drawing influence from \gls{acr:sgd}.\sidenote{This is indicative of the high \emph{sample complexity} of \gls{acr:drl} methods in general.}
\emph{DDQN}~\parencite{DBLP:conf/aaai/HasseltGS16} addresses value over-estimation in this family of algorithms by using two parameter sets $\wvec{}$ and $\wvec{}'$, which alternate between the roles of computing action-values and the action value for the \gls{acr:td} update $\delta_t$.

%?? Space from direct policy search (ES, Policy gradients) towards value-based \url{https://icml.cc/2015/tutorials/PolicySearch.pdf}

The flexibility of \glspl{acr:dnn} in representing a wide variety of policies, having or combining continuous actions with discrete, has led to a rich base of work in actor-critic algorithms to exploit this functionality.
\emph{DDPG}~\parencite{DBLP:journals/corr/LillicrapHPHETS15} acts as a Q-learning-style actor-critic algorithm, building on the theoretical basis of DPG with the inclusion of DQN-style minibatches.
\emph{A3C}\sidenote{A3C is `asynchronous' in the sense that it is trained from parallel agents, which is different from the below discussion on \emph{environmental} asynchrony. When this is excluded, the algorithm is known as A2C.}~\parencite{DBLP:conf/icml/MnihBMGLHSK16} learns from $n$-step returns (applying the critic value as a baseline to reduce variance), but makes the key choice to replace experience replay by learning from parallel agents controlling simulations on the same device (increasing sample diversity across input states).
\emph{TD3}~\parencite{DBLP:conf/icml/FujimotoHM18} builds on DDPG, but draws on the development of DDQN to attempt to reduce the upward bias in the critic function by having one actor networks and two policy networks: both critics are trained based upon the smaller estimate output by either.
The actor's update steps are also delayed and occur more infrequently, to accelerate its learning using a (slightly) more trained critic.
\emph{D4PG}~\parencite{DBLP:conf/iclr/Barth-MaronHBDH18} introduces to DDPG a distributional critic~\parencite{DBLP:conf/icml/BellemareDM17} (as opposed to modelling the expected value for a state), improving its performance further by allowing learning to be aware of noise inherent in the value signal due to the environment and critic itself.

Policy gradients methods closer to direct policy search, such as \emph{TRPO}, have equally seen key successes.
TRPO~\parencite{DBLP:conf/icml/SchulmanLAJM15} recasts the learning problem as a constrained optimisation problem to be solved by \gls{acr:sgd} or another stochastic optimiser.
The policy parameters are changed to maximise the expected return from Monte Carlo trajectories, given the constraint that the new policy's expected outputs must fall within a Kullback-Leibler error bound (i.e., must not differ too greatly).
\emph{PPO}~\parencite{DBLP:journals/corr/SchulmanWDRK17} remains a Monte Carlo algorithm over independent agent traces but clips the contribution of an action's probability ratio, leading to a simpler implementation and increased performance as this has the \emph{side-effect} of implicitly limiting the size of policy changes.

\gls{acr:es} algorithms, while not strictly belonging to the \gls{acr:rl} family, have become relevant in the same \gls{acr:mdp}-like control problems we are interested in.
Hybrid \gls{acr:rl}-\gls{acr:es} works~\parencite{DBLP:journals/corr/SalimansHCS17} inject Gaussian noise into the policy parameters themselves, and then use the Monte Carlo return values to perform a weighted combination of noise updates between distributed actors.
This can be considered as an approximation of policy gradients, \emph{without} executing any gradient function (and so removing the cost of backpropagation).
This is primarily justified by systems-level optimisations---deterministic noise generation means that agents need not send full policy updates to one another, merely their final reward values (saving on communications overhead).
More canonical \gls{acr:es} algorithms~\parencite{DBLP:journals/corr/abs-1802-08842} begin again with distributed actors using perturbed versions of a starting policy, but keep only the top-$k$ policies.
The mean of their noise vectors is then taken to be the `true' policy update.

%
%Note -- are actor-critic methods needed for continuous action spaces? Might be worth mentioning whjat it takes to learn both discrete and continuous. Need to use different methods to explore, i.e. Ornstein-Uhlenbeck process.
%
%?? Also want to spend some time discussing various action-selection strategies, that the output can be
%
%?? The reference~\parencite{RL2E}

%?? Can we relate this to the broader ML notion of concept drift?

\paragraph{Exploration vs. exploitation in practice}
Consider again the dilemma presented in \cref{fig:toy-rl-motiv}: discovery of an optimal policy relies on either global knowledge, or \emph{exploration} of a suboptimal portion of the state-space.
\gls{acr:rl} agents are typically initialised with either zeroed or random policy parameters, but we cannot expect that for larger state spaces this produces \emph{meaningful} exploration.
I.e., it may well be the case that following the current optimal policy up to some uncertain state and then exploring can provided targeted and useful samples, whereas randomised parameters are something more akin to a random walk for all timesteps spent in early episodes.

To encourage exploration in discrete action spaces we typically introduce some randomness into action selection.
$\epsilon$-greedy methods pick another action at random with probability $\epsilon$---typically annealing the value of $\epsilon$ towards 0 over some number of timesteps or training episodes, while the simplest way to achieve this in many \gls{acr:nn} is to use the outputs of a softmax~\parencite{luce-softmax} layer as a probability distribution over actions.
Boltzmann and Max-Boltzmann action selection~\parencite[pp. 73]{WieringMThesisRLExploration} constitute variations of these schemes which control the concentration of action probabilities.
Active estimation of the degree of exploration has also attracted healthy degrees of interest (particularly with regard to evolving or non-stationary problems): VDBE~\parencite{DBLP:conf/ki/Tokic10,DBLP:conf/ki/TokicP11} uses changes in the \gls{acr:td} error to control $\epsilon$ over time, while \textcite{DBLP:conf/annpr/TokicP12} train a continuous \gls{acr:nn}-backed agent to control exploration parameters.

In the case of continuous \gls{acr:rl} action spaces, an initial consensus in the wake of the DDPG~\parencite{DBLP:journals/corr/LillicrapHPHETS15} was the use of Ornstein-Uhlenbeck processes~\parencite{PhysRev.36.823} to inject noise in the action space.
However, more recent ablation studies have shown that this offers no tangible benefits over Gaussian noise~\parencite{DBLP:conf/icml/FujimotoHM18,DBLP:conf/iclr/Barth-MaronHBDH18}.

\paragraph{RL in asynchronous data-driven networking}
Automatic, adaptive control requires accurate, recent state to make optimal decisions and to act in a timely manner.
Action execution, computation and training have real costs, which have been shown to negatively affect the performance of asynchronous \gls{acr:rl} systems~\parencite{DBLP:journals/firai/TravnikMSP18}.
Hence, \gls{acr:ddn} applications are profoundly affected, as they must often reckon with inference times which are significant compared to the systems they control.
As it stands, state measurement and policy execution require additional hardware and infrastructure, increasing delays and costing rack-space in the datacentres or networks where we aim to deploy \gls{acr:ddn}.
%Placing learning algorithms, policy execution, and measurement into the network fabric will increase performance, the accuracy of system state and simplify network architectures which use data-driven concepts.

There remains some degree of divergence between the theory and implementation of \gls{acr:rl} agents.
Consider \cref{fig:state-slip}: the traditional formulation of an \gls{acr:mdp} assumes that an agent receives a new view of the world's state at fixed time intervals, and then decides upon and executes an action instantly.
The reality is that state information takes time to traverse the network, service times are offset by how quickly hosts respond to interrupts and deserialise requests, and action preference lists are often computed via expensive policy approximations.
Action installation also incurs costs in fields such as network administration, initially to contact the controller and then for those actions to be installed via the control plane.

These delays (and variance thereof) add noise to the state-action mapping being learned, which has a potent reduction to learning rate and final accuracy, even for simple grid world tasks according to \textcite{DBLP:journals/firai/TravnikMSP18}.
They in turn show that reordering algorithmic steps can reduce these costs for online single-step algorithms, but that reducing this further requires detailed agent-environment co-design.
%This principle has influenced the design of real network use cases, such RL-based congestion-control algorithms~\parencite{DBLP:journals/corr/abs-1910-04054}, showing that asynchrony is necessary for high-speed applications.
Achieving this often requires that state measurements are combined or coalesced~\parencite{DBLP:journals/corr/abs-1910-04054,DBLP:journals/tnsm/SimpsonRP20} while expensive computations are ongoing.
`Stopping the world' in the algorithmic sense causes significant performance degradation, as inference takes up to \SI{30}{\milli\second} in the above work on congestion control, or any time-sensitive control problems.

\begin{figure}
	\begin{subfigure}{0.45\linewidth}
		\centering
		\resizebox{0.975\linewidth}{!}{\input{diagrams/rl/sa-slip1}}
		\caption{Theory: state measurement, action computation, and learning are zero-cost.}
	\end{subfigure}
	\hspace{0.05\linewidth}
	\begin{subfigure}{0.45\linewidth}
		\centering
		\resizebox{0.75\linewidth}{!}{\input{diagrams/rl/sa-slip2}}
		\caption{Reality: costs of measurement and action lead to \emph{state drift}---over a time delay $t_1+t_2+t_3$, inaction transforms state $S$ into $S'$.}
	\end{subfigure}
	\caption[State slippage in an asynchronous RL agent.]{One issue which arises when we aim to introduce \gls{acr:rl} into temporally fine-grained environments is that the \gls{acr:mdp} formulation can fail to capture how state drifts during computation. Input states may take some time to acquire and transmit to an agent over the network ($t_1$), the function approximator itself has an associated computational cost for inference ($t_2$), and enacting said action can involve network latency \emph{and} expensive table preprocessing in the case of hardware P4 implementations ($t_3$). As a result, the system actually undergoes the transition $(S', A)\rightarrow(S'')$, introducing noise or variance into the value function being learned. \label{fig:state-slip}}
\end{figure}