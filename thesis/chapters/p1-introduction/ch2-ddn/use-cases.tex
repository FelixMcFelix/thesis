To give a clear (if somewhat informal) introduction to what different processing techniques can offer, I present a selection of \gls{acr:ddn} use cases.
The aims of this section are threefold: to offer a rough intuition of the capabilities of state-of-the-art \gls{acr:ml}/\gls{acr:rl} techniques, to present the breadth of optimisation and control problems in \gls{acr:ddn}, and to describe the sorts of interaction model and co-design required to meet performance guarantees.
In the case of \gls{acr:rl}-based works, I devote extra space towards highlighting the state space \prllitstate{}, action space (\rllitact{} if discrete, \rllitactreal{} if continuous), and reward source \prllitreward.
If live-control approaches are evaluated using network traces instead of a live environment or suitable simulation, I mark them with a `$\dagger$'---this does not invalidate those authors' findings, but should invite a hint of scepticism based on the discussions of \cref{sec:ddn-challenges}.
Readers anxious to see the common threads between these very separate applications might skip to \cref{sec:ddn-use-takeaways}.
%
%?? Optimisation
%
%?? Design
%
%?? Detection / telemetry / inference
%
%?? Refer back to the computer networks chapter: topologies, routing, defence, ... all present problems who are often served and solved by the use of heuristics.
%
%?? \gls{acr:ddn} risks~\parencite{DBLP:conf/hotnets/BartulovicJBSS17}.
%
%led the charge in data-driven networking, enhancing automatic traffic optimisation~\parencite{DBLP:conf/sigcomm/ChenL0L18,DBLP:conf/sigcomm/MaoNA17}, congestion control~\parencite{DBLP:journals/corr/abs-1910-04054}, adaptive routing~\parencite{DBLP:conf/hotnets/ValadarskySST17,DBLP:conf/conext/GiladSGRS20}, resource management~\parencite{DBLP:conf/hotnets/MaoAMK16}, and packet classification~\parencite{DBLP:conf/sigcomm/LiangZJS19}.

%?? Can I please dig up some CNN/LSTM/non-RL-based variants so that this isn't the RL show?

?? Ideally, should mirror the ``networking problems'' section in Ch1, so that there I can explain the problems and give a bunch of cites and run down how people solve packet classification etc. without ML/RL

\subsection{Network Management}

\paragraph{Routing and Traffic Optimisation}
As discussed earlier, routing is the task of moving packets of network data from their source to their destination, ideally without losing any in transit and as quickly as possible.
We can look at this as how to move a packet towards the \gls{acr:as} where the destination is located using logical boundary information (\emph{inter-\gls{acr:as} routing}), and how to move packets over the physical infrastructure within an \gls{acr:as} (\emph{intra-\gls{acr:as} routing}).
As inter-\gls{acr:as} routing requires consistent protocols and negotiation between organisations, intra-\gls{acr:as} routing offers more scope for optimisation and innovation.
The usual term for such processes is \gls{acr:to}/\gls{acr:te}, aiming to minimise congestion and increase client \gls{acr:qos}~\parencite{rfc3272}.

\textcite{DBLP:conf/hotnets/ValadarskySST17} show how \gls{acr:rl} can be used to route traffic by mapping the last $k$ demand matrices \prllitstate{} into a set of edge weights \prllitactreal.
The calculated weights are used as the input to compute probabilistic forwarding strategies based on classical hop-by-hop routing, which then allow congestion to be computed for the following demand matrix \prllitreward.
Policy approximation is provided by a fully connected \gls{acr:nn}, trained using the TRPO algorithm~\parencite{DBLP:conf/icml/SchulmanLAJM15}.
This is striking work because it presents an environment where RL categorically beats supervised learning---where predicting a set of actions to take is more effective than predicting the next state and then computing an optimal assignment---and is able to outperform the non-ML \emph{oblivious routing}~\parencite{DBLP:conf/stoc/AzarCFKR03} for some problem models.
There are several key takeaways from this work: their exploratory designs showed that system performance and learning rate rely heavily upon output model size, emphasising the need for a minimal representation of actions/predictions made\sidenote{Even though a smaller model size is arguably less expressive, the fact that there are fewer parameters to learn can be instrumental in converging to a more effective solution more quickly.}; policy execution occurs outside the packet path, and so learns feasibly online; and that using \gls{acr:ddn} outputs as the input for a well-defined algorithm can offer more interpretability and trust in an optimised system.
A drawback worth discussion is their \gls{acr:nn} architecture's input and output dimensions depend on the network under control ($k\cdot\left|V\right|^2\rightarrow\left|E\right|$), and so learned policies are not portable even under simple alterations like runtime switch/link additions.
Memory cost, compute time, and parameter count would equally scale poorly in larger networks.

\emph{AuTO}~\parencite{DBLP:conf/sigcomm/ChenL0L18} examines several \gls{acr:to} problems in greater depth, explicitly aiming to optimise datacentres of $>$\num{10000} servers via \gls{acr:drl}.
This presents a key problem: inference using their architecture has a $\sim$\qty{100}{\milli\second} latency, which is rather at odds with the long-tailed distribution of datacentre traffic (namely, that shorter \emph{mice} flows greatly outnumber longer \emph{elephant} flows~\parencite{DBLP:journals/ccr/PanBPS03}).
The primary consequence is that trying to take per-flow actions in such low-\gls{acr:rtt} environments causes decisions to either apply late into the flow lifecycle or miss their target entirely, unless they can be reliably taken sub-millisecond.
The posed solution uses two agents concurrently, for mice and elephant flows respectively.
\emph{sRLA} produces a set of flow size thresholds for simple queue priority assignment\sidenote{Smaller flows are prioritised, as they are assumed to be more deadline sensitive or to suffer higher relative \glspl{acr:fct} in the event of losses.} \prllitactreal, using the 5-tuple, \gls{acr:fct}, and size of each completed flow \prllitstate{} to optimise the ratio of average per-packet queue times with regard to the last timestep \prllitreward.
Flows in all but the last priority class are routed using \gls{acr:ecmp}.
\emph{lRLA} then makes bespoke decisions for the remaining flows which---with high probability---will continue long enough to be meaningfully benefited.
For all live and completed flows, it uses the 5-tuple with the current priority (if live) or the \gls{acr:fct} and size (if complete) \prllitstate{} to choose the flow's priority, rate, and route as an XPath ID~\parencite{DBLP:journals/ton/Hu0W0L0ZG16} \parenglance{$\rllitactraw\times\rllitactrealraw\times\rllitactraw$}.
This is conditioned on the ratio of average throughputs between two timesteps \prllitreward.
AuTO uses a \gls{acr:dnn} for policy approximation in both agents, trained using the DDPG algorithm~\parencite{DBLP:journals/corr/LillicrapHPHETS15}, offering an average \qty{8.71}{\percent} improvement over heuristics in evolving traffic after \qty{8}{\hour} of online training.
The main design feature of interest to us is this agent separation; that an RL agent can be used to control a time-sensitive system by generating a compact set of parameters for another, more efficient algorithm.
However, the reliance on XPath route numbers as an action ties the lRLA policy to the network it was learned in, preventing shared training in spite of the fixed-size architecture.

\emph{SmartEntry}\littrace~\parencite{DBLP:conf/sigcomm/00010YC20} uses an alternate formulation of \gls{acr:te} to selectively route traffic at key switches based on the destination.
This differs from \citeauthor{DBLP:conf/hotnets/ValadarskySST17} by using the REINFORCE~\parencite{DBLP:journals/ml/Williams92} \gls{acr:rl} algorithm with \glspl{acr:cnn} to choose a set of \emph{location-destination pairs} to install route changes \prllitact{} from the \emph{current} traffic matrix \prllitstate.
For these nodes, an \gls{acr:ilp} model calculates an optimal probabilistic forwarding policy among their neighbours, whose maximum utilisation is used as a loss function \prllitreward.
Although this outperforms (weighted) \gls{acr:ecmp}, this has much the same scale and transfer issues as \citeauthor{DBLP:conf/hotnets/ValadarskySST17} ($|V|^2\rightarrow|V||V-1|$)---in \gls{acr:isp} networks this is to some extent acceptable given that $|V|\leq49$ in representative trace data.
The key concern is that the runtime cost of the ILP formulation isn't documented, which can have a severe impact on stability if traffic matrices change quickly.\sidenote{\gls{acr:cnn} execution is performed \emph{only once} irrespective of how many reroutes are inserted, so this cost is likely dominated by the \textsf{NP}-hard \gls{acr:ilp}.}

Inter-\gls{acr:as} routing in the modern Internet is fairly fixed, operating according to the fixed principles of the \gls{acr:bgp} suite.
However, mapping operator intent into effective, bug-free route announcements presents some scope for optimisation.
DeepBGP~\parencite{DBLP:conf/sigcomm/BahnasyLXC20} uses \gls{acr:es} and \glspl{acr:gnn} to generate prefix announcements for \gls{acr:as} pairs \prllitact{} from an input matrix of reachability preferences \prllitstate{}.
Each proposed solution is then graded on the number of routing constraints upheld \prllitreward{}---training continues until a solution is found meeting all constraints.
There are, of course, caveats to solving what is fundamentally a \gls{acr:csp} in this manner.
\gls{acr:smt} solvers produce outputs faster than DeepBGP takes to train, and it is unclear whether any transferable properties of an input instance are learned even though raw inference time is faster\sidenote{Given that the input/output formats depend on both the high-level intent and the \gls{acr:as} relationship graph, the model architecture is intrinsically tied to the given problem.}.
As with other \glspl{acr:csp}, heuristic solvers are unable to assert whether the input problem is unsatisfiable (and if so, whether the number of constraints have been met is maximal).

\paragraph{Flow/Packet Classification}
Identifying the type of traffic carried in a flow is a key part of ensuring \gls{acr:qos}/\gls{acr:qoe} guarantees, traffic optimisation, and network security.
However, the realities of Internet traffic require that classification is \emph{fast}, contrary to the inference costs typical to \gls{acr:dnn}.
Many approaches to packet classification assume we begin with a full set of enumerated rules ($\sim$\numrange{e5}{e6}) and matching priorities, making scalable lookup (i.e., significantly faster than $\mathcal{O}\left(n\right)$) a key challenge.

NeuroCuts~\parencite{DBLP:conf/sigcomm/LiangZJS19} successfully applies \gls{acr:drl} to this task.
This is, interestingly, quite different from most \gls{acr:rl} applications in that they \emph{build a decision-tree classifier} from input rules.
To handle the variable size of generated trees, for each non-terminal node the agent uses the min/max bounds of all its inputs \prllitstate{} to choose both a dimension and cutting/partition point \prllitact{}.
These are fixed-size subproblems, giving a generalised and transferable policy.
The set of classifier rules to encode is never passed in as state, only being exposed indirectly via node termination and a tradeoff score between subtree size and depth computed at completion \prllitreward.
Constructed models have the benefit of being interpretable and fully deterministic.
The most clever part of this work is that it keeps the slow \gls{acr:drl} work out of the critical path (a necessity for fast, line-rate traffic classification), while learning environment-specific behaviour.
\gls{acr:drl} is not directly suited to high-rate, low-throughput classification (nor is \gls{acr:rl} suited to classification versus \gls{acr:ml}), making this strategy particularly useful.
\emph{NeuvoMatch}~\parencite{DBLP:conf/sigcomm/RashelbachRS20} uses several trees composed of small \glspl{acr:nn} to store lookup information in a more compact way.
This effective compression offers improved latency and throughput on x86 hosts as the entirety of each model now fits into cache memory.
Rules not captured by these \gls{acr:nn} trees are looked up using a decision-tree or other standard packet classifier.
This does present a large tradeoff against the above: decision trees can be used natively in \gls{acr:tcam} hardware or admit conversions to \gls{acr:mat} structures, meaning that NuevoMatch cannot be trivially ported to network hardware.

In the case that we lack \emph{a priori} knowledge of labelling rules (but do have labelled training data), it becomes straightforward to train and apply \gls{acr:ml} for classification.
Historically, packet bodies have been useful in this task (making this a variation of \gls{acr:dpi}), investigated using i.e., $n$-gram models~\parencite{DBLP:journals/ton/YunW0Z16} and segmented~\parencite{DBLP:conf/iwqos/LiXNZX18} packets as inputs to \glspl{acr:lstm} or \glspl{acr:gru}.
This is no longer the case in the wild; a key issue nowadays is that encryption of traffic is fairly ubiquitous due to the proliferation of application-level security (HTTPS), secure transports (QUIC) and \glspl{acr:vpn}---which severely limits the input data we can glean from packets.\sidenote{This ubiquitous encryption affects the non-ML approaches we examined in ??, as well as \glspl{acr:ids} and anomaly detection use cases.}
Using headers alone, there have been successes on common datasets using Na\"{i}ve Bayes~\parencite{DBLP:conf/sigmetrics/MooreZ05}, Bayesian \glspl{acr:nn}~\parencite{DBLP:journals/tnn/AuldMG07}, \glspl{acr:cnn}~\parencite{DBLP:journals/soco/LotfollahiSZS20}, and self-attention mechanisms~\parencite{DBLP:conf/sigcomm/Xie0JDSLSX20} which have enjoyed success in natural language processing~\parencite{DBLP:conf/nips/VaswaniSPUJGKP17}.
What is often not masked, however, are application-level timing characteristics of this traffic such as patterns of up/down rates, interarrival times, and statistics gathered over traffic bursts.
This additional information makes the task tractable on e.g., \gls{acr:knn} and decision tree classifiers~\parencite{DBLP:conf/icissp/Draper-GilLMG16}, or \glspl{acr:lstm} and \glspl{acr:cnn}~\parencite{DBLP:journals/tnsm/AcetoCMP19}.
This extends towards passive \gls{acr:cca} identification: for window-based algorithms \glspl{acr:cnn} have been used to estimate the \emph{cwnd} parameter and observe its reaction to loss events~\parencite{DBLP:conf/icccn/HagosEYK18}, and modern \glspl{acr:cca} are handled using both \glspl{acr:cnn} and \glspl{acr:lstm} in DeePCCI~\parencite{DBLP:conf/sigcomm/SanderRHW19}.\sidenote{It's worth noting that this approach in particular is strikingly similar to my own \seidr{} histograms and associated use case in spite of their parallel development---I contrast the differences in input data, processing, and systems engineering in considerable detail in ??.}
There are significant issues with these approaches in practice, in spite of their impressive performance.
Inference times on one state-of-the-art design~\parencite{DBLP:conf/sigcomm/Xie0JDSLSX20} are \qty{180}{\micro\second} when accelerated using \gls{acr:gpu} offload, suggesting that throughput and latency guarantees of modern \glspl{acr:as} can't be met without aggressive sampling.
Some of these input features are also difficult to collect in-network without traffic mirroring and analysis at hosts---which already handle packets at a rate far lower than line-rate network hardware~\parencite{DBLP:conf/sigcomm/GuptaHCFRW18}---particularly relevant for encrypted traffic.

\paragraph{Performance analysis}
Bayesian optimisation using Gaussian processes has seen some degree of success in identifying unexpected performance ``hotspots'' in Open vSwitch~\parencite{DBLP:conf/nsdi/PfaffPKJZRGWSSA15} through \emph{NetBOA}~\parencite{DBLP:conf/sigcomm/ZerwasKHRKB019}, and cloud instance configuration via \emph{CherryPick}~\parencite{DBLP:conf/nsdi/AlipourfardLCVY17}.
This mirrors its successes in \gls{acr:ml} hyperparameter optimisation~\parencite{DBLP:conf/lion/HutterHL11,DBLP:conf/aaai/FeurerSH15}, as this family of techniques is effective at optimising input parameter distributions towards minimising a cost function using limited data (i.e., when there's a high monetary or compute cost to acquire each sample).
For optimisation tasks their use is straightforward, but it must be noted that hotspot identification still requires high-level operator knowledge.\sidenote{In particular, human knowledge is currently needed to show that a so-called adversarial scenario is more than just an expected scaling characteristic; not to mention the subsequent root-cause analysis.}

\subsection{Protocol Optimisation and Design}
\paragraph{Congestion control}
As discussed and introduced earlier in ??, the design of effective \glspl{acr:cca} very much remains an open topic.
The degree of diversity in networks, from long-fat Internet-style networks to dense low-\gls{acr:rtt} data centres, in buffering and forwarding behaviours of different path segments, \emph{and} the unforeseen interactions between disparate \glspl{acr:cca} mechanisms presents a huge problem space to work in.
Incorrect assumptions can have knock-on effects in not just overall performance, but in fairness of longer-lived flows to other traffic, or in catastrophic increases to the \glspl{acr:fct} of short flows.
As a result, automated \gls{acr:cca} learning is a particularly attractive prospect; more so when we recall the dominance of congestion-aware traffic (section ??).

\emph{MVFST-RL}~\parencite{DBLP:journals/corr/abs-1910-04054} uses \gls{acr:drl} to manage window-based congestion control in QUIC~\parencite{DBLP:conf/sigcomm/LangleyRWVKZYKS17}.
%For context, a congestion-aware flow's \emph{cwnd} determines how much traffic may be ``on the wire'' at any point in time, where a higher cwnd implies higher bandwidth consumption.
%Typically, the cwnd value oscillates to prevent congestion and packet loss while maximising throughput.
An agent then controls the congestion window size; incrementing, decrementing, halving, doubling, or keeping its value \prllitact{} to optimise throughput and latency \prllitreward.
In contrast with many prior \gls{acr:rl} works built on the OpenAI Gym~\parencite{DBLP:journals/corr/BrockmanCPSSTZ16}, their RL agent takes actions asynchronously by coalescing state updates over time, between action choices.\sidenote{The \gls{acr:ddos} mitigation use case I develop and describe later uses a similar trick, though this arises due to delayed \emph{reaction} times in the environment rather than inference cost. See section ??.}
Input states are comprised of \gls{acr:rtt} statistics, byte transmission and receive counts and loss information, combined with the last \num{5} actions \prllitstate.
By applying fully-connected \glspl{acr:nn} followed by an \gls{acr:lstm}~\parencite{DBLP:journals/neco/HochreiterS97} for policy approximation, this work is competitive with the state-of-the-art due to \glspl{acr:lstm}' particular suitability for identifying long-term relations in time-series data.
Their work raises again the primary drawback of applying \glspl{acr:dnn} in latency sensitive applications like \gls{acr:cca} design: they observe up to \qty{30}{\milli\second} action computation time, and have only trained agents via parallel simulation, requiring vast amounts of training data.
Moreover, MVFST-RL was unable to generate policies which are simultaneously applicable to different environmental delay and bandwidth characteristics.

\emph{DRL-CC}~\parencite{DBLP:journals/jsac/XuTYWX19} examines how one \gls{acr:rl} agent can jointly optimise \gls{acr:mptcp} subflows and \gls{acr:tcp} flows.
\gls{acr:mptcp} differs from traditional transports by allowing data segments in a single logical connection to be sent over several interfaces, who naturally then have their own per-subflow congestion control in addition to shared co-ordination.
The state of any (sub)flow is its rate, goodput, \gls{acr:rtt} statistics, and congestion window size.
DRL-CC passes all current states into an \gls{acr:lstm} to obtain a fixed-size representation for all flows, which is then combined with the overall state for a target flow \prllitstate.\sidenote{This \gls{acr:nn} architecture is often known as a \emph{two-headed network}. This allows end-to-end training of a feature extraction network and downstream \glspl{acr:nn} (in this case, the actor and critic networks). Training of the actor and critic component networks jointly improves the base feature extractor.}
Using actor-critic methods, an \gls{acr:nn} produces a vector of congestion window deltas for all the target flow's subflows \prllitactreal, conditioned on the sum of log-goodputs of live flows \prllitreward.
Inference latency is kept to a moderate \qty{0.5}{\milli\second} using the \gls{acr:cpu}, and performance is comparable to classical \gls{acr:mptcp} \glspl{acr:cca} on lossy networks---where a high packet loss of \qtyrange{0.5}{4}{\percent} can be justified by the focus of \gls{acr:mptcp} on cellular networks.
While it is shown to be fair to itself, it's unclear how DRL-CC multiplexes with other \glspl{acr:cca}.

The \emph{PCC} family of \glspl{acr:cca}~\parencite{DBLP:conf/nsdi/DongLZGS15,DBLP:conf/nsdi/DongMZAGGS18}, Copa~\parencite{DBLP:conf/nsdi/ArunB18}, and their \gls{acr:mptcp} variant \emph{MPCC}~\parencite{DBLP:conf/conext/GiladSGRS20} offer a control-theoretic perspective on effective congestion control, improving on heuristic methods.
These approaches combine flow throughput, loss, latency and goodput for each (sub)flow into a single utility score, choosing target rates which maximise this score via simple gradient ascent.
Although this branch of research doesn't \emph{learn} any function approximation, the fact that operational modes and behaviours are all well-defined allows for convergence to be proven under typical network conditions.

\emph{Aurora}~\parencite{DBLP:conf/icml/JayRGST19} then modifies rate selection in the PCC framework to use a simple fully-connected \gls{acr:nn}, trained using the PPO~\parencite{DBLP:journals/corr/SchulmanWDRK17} algorithm.
It computes multiplicative increases or decreases to a flow's send rate \prllitactreal{} given an $m$-long history of latency statistics and loss rates \prllitstate.
The agent then acts to maximise packet-per-second rate, penalising latency and loss \prllitreward.
By keeping the explicit operational modes of the PCC family, the policies it learns from offline training \emph{do} effectively generalise to unseen network characteristics and designs.
However, this formulation was later shown to be unfair to other \glspl{acr:cca}~\parencite{DBLP:conf/sigcomm/AbbaslooYC20}.

\emph{Orca}~\parencite{DBLP:conf/sigcomm/AbbaslooYC20} eschews the ``clean-slate'' approach common thus far, using a classical \gls{acr:cca} (\gls{acr:tcp} Cubic) as its basis.
This decision is empirically and strongly motivated; doing so greatly simplifies the learning task for an \gls{acr:rl} agent (improving the learned policy) \emph{and} reducing \gls{acr:cpu} and \gls{acr:gpu} utilisation in deployment.\sidenote{Recall that \glspl{acr:cca} almost always control how data is \emph{sent} across the network, and that clients typically send small requests for servers to transmit larger content. This leaves the burden of performing expensive per-packet and per-flow operations with the server, which by this same assumption has to handle many such flows!}
Orca tracks $m$-long histories of a flow's current (and best) throughput and \gls{acr:rtt} information alongside its loss rate and congestion window \prllitstate.
Using the TD3 actor-critic algorithm~\parencite{DBLP:conf/icml/FujimotoHM18}, Orca chooses some $\alpha\in\left(-2, 2\right)$ every \qty{20}{\milli\second}, multiplying the congestion window by $2^\alpha$ \prllitactreal, and allows the baseline classical \gls{acr:cca} to otherwise act as normal.
Each flow acts to improve the current ratio between its current \emph{power} and the best estimate of the Gail-Kleinrock optimal operating point~\parencite{KleinrockPoint1,KleinrockPoint2}---with some tradeoffs to minimise loss and allow small \gls{acr:rtt} variance \prllitreward.
While this naturally requires higher resource use than a heuristic method such as Cubic or BBRv2, this strategy reduces resource costs beyond even the control-theoretic PCC family of \glspl{acr:cca} (with better, fairer operation).
Reducing the length of time between DRL actions predictably increases resource demands, but leads to better flow performance.

\paragraph{Media access control}
An exciting, perhaps unexpected, network environment is within \glspl{acr:cpu} themselves---a \emph{network on a chip}---for coordination in multi-threaded programs and ensuring cache coherency in many-core architectures.
This design class is necessitated by the limitations of a shared bus at high core counts.
Core-to-core communication is either packet-switched using local routers (incurring latency costs) or wireless (potentially leading to collisions).
\emph{NeuMAC}~\parencite{DBLP:conf/nsdi/JogLFFATH21} approaches optimal wireless transmission via \gls{acr:drl}.
Training of a small fully-connected \gls{acr:nn} occurs offline from simulation, using the REINFORCE algorithm as a Monte Carlo method with complete execution traces.
In deployment, an agent is quantised to \qty{8}{\bit} fixed-precision values on low-latency \gls{acr:sram}\sidenote{See \cref{sec:numerical-representations-for-embedded-ml} for an in-depth discussion around the topic of embedded \gls{acr:ml} design decisions such as this.}
Each core has a dedicated transmission timeslot, while the agent chooses a list of per-core probabilities every \qty{10}{\micro\second} to allow transmissions outside this window \prllitactreal{}, which are halved on a collision.
An agent passively listens to broadcast signals for the successful transmissions per core and total number of collisions observed \prllitstate{}, minimising the cycles spent running a program to completion \prllitreward{}.
%?? Quantises the final policy, shows that small NNs can be encoded in hardware with small latency (\qty{512}{\nano\second}), small power draw (\qty{1}{\milli\watt}) (parallel multiply-accumulate units, low-latency sram, dedicated circuit for small FCNN)
Interestingly, this shows that small, pre-trained, quantised \glspl{acr:nn} can be placed into core hardware control loops at low latency (\qty{512}{\nano\second}) and low power draw with \emph{bespoke} designs.

%* https://dl.acm.org/doi/10.1145/3405671.3405817 -- An Adaptive Tree Algorithm to Approach Collision-Free Transmission in Slotted ALOHA

\subsection{Security, Defence, and Verification}
\paragraph{Network and Computer Defence}
\emph{Marl}~\parencite{DBLP:conf/iaai/MalialisK13,DBLP:journals/eaai/MalialisK15} examines the automated detection and mitigation of \gls{acr:ddos} attacks using the Sarsa \gls{acr:rl} algorithm.
As a multiagent system, Marl agents are distributed at the edges of a network and adaptively learn a policy to control traffic \emph{without} explicit communication or sharing of policy updates.
Agents reside at the \gls{acr:as}'s ingress points, and choose a packet drop probability for \emph{all inbound flows} from the discrete choices $a\in\left\{0.0,0.1,...,0.9\right\}$ \prllitact{} according to load measurements along their route to the protected server \prllitstate.
They create a tree overlay topology, subdivided into teams which each receive a separate reward measurement.
This aids credit assignment by not punishing teams which contribute little to the total incoming bandwidth.
Agents are punished when the network is overloaded \emph{and} their team contributes more than its fair share of traffic, otherwise receiving the proportion of legitimate traffic observed at the team leader \prllitreward.
%?? Recap their flaws, since they've been cut form every other aspect.
%Our results show that their technique underperforms at high host density and when congestion-aware traffic dominates---that their results do not demonstrate this suggests an evaluation driven purely by traces (rather than live application dynamics).
%?? Mention why congestion-aware traffic gets screwed.
Although their results appear competitive, their simulation environment uses only congestion-unaware \gls{acr:udp} traffic, in opposition to the realities of Internet traffic as discussed in ??.
Congestion-aware protocols dominate in many networks; incorrectly applying a packet drop action imposes greater \emph{pushback}~\parencite{DBLP:journals/ccr/MahajanBFIPS02a} on these legitimate flows than it would on attack traffic.
For congestion-aware traffic, this is non-negligible; when packet loss occurs with probability $p\ne0$, the Mathis equation~\parencite{DBLP:journals/ccr/MathisSMO97} states that TCP bandwidth is proportional to $1/\sqrt{p}$ (while modern TCP Cubic is proportional to $1/p^{0.75}$~\parencite{rfc8312}).
Congestion-unaware, \gls{acr:cbr} traffic then occupies bandwidth proportional to $1 - p$, and recalling section ?? we understand from the literature that volumetric \gls{acr:ddos} attack traffic mainly falls into this category.
%Congestion-unaware, \gls{acr:cbr} traffic then occupies bandwidth proportional to $1 - p$---when we consider that analysis of CAIDA datasets~\parencite{caida-2018-passive} shows that congestion-aware traffic makes up at least \SIrange{73}{82}{\percent} of packets\footnote{\url{https://github.com/FelixMcFelix/caida-stats}}, it is clear that collateral damage applied by Marl is greatly worsened.
%?? Reward measurement relies on perfect a-priori knowledge.
Furthermore, the static overlay topology does not account for the defence of load-balanced or multipath networks, and the reward function relies on \emph{a priori} knowledge of traffic (or an accurate heuristic---an open challenge).
These weaknesses are shown more concretely throughout section ??, and motivate the design of \{name\} throughout Chapter ??.
%?? I investigate this much further in-depth in section ??.
%?? Sim issue: All UDP

Other \gls{acr:ml} techniques have been applied to \gls{acr:ddos} detection in the context of \glspl{acr:sdn}.
\textcite{DBLP:conf/lcn/BragaMP10} have shown that \emph{self-organising maps} (an unsupervised, \gls{acr:nn}-based approach) can act as effective classifiers from flow statistics given ample captures of both normal and attack behaviour.
\emph{Athena}~\parencite{DBLP:conf/dsn/LeeKSPY17} improves on this through a more generalised (albeit heavyweight) \gls{acr:sdn} framework for intrusion detection, showing the use of \emph{k-means clustering} to detect individual attack flows.
However, their comparison against modern \emph{algorithmic} \gls{acr:ddos} defence techniques such as SPIFFY~\parencite{DBLP:conf/ndss/KangGS16} lacks any quantitative evidence.

Most modern malware makes use of evasion techniques or alters its behaviour to appear more benign in the presence of dynamic analysis, such that understanding the behaviour of malware (particularly those with self-modifying code) becomes more difficult for security analysts.\sidenote{This problem long predates the class of evasion attacks on \gls{acr:ml} models mostly considered throughout \cref{sec:attacks-on-data-driven-techniques}. Evasive malware relies more on introducing \emph{semantic} or \emph{behavioural} differences rather than abuse of decision boundaries in high-dimensional spaces.}
\emph{TAMALES} makes use of this principle to great effect~\parencite{DBLP:conf/acsac/CoptyDEEMZ18}; where most analysis tools aim to mimic a real OS as closely as possible, their ``extreme abstraction'' relies upon deviating from specifications and expected behaviour to induce anomalous behaviours.
Using \emph{random forest classifiers}~\parencite{DBLP:journals/ml/Breiman01}, they combine static program features with dynamic behaviours observed from buggy \gls{acr:os} emulation.
The most interesting (and general) feature of this design is that more expensive features and analyses are added to the classifier over time while the output classification remains ambiguous.
However, this solution regularly marked benign programs which had been processed by an executable packer as though they were malicious.

\Textcite{DBLP:conf/networking/QinPLT20} attempt to combine the distributed training offered by \gls{acr:fl} with the recent advances in \gls{acr:bnn} use in the dataplane (\cref{sec:numerical-representations-for-embedded-ml}) for attack traffic detection.
P4-capable edge switches or \glspl{acr:nic} host a \gls{acr:bnn} computed from a local (full-precision) model trained on a co-hosted machine, which communicates model updates to and from a central parameter server as is common in \gls{acr:fl}.
Their work supports the hypotheses that \glspl{acr:bnn} achieve sufficient accuracy on existing \gls{acr:ids} datasets and that overall model convergence makes \gls{acr:fl} suitable for this type of data.
However, this work neither mentions nor considers the central limitation of \gls{acr:fl}; that edge models need some local means of generating labels for new data.\sidenote{\gls{acr:fl} can still be used to train \emph{unsupervised methods} without a local oracle, but clustering and forecasting have limited application for this class of flow filtering.}
As a result, it's not clear whether \gls{acr:fl} is even a suitable choice for this task, and so this remains far from a feasible system.
A core element of the design---that the P4-enabled device can add rules to its own tables from inside the RMT pipeline---is in fact impossible on reasonable hardware implementations as I discuss in section ??.
This is approximated using a \num{65535}-entry register file and primitive flow-key reduction into this space.
Even the claim of line-rate operation is unsubstantiated, given that the evaluation focusses only on a software-based BMv2 implementation (costing a heavy \qtyrange{10}{25}{\milli\second} per-packet classification latency) and fails to quantify the costs of flow state extraction.

\paragraph{Verification}
\emph{\textsc{P4rl}}~\parencite{DBLP:conf/sigcomm/ShuklaHH019} applies \gls{acr:drl} (DDQN) towards guided fuzzing of complete P4 dataplanes.
Fuzzing (as opposed to static analysis) allows for the detection of bugs that lie outside of the P4 language itself, e.g., through interactions with the control plane, or in hardware-specific behaviour.
The key drawback of fuzzing without some manner of guidance, however, is the colossal size of the input value state space.
Beginning with a set of invariants extracted from their \emph{p4q} \gls{acr:dsl}, \textsc{P4rl} iteratively modifies the header bytes of an output packet, starting from an initially valid state \prllitstate{}.
The \gls{acr:rl} agent then chooses a field and value pair (choosing either random values or boundary values known from \emph{p4q}) \prllitact{}, conditioned on whether that packet violated any given invariant \prllitreward{}.
This notably improves on the number of packets needed to trigger any bugs versus a random baseline, but it remains unclear whether this reduces the wall-clock time needed to output such a packet.

\emph{DeepMPLS}~\parencite{DBLP:conf/networking/Geyer019} applies \glspl{acr:gnn} towards network verification in the face of link failures for \gls{acr:mpls} routed networks.
\gls{acr:mpls} is commonly used in \gls{acr:isp} networks~\parencite{DBLP:conf/imc/VanaubelMPD15}, and has comprehensive (though slow) tools for discovering routing violations given complex predicates~\parencite{DBLP:conf/conext/JensenKM0ST18}.
Given that \glspl{acr:gnn} can be applied to variable-size input graphs\sidenote{Note that the vertex attributes remain fixed-size vectors in the inout graph.}, this allows for a useful model to be trained over many instances.
This offers both a 2 order-of-magnitude speedup over conventional solvers, \emph{and} enables the new use case of suggesting additional links and actions to meet the constraint.
The main caveat is that outputs are only \qtyrange{80}{90}{\percent} likely to be valid, while an algorithmic solution is correct by construction.

\subsection{Multimedia}

\paragraph{ABR Video Selection}
Streaming video is a common use case in the modern Internet.
Here, users typically want to receive the highest quality video they can, while minimising any noticeable quality changes and the amount of time spent \emph{rebuffering}, which are their core \gls{acr:qoe} metrics.
Servers allow clients to control this via \gls{acr:abr} selection, splitting videos into many fixed-length chunks (of \qtyrange{4}{10}{\second}) served via \gls{acr:hls}~\parencite{rfc8216} or \gls{acr:dash}~\parencite{mpeg-dash}.
However, chunk selection is delegated to the client using heuristic approaches such as MPC~\parencite{DBLP:conf/sigcomm/YinJSS15}. 
An exciting question to consider is whether data-driven metrics can do better still.

\emph{Pensieve}~\parencite{DBLP:conf/sigcomm/MaoNA17} applies \gls{acr:drl} to client-side observations of network state and video performance metrics as a state space for effective optimisation of bitrate selection in multimedia streaming.
Using the A3C algorithm with \glspl{acr:cnn}, throughputs and download times for the last $k$ chunks are combined with current chunk and buffer length statistics \prllitstate{} to choose the next chunk's quality from the standard list (masking any illegal choices) \prllitact{}.
Pensieve acts to maximise an aggregate \gls{acr:qoe} score, maximising quality\sidenote{This can be a linear or log-linear function of actual bitrate, or can explicitly penalise non-HD choices.}, while penalising bitrate changes and the time spent rebuffering \prllitreward{}.
To reconcile the costs of \gls{acr:drl} inference with mobile devices, Pensieve is server-hosted and periodically queried by clients (though it remains effective even under $\sim$\qty{100}{\milli\second} \gls{acr:rtt}).

\emph{Stick}~\parencite{DBLP:conf/infocom/HuangZZW0S20} uses the DDPG \gls{acr:drl} algorithm to train a smaller \gls{acr:cnn} to provide the target buffer occupancy used by heuristic, buffer-based chunk selection methods \prllitactreal{}.
The main rationale for doing this is to reduce runtime execution costs and retain the interpretable behaviour of traditional \gls{acr:abr} strategies.
Stick uses the same input as Pensieve, adding in the current reward \prllitstate{}, optimising the (linear) \gls{acr:qoe} score discussed above \prllitreward{}.
To further reduce inference costs, a very small \gls{acr:cnn} is used to estimate whether the current state is likely to cause a large change in the buffer's target occupancy.
Overall, this leads to slightly better client \gls{acr:qoe}, and offers far lower execution costs: completely removing the impact of server \gls{acr:rtt} as all rate selection can be managed on-device.

\emph{PERM}~\parencite{DBLP:conf/infocom/GuanZWBXS20} considers this problem over \gls{acr:mptcp} connections, via \gls{acr:drl} on standard feed-forward \glspl{acr:nn}.
Modifying Pensieve's state to use per-\emph{port} throughputs over $k$ timesteps \prllitstate, PERM
chooses both the next chunk's bitrate and traffic splitting proportions over registered links \parenglance{$\rllitactraw\times\rllitactrealraw$}.
By optimising the linear \gls{acr:qoe} score with added per-link cost penalities \prllitreward{},
they reduce the use of high-cost links (e.g., 4G).
However, their evaluation is unclear in how \emph{only} the underlying video \gls{acr:qoe} is affected when link costs are disregarded.

\paragraph{Server- and network-driven QoE enhancements}
\emph{LiveNAS}~\parencite{DBLP:conf/sigcomm/KimJYYH20} extends recent work on offline \gls{acr:ml}-driven video upscaling towards live content.\sidenote{Optimal upscaling relies on having a content-specific \gls{acr:dnn}, as generic models can still generate noticeable errors compared to the input stream. Successful online training of such a model from incomplete data is an exciting challenge.}
The main value in doing so is that \gls{acr:ml} can be applied to both increase user \gls{acr:qoe} and reduce upstream bandwidth requirements in livestreaming (i.e., in the event that a popular content provider is limited by their own network).
Server-side upscaling is provided when high-quality video is needed but the sender cannot offer, training a per-stream \gls{acr:dnn} model.
LiveNAS solves the local training problem (i.e., of acquiring ground-truth data) by having each sender also encode their stream to a higher quality level than their connection can support.
Small high-quality patches with maximal error versus a bilinear upscale are included alongside the lower-quality stream, acting as valuable ground truth for the model to learn from at moderate bandwidth cost.
This offers strong \gls{acr:qoe} improvements, low deviation from the true input video, and can in principle cut the bandwidth requirements for high-profile streamers.

\Textcite{DBLP:conf/conext/ManglaHZA20} use several \gls{acr:ml} techniques to investigate whether \gls{acr:isp} or other transit networks are able to estimate video session \gls{acr:qoe} using cheaper input state such as \gls{acr:tls} session lifetimes and flow-level measurements.
The detection of \emph{low} \gls{acr:qoe} scores would allow e.g., cellular networks, to provision greater bandwidth or prioritisation to multimedia flows which require it.
These methods are most effective at splitting low- and high-\gls{acr:qoe} flows (with a high degree of confusion in middling flows), suggesting that these could be a stage-1 metric to enable more expensive per-packet analysis.

\emph{Alohamora}~\parencite{DBLP:conf/nsdi/KansalRN21} uses \gls{acr:drl} to generate \gls{acr:http}/2 asset push and preload policies to reduce page load times over limited networks or on constrained devices.
The approach trains offline using \glspl{acr:lstm} with A3C by grouping page families into clusters, and inferring policies at runtime as needed.
Link capacity and \gls{acr:rtt} statistics, client \gls{acr:cpu} capacity, and the target page's resource dependency graph \prllitstate{} are used to output a sequence of push/preload item and prerequisite pairs until an end-token or illegal state is output \prllitact{}.
Alohamora optimises the relative \gls{acr:qoe} change according to cheap and accurate simulations, with explicit bonuses added whenever a better incumbent policy is produced \prllitreward{}.
While considerably more effective than past works on policy generation, the ablation studies shown by the authors indicate a strong dependency on device-specific state (especially the \gls{acr:cpu} speed of each client).
Inference is cheap compared to page load times, around \qtyrange{11}{40}{\milli\second}, offering strong \gls{acr:qoe} improvements when all input data are known.

\subsection{Resource Placement and Management}

\paragraph{Job Scheduling}
\emph{DeepRM}~\parencite{DBLP:conf/hotnets/MaoAMK16} is one of the first works on simple \gls{acr:drl}-based job scheduling among a finite number of resource-constrained \glspl{acr:cpu}, aiming to minimise the average job slowdown.
What is particularly notable about this work is that it employs intelligent sampling and monitoring while taking multiple actions per timestep.
In particular, it maps pixel images of current resource use and the costs of the next $k$ jobs \prllitstate{} into a discrete set of job choices to schedule and a null action \prllitact.
The timestep is advanced on either an illegal or null action, giving an agent a negative reward for all incomplete (arrived) tasks \prllitreward.

In reality, scheduling of (data-parallel) jobs is far harder; these are often expressed as a \gls{acr:dag} of sub-tasks with inter-connected data dependencies.
\emph{Decima}~\parencite{DBLP:conf/sigcomm/MaoSVMA19} applies \glspl{acr:gnn}-based \gls{acr:drl} to completely control job scheduling as part of Apache Spark.
To minimise the average \gls{acr:jct} \prllitreward, Decima chooses the next job stage to schedule and the number of workers to be spawned \prllitact{} in response to any scheduler events, until all jobs are assigned or executors are busy.
Agents use the output embeddings of nested \glspl{acr:gnn}, processing per-task and executor statistics into job- and system-level summaries \prllitstate.
To make training feasible\sidenote{The Monte Carlo REINFORCE algorithm needs complete execution traces to update an \gls{acr:rl} policy, but excessively poor policies could take far longer than realistic runs to terminate.}, episodes are modified to end early in initial training phases.
Equally, in any scenario job arrival times are perturbed (maintaining arrival \emph{order}) to prevent excessive punishment due to bursty arrivals.
By using smaller \glspl{acr:nn} at each stage, each decision can be made in around \qty{15}{\milli\second}.

Distributed \gls{acr:ml} model training is a variation on this problem, typically having high bandwidth costs and \glspl{acr:jct} which exist in a trade-off with final model accuracy.
\emph{MLFS}~\parencite{DBLP:conf/conext/0002LS20} cleverly operates by starting with a heuristic approach to gather samples for \gls{acr:drl} training---initially prioritising jobs with faster \glspl{acr:jct} or expected accuracy improvements (i.e., fresh training jobs).
The agent chooses a set of task-to-executor pairs \prllitact{} from the full set of task resource demands and parameters alongside executor utilisation \prllitstate.
Agents act to optimise a weighted combination of minimised average \glspl{acr:jct} and bandwidth, and maximising average (and sufficient) accuracy and jobs completed before their deadline \prllitreward.
The \gls{acr:rl} model is used in place of the heuristic after it successfully converges, and simple statistical methods are used to terminate jobs whenever overfitting appears to begin.
However, MLFS's execution costs aren't specified, and it remains unclear how it handles (what appear to be) variable-size inputs and outputs.

Some works choose to focus on the simpler (though important) task of parameter optimisation for existing schedulers.
The degree of parallelism offers one such `knob' to tweak in data-parallel job allocation, however it is not one which universally leads to performance gains when increased.
In partition-aggregate workloads, coordination overheads dominate if a task is divided between too many workers---\emph{ReLoca}~\parencite{DBLP:conf/infocom/HuLZC20} successfully trains \glspl{acr:dnn} to predict job completion from a given worker count and \gls{acr:dag} statistics, using a novel sampling method to concentrate training around optimal choices.
In the case of independent jobs (e.g., replicated services), \emph{Autopilot}~\parencite{DBLP:conf/eurosys/RzadcaFSZBKNSWH20} optimises vertical scaling (the \gls{acr:cpu} and \gls{acr:ram} limits allocated for a task) and the number of workers to minimise user spend below heuristics.
In the former case, an ensemble of simple optimisers (differing by cost model) is used to provide an interpretable suggestion, in the latter case a user-specified strategy is applied to minimise resource use.

\paragraph{Cache Management}
Resource caching of Internet resources (e.g., webpages or video) is commonly employed by \glspl{acr:cdn} to serve content to users in a way which minimises latency as well as offering load-balancing for content providers.
\emph{RL-Cache}~\parencite{DBLP:journals/jsac/KirilinSGS20} offers a cache admission policy learned through \gls{acr:drl} such that the hit rate is maximised.
When a resource is requested, RL-Cache chooses to admit or remove that item from the cache \prllitact{} based on that objects's size, recency and frequency statistics \prllitstate.\sidenote{Somewhat curiously, the authors choose to quantise these measures into fixed bins; effectively using a one-hot encoding of bin hits for each statistic as the input.}
The reward is simply 1 or 0 (hit or miss) per-object in the next batch, divided amongst the previous $k$ decisions \prllitreward.
Although it is effective after the authors reduce the runtime cost by performing inference only on cache misses, batching is required to meet any reasonable level of throughput.
This comes at a cost of latency; a totalled \qty{65}{\milli\second} for a batch of size \num{1024}, comparable to \glspl{acr:rtt} in the best case (and with tail latencies left unspecified).

\emph{MacoCache}~\parencite{DBLP:conf/infocom/Wang0LSS20} examines a more targeted form of resource caching at telephone base stations via multi-agent \gls{acr:drl} to minimise latency and bandwidth demands in mobile edge networks \prllitreward.
Agents estimate a cache probability for every video file, choosing the top $k$ entries \prllitactreal{} based on per-item demand rates and cache status \prllitstate.
Agents don't share information directly, but do receive a portion of their neighbours' rewards and use neighbours' policies and cache state as further inputs to account for their impact on the overall system.
Although \glspl{acr:lstm} are used to allow variable-size processing of local and neighbour state vectors, one element which remains suspect is that each node's state (and the model's output) contain a value \emph{for each cacheable item}.
Given that the total number of cached video files is never specified, this does not suggest great scalability; this is only worsened when we consider that such resource is explicitly said to be a \gls{acr:dash} chunk (thus, hundreds per video).

\paragraph{System and Network Planning}
In network planning, \glspl{acr:ilp} are often employed for short- and long-term planning of fibre placements and \gls{acr:ip} route provisioning.
\emph{NeuroPlan}~\parencite{DBLP:conf/sigcomm/ZhuGATZJ21} uses \gls{acr:drl} to suggest a better starting point and prune the \gls{acr:ilp} search space for this problem, greatly reducing runtimes from \qtyrange{3}{4}{\day} in hypergiant networks.
The variable-size and structure of networks make graph convolution~\parencite{DBLP:conf/iclr/KipfW17} a natural fit, learning to optimise the normalised cost of any new bandwidth provisioned \prllitreward{} through actor-critic methods.
Given the line graph transformation~\parencite{Harary1960} of the network with \gls{acr:ip} route capacities as edge labels \prllitstate, the agent chooses both a link and the number of discrete capacity units to add \prllitact.\sidenote{Agents are prohibited from removing capacity, aiding learning by making it impossible to regress into an illegal state.}
Training is accelerated by stopping each episode once either the constraints are met or too many steps have elapsed, and the final link weights are used as new maxima for each \gls{acr:ilp} variable---this state-space pruning accelerates the \gls{acr:ilp} by \qtyrange{7}{14}{\times} while achieving similar total costs.

Chip floorplanning, the process of placing and interconnecting \glspl{acr:fu} for fabrication, can equally be considered as a resource placement, routing, and networking problem.
\Textcite{Mirhoseini2021} show that \gls{acr:drl} can successfully learn to output compliant designs in around \qty{6}{\hour} of datacentre training, compared to months of human iteration.
Using Edge-\glspl{acr:gnn} and PPO~\parencite{DBLP:journals/corr/SchulmanWDRK17} to process hypergraphs of macros, standard cells, and their interconnections, an \gls{acr:rl} agent places macros onto a masked \numproduct{128 x 128} grid \prllitact, from large to small.
The feature extraction part of the network processes the complete hypergraph, the dimensions of  each macro node, required connections, and associated metadata as inputs for the policy network and value network \prllitstate.
An agent receives a single reward at the end of an episode: a negative weighted sum of wirelength, congestion and density \prllitreward.
Effective and fast, this technique has been put into action in the design of forthcoming tensor accelerator hardware---interestingly, it appears to learn more `radial' macro placements than humans often consider.
A more generally useful insight of their work is that completed placements are easy to estimate a reward for; as such, it is possible to bootstrap the learning of a feature extraction network as a supervised, offline problem.

\subsection{Takeaways for effective data-driven networking}\label{sec:ddn-use-takeaways}

Hopefully, it should be apparent that a consistent feature ...
?? Decisions taken with reduced frequency, out of the critical data path
?? Per-packet/flow are mostly restricted to end-hosts.

?? Common drawback here is that inference, while quite fast, is too slow 

?? Any general points on training duration I can/should make?

?? NN exec time is huge, due to GPU offload (cite sources for this)
?? Use the worked example from the OPaL paper to illustrate why this is bad (or worse) the closer you go to the metal?

?? Using existing heuristics/algorithm as a basis or teacher can be a free win
?? Not doing inference all the time in the name of not killing your servers.

?? Keep huge exec times out of the packet path
?? Powerful to produce the structure/params for a more efficient algorithm to use.

?? Feature extraction layers can be trained ahead-of-time if they are separable from policy (i.e., entry point to an actor-critic) and enough samples exisdt w/ easy reward estimation.

?? Common need to diverge from ``standard'' RL formulation to aid learning according to characteristics of environment. Though rarely justified in an analytic way, but can overcome roadblocks like reward arrival patterns etc.
?? Necessary in many domains to act asynchronously as a result; combine and coalesce state data as it comes in, and then have action computation act upon a view of this data (still gather stats in the interim, however).

?? Input, output and model shape should ideally be kept at a fixed size irrespective of the target deployment, to allow not just deploying in these other environments but also training from as many separate systems as possible! (and enable model combination/sharing)
?? When is it fine to do otherwise? Attempting to solve a CSP (i.e., similar to genetic algos, ).
?? trade-offs.
?? break var-width problems into fixed-size chunks if possible.
?? Note: GNNs are great here! This is a novel and surprising development I failed to keep an eye on.
?? LSTMs useful too, but more suited for sequences (so sensitive to/can learn form order of input samples).

?? Model architecture should be kept to as small/compact a representation as possible; not only fewer params to learn, but also fewer operations in inference --- runtime perf!
?? N-square scaling okay in some environments: take care based on the problem!

?? Nature of classifiers/fn approxs used can limit where an agent/accelerator can be deployed.

?? Cheap initial analyses can be used to drive more expensive analyses when the extra data is needed to make effective decisions (but too expensive to collect in general).
