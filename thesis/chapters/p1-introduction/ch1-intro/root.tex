\chapter{Introduction}\label{chap:intro}

%\epigraphhead{\epigraph{Insert thoughtful quote here.}{Some author---\emph{Some Book}}}

%?? PROBLEM
Computer networks are complex, yet critical infrastructure.
The Internet of today is a prime example.
From its birth as \emph{ARPANET}~\parencite{DBLP:conf/afips/HeartKOCW70}, interlinking the computers of research institutions under sole administration, it has scaled to connect together billions of devices over chains of smaller networks owned by totally separate entities.
This growth has come at a cost; large-scale networks are beset with layers upon layers of interlocking and overlaid systems divided amongst endpoint hosts and operators of the network fabric.
The isolated, resilient design of all these layers is a strength in itself as these layers can, in theory, be replaced.
Yet owing to this complexity, keeping the `Internet machinery' well-oiled and performant is a difficult task.

As these networks have grown larger and faster over the last half-century, they have become more flexible to do just this.
Early design choices had been bonded to fixed-function hardware, such as routing algorithms.
To escape these shackles, the community sought through the early 2000s to separate the high-level forwarding behaviour of network packets (the \emph{control plane}) from the hardware dataplane, giving rise to \gls{acr:sdn}.
This is a research success that has meaningfully impacted the design, adaptability, performance, and fault-tolerance of production networks.
For instance, making routing decisions per-flow has enabled \gls{acr:te} that otherwise requires complex workarounds.
What this did not solve was inflexible \emph{dataplane} behaviour; switches still supported a fixed set of actions applied to a fixed set of protocols known ahead-of-time, with limited shared state for measurement purposes.
Many packet actions---security functions and the like---were and are implemented as \glspl{acr:asic} inside \emph{middleboxes} to process traffic at line rate.
Infamously, these add yet more inflexibility by relying on (possibly incorrect) handling of known protocols.
Making up for this shortfall in malleability by implementing these tasks in host machines causes significant, orders-of-magnitude reductions in packet throughput and added latency.
Between fixed-function dataplanes and commodity hosts, there is no way to balance performance and arbitrary programmability.
While it is one thing to argue around performance and ossification, what of new functionality?
To observe incorrect (yet transient) paths followed by packets, or to inspect and aggregate queue state and nanosecond-level timestamps to detect recent issues such as \emph{microbursts} both require packet modification and access to state which past switches simply do not allow.
%For the longest time, , and so the dataplane must evolve.
%?? such as, while adverse performance events like \emph{microbursts} 
%??? traffic scales 
%?? need a pdp problem -- measurement? -- modification? -- new problems like microburst dependent on speed \& workload? --


%(why? larger? more clients?) Control plane, \gls{acr:sdn}. ?? step away from limits and hard, fixed impls? ?? avoid being shackled to archaic design choices
%?? SDN proved `layers being replaced' as above.
%?? TE/TO trying to work around it
%?? need a pdp problem -- measurement? -- modification? -- new problems like microburst dependent on speed \& workload? -- 

Many other aspects of networks are run by heuristic methods: each complex, hand-tuned, and operating on limited information.
Their responses must be both approximately correct \emph{and} computationally cheap to compute to scale reasonably.
To make the point clear, consider \glspl{acr:cca}.
\glspl{acr:cca} underpin the majority of Internet traffic's ability to dynamically scale send rates up or down---in the absence of actual network state, they must in effect reverse-engineer optimal actions by following a proxy metric such as delay or packet losses.
%?? effectively reverse-engineering network state by a proxy metric
While this is impressive, one can't help but ask if better information about the network itself could allow more useful decision making.
What also happens, then, as networks change? % holy shit iambic pentameter
Experience has shown that we simply iterate, from NewReno~\parencite{rfc6582} to Cubic~\parencite{rfc8312} to BBR~\parencite{DBLP:journals/queue/CardwellCGYJ16}\sidenote{Deployed circa 1999, 2006, and 2016 respectively.} in long-fat networks.
But even after years of design and tuning, these are easy to get subtly or fatally wrong; initial actively-deployed BBR versions were notably unfair to other flows~\parencite{DBLP:conf/imc/WareMSS19}.
The takeaway is that it's infeasible to develop and hand-tune strategies per workload, per topology, and per protocol distribution.
Should we not be able to automatically infer tailored mechanisms or parameters, robust to changes and evolution, from local performance and global management data---a \emph{data-driven} solution?
%?? better info? and machines/stats can do better---\emph{data-driven} solution.
\glspl{acr:cca} are but one case where we should ask ourselves these questions.
Consider general optimisation of network protocols and infrastructure, or protection against the abuse of network resources such as \gls{acr:ddos} attacks, or even the above dataplane measurement---equally strong candidates to consider whether \emph{data-driven} logic and \emph{network cooperation} can lead to meaningful improvement.

%?? divided among endpoints and the network, TE/TO trying to work around it...
%?? routing policies built on black magic
%?? need for flexibility: don't want to be shackled to the decisions of 1980s
%?? cursed with the awareness of the fragility.
%?? why we want DDN: in combination, many aspects of networks complex, heuristic, hand-tuned \glspl{acr:cca} etc. operating on limited information
%?? optimisation, protection against abuse of network resources such as \gls{acr:ddos} attacks.

%?? TOOLS PDP
%As these networks have grown larger over the last half-century, become more flexible to... (why? larger? more clients?) Control plane, \gls{acr:sdn}.
%?? such \gls{acr:pdp} hardware can...
%?? install new approaches at runtime, as net evolves.
%?? difficulties of line-rate, latency

%?? Even with the advances above, network can't really expose better info because it has one job.
%?? Coop means reprogrammability, capability to express general logic.
While these aims are lofty, the last decade has seen surprising and rapid kinds of change, first of all in the design and introduction of \emph{programmable} switching hardware and \glspl{acr:nic}.
\gls{acr:pdp} switch hardware was originally designed to evolve past the fixed action sets of \gls{acr:sdn} at line rate, using a limited compute model rather than aiming for full programmability on par with host \glspl{acr:cpu}.
Indeed, the turnaround from the original \gls{acr:rmt} proposal~\parencite{DBLP:conf/sigcomm/BosshartGKVMIMH13} to full-scale switches based on Intel's \emph{Tofino 2}~\parencite{tofino2} and Nokia's \emph{FP5}~\parencite{nokia-fp5}, aggregating \qtyrange{12.8}{14.4}{\tera\bit\per\second}, in a scant few years is remarkable.
%\gls{acr:pdp} hardware, enabling tighter cooperation with end hosts, must be factored into the design of switching hardware, ?? rather than starting with full prog, ask how much we can get 
%\gls{acr:pdp}
%?? The last decade has seen a more surprising kind of change...
%?? Why? greater control/adaptability of networks, but also now measurement, exec? ?? enabling tighter cooperation with end hosts ?? arbitrary-ish.
%?? manycore SOCs vs ASICs? Emphasise heterogeneity? mention legacy NPUs `diverse'
Diversifying the field further, the legacy of older \glspl{acr:npu} has led to \emph{SmartNICs}, offering more expressive and capable compute at a smaller scale such as via Intel's \emph{infrastructure processing units}~\parencite{intel-ipu}.
As it happens, these tools have not only enabled greater control and adaptability of networks but also powerful schemes to measure them, a new environment to execute program logic, and tighter cooperation with end hosts.
%Fast turnaround from \gls{acr:rmt}~\parencite{DBLP:conf/sigcomm/BosshartGKVMIMH13} to ... Intel's Tofino 2~\parencite{tofino2} and Nokia's FP5~\parencite{nokia-fp5} aggregating \qtyrange{12.8}{14.4}{\tera\bit\per\second} full-prog switches, Intel's \emph{infrastructure processing units}~\parencite{intel-ipu} present a combined \gls{acr:fpga}- and Xeon-based series of SmartNICs for accelerating datacentre applications.
%?? The resurgence of PDP
What's fascinating is that \emph{these} ideas and use cases are not entirely novel, reflecting an undercurrent present since the \emph{active networking} movement~\parencite{DBLP:journals/ccr/TennenhouseW96}.
%?? an old idea returns to the fore, undercurrent which has always been here.
%?? manycore SOCs vs ASICs? Emphasise heterogeneity? mention legacy NPUs `diverse'
%?? So we must ask: what has changed now?
Instead, both classes of efficient hardware have revealed the value of \emph{offloading} and \emph{in-network compute}---moving all or part of an application's logic to the network fabric to accelerate it further, in spite of its different compute capabilities versus a typical \gls{acr:cpu}.
Moreover, these new classes of hardware are fully reprogrammable at runtime, allowing line-rate services to be easily installed, upgraded, and replaced.

%?? \gls{acr:psa}~\parencite{p4-psa}

%?? Intel's Tofino 2~\parencite{tofino2} (\qty{400}{\giga\bit\per\second}, \qty{12.8}{\tera\bit\per\second} aggregate) and Nokia's FP5~\parencite{nokia-fp5} (\qty{800}{\giga\bit\per\second}, \qty{14.4}{\tera\bit\per\second} aggregate) full-prog switches, Intel's \emph{infrastructure processing units}~\parencite{intel-ipu} present a combined \gls{acr:fpga}- and Xeon-based series of SmartNICs for accelerating datacentre applications.

%\gls{acr:nic}

%?? TOOLS DDN
The second substantial change of the last decade is the meteoric rise of \gls{acr:ml} and \gls{acr:rl} through high-profile, breakaway successes in difficult domains such as classification~\parencite{DBLP:conf/cvpr/HeZRS16} and game playing~\parencite{DBLP:journals/corr/abs-1912-06680,DBLP:journals/nature/SilverSSAHGHBLB17}.
These approaches learn a function to map input data (like the statistics of a monitored flow) to output labels or actions, repeatedly transforming it according to complex learned statistical properties, with the aim that a learned function extends well from seen to unseen data.
This, too, is the revival of an older line of research---statistical and connectionist ideas which have been extended and empowered by powerful, specialised compute resources like commodity \glspl{acr:gpu}.
%?? Meteoric rise of \gls{acr:ml}, \gls{acr:rl} due to high-profile, breakaway successes in difficult tasks (such as?)
%?? why we want \gls{acr:ddn}: in combination, many aspects of networks complex, heuristic, hand-tuned
%?? the tools to do this automatic tuning.
%?? input-output relationships from seen to unseen data
%?? learn from every change and its effects. -- RL at a high level, incl. deliberate exploration
%?? providing the tools to do this automatic, data-driven tuning that networks cry out for.
What this offers us is the necessary toolkit for \gls{acr:ddn}---the automatic tuning that networks cry out for---allowing the development of better generalised solutions to network problems, or even policies specifically tailored to the needs of a deployment environment.
\gls{acr:rl} methods in particular have a unique affinity for closed-loop control tasks.
These policies are iteratively learned by taking an action---deliberately exploring supposedly suboptimal choices from time to time---before observing the controlled system's state some time later and using a measured \emph{reward} score to improve the policy itself in its own feedback loop.
We have, at last, the tools to learn complex decision boundaries and effective control in spite of the very non-trivial (and often surprisingly involved) system dynamics of computer networks.
%?? Relate with example -- specifically RL i.e. act, deliberate explore, observe, improve.
%?? closed loop, feedback loop whatever.
The beauty is that, even as our networks evolve, we should be able to learn adjustments and corrections to account for new protocols, behaviours, and topology changes by learning from (always-available) performance and \gls{acr:qos} metrics.

%\gls{acr:ddos}

%?? Trim down, recast.
%Commercial developments along the same lines as this modern \gls{acr:pdp} hardware are proceeding apace as network bandwidth demands grow larger.
%Intel's Tofino 2~\parencite{tofino2} represents the latest product in the lineage of \gls{acr:rmt} hardware, offering \qty{12.8}{\tera\bit\per\second} with support for \qty{400}{\giga\bit\per\second} Ethernet.
%Nokia's FP5~\parencite{nokia-fp5} similarly promises full programmability for high-density switching and routing at \qty{800}{\giga\bit\per\second} Ethernet (\qty{14.4}{\tera\bit\per\second} total), while Intel's \emph{infrastructure processing units}~\parencite{intel-ipu} present a combined \gls{acr:fpga}- and Xeon-based series of SmartNICs for accelerating datacentre applications.

%?? SOLUTION
While these developments enable \gls{acr:ddn} and network programmability, it is at their intersection that the field of networking is truly on the cusp of something promising.
%This thesis explores and chronicles how these angles benefit the network and one another, as well as the challenges introduced by the limited capabilities of a \gls{acr:pdp} environment.
%?? The intersection
%?? new data to play with
For instance, \gls{acr:pdp} hardware and the integrated network measurement it enables expose new sources of data and state, such as port and queue occupancies, or precise and accurate flow telemetry.
In the case of flows, \gls{acr:ddn} processes can then act based on such data rather than sampled metrics, potentially offering more accurate classification for uses like \gls{acr:qos} assignment.
In the network at large, they may act on a network-wide picture of device state which \emph{would have otherwise been unavailable}.
Although this data is evidently too much for any one host machine to process---particularly per-packet events at a switch's aggregate $\mathcal{O}\left(\unit{\tera\bit\per\second}\right)$---the enhanced programmability allows \gls{acr:pdp} devices to pick up the slack by acting on it \emph{in situ}.
The first way we could achieve this is by aggregating and reducing data in \gls{acr:pdp} hardware to make it feasible to export (at lower volumes and rates), or to apply early statistical processing to ease the workload on process machines.
The second is by \emph{moving \gls{acr:ml} and \gls{acr:rl} logic directly into \gls{acr:pdp} hardware}.
\gls{acr:ddn} decisions may then be instantly factored into the routing and processing of typical dataplane \glspl{acr:mat}, at minimal latency cost.
%?? in-situ processing
However, programming in-network services has its own challenges: the hardware offers restricted instruction sets, program lengths, data types, \glspl{acr:fu} which would have provided floating-point support, and memory.
Each limits the kinds of processing we aim to perform, but nowhere is this felt more keenly than the mismatch between the needs of \gls{acr:ml} algorithms and capabilities of network hardware.
While this logic could also be pushed to host machines, there are significant drawbacks in decision latency and throughput.
This thesis explores and chronicles how these approaches benefit the network and one another, as well as the challenges introduced by the limited capabilities of a \gls{acr:pdp} environment.
To that end, I show through the community's advances and my own additions to the state of the art how these approaches can offer meaningful benefit to network operators, as well as how we might enable both online and offline learning in the \gls{acr:pdp} networks of tomorrow.
%specifically resource costs vs availability

%?? ALSO: how the network can help ML.

%?? The complexities of running complex stuff in the network
%?? diff capabilities, embedded, no \glspl{acr:fpu}...


%?? ML: act on unspecified or difficult system dynamics, and with rl possibly as they evolve too

%?? cusp of a promising field
%?? or networking is on the cusp of two promising fields?

%?? Remember: Topic Sentences!
%
%?? HOW TOOLS HELP
%?? Why we want to run complex stuff in the network
%?? measure more
%?? The complexities of running complex stuff in the network
%?? diff capabilities, embedded, no \glspl{acr:fpu}...
%?? host-network cooperation?

\section{Thesis statement}
This thesis asserts that:
\begin{quotation}
	\noindent
	\remembertext{0}{Data-driven networking---enhancing networks with \gls{acr:ml}---and dataplane programmability are key tools in aiding the control and measurement of future networks}.
	\remembertext{1}{Data-driven methods such as reinforcement learning can lead to improved performance in network optimisation and control problems, such as \gls{acr:ddos} prevention}.
	\remembertext{2}{In-network compute can make data-driven networking more efficient, effective, and responsive}.
	Finally, \remembertext{3}{dataplane programmability will allow the precise measurement \emph{and} data aggregation that can enable fine-grained data-driven analyses to scale to high flow rates or large networks}.
	Applied together, programmable data-driven networks can improve computer network operation beyond the sum of its parts.
%	?? Modern programmable network hardware will allow these key parts of data-driven or RL systems to run within the network fabric, improving performance and response times.
%	?? \emph{Reinforcement learning can lead to improved performance in network optimisation and control problems, such as DDoS prevention.}
%	?? \emph{Programmable dataplanes and in-network computation can enable new data-driven networking use-cases.}
%	?? \emph{Programmable dataplanes and in-network computation can make data-driven networking more efficient, effective, and responsive.}
\end{quotation}

While claims \stmtno{\numrange{0}{1}} fall in-line with expected uses of these new technologies, the others require some extra explanation to unpack.
%?? Points for \stmtno{0}?
Claim \stmtno{2} may be somewhat surprising, if we think only of the massive $\mathcal{O}\left(\text{\unit{\mebi\byte}--\unit{\gibi\byte}}\right)$ models which dominate classification, control, and language tasks.
By considering changes to algorithms and numerical formats, smaller models can be executed in the limited resources of \gls{acr:pdp} hardware.
The architecture of these devices is specialised around processing high rates of packet events---by parallelism or pipelining---which can allow line-rate operation of models transformed as above.
If such decisions can be made \emph{at the same time and location as input data arrive}, then the network can (re-)act faster.
%-- dplane more eff: reduced time for decisions to be acted upon, possibly reduced time to make said decisions. local state... line rate. architecture of these devices? Smaller models
Claim \stmtno{3} arises due to the scale and volume of data which \gls{acr:pdp} hardware can produce.
Consider a single \qty{100}{\giga\bit\per\second} port on a switch, operating at line rate with a mean packet size of \qty{500}{\byte}, from which we want to make some \gls{acr:ddn} decision based on \gls{acr:pdp}-only state such as \unit{\nano\second}-level timestamps and queue occupancies.
On average, this produces \qty[per-symbol=p,sticky-per=true]{25}{\mega\packet\per\second} events \emph{per-port}, which is difficult for a single machine to handle---let alone when it must perform per-packet inference.
\gls{acr:pdp} hardware thus has an important role to play in digesting and summarising its newly available metrics for host machines.

%?? point \stmtno{3}: good local data, but volumes of it too high  to export at scale and possibly even train hosts machines from w/o impact.

%?? \recallthesis{0}

\section{Contributions}
Grouped according to the claims (\stmtno{\numrange{0}{3}}) in the thesis statement:
\begin{itemize}
	\item \emph{A thorough summary of the literature on modern, programmable computer networks (including recent hardware trends) (\cref{chap:nets}), and of machine learning techniques suitable for their control (\cref{chap:ddn})}. This includes the history of a spectrum of tools to optimise dataplane processing---both automatically and by bespoke design. Not just how \gls{acr:ml} benefits networks, but how creative networking can benefit \gls{acr:ml} use cases (\stmtno{0,2}).
	\item \emph{A novel synthesis of best practices, design decisions, and environmental tradeoffs to consider in the design of \gls{acr:ml}-led system control (\cref{sec:ddn-use-takeaways}, \stmtno{1})}.
	\item \emph{An improved \gls{acr:rl}-based \gls{acr:ddos} prevention scheme (\cref{chap:ddos-rl}, \stmtno{1})}
	\begin{itemize}
		\item Two protocol-agnostic, flow-granularity \gls{acr:rl} agent designs (\emph{Instant} and \emph{Guarded} action models),
		\item Algorithmic modifications to Sarsa to enable better concurrent learning from many in-progress \gls{acr:rl} trajectories,
		\item Reward functions tailored to detecting the negative impacts of amplification \gls{acr:ddos} and \glspl{acr:lfa},
		\item A quantitative investigation of suitable flow features for attack traffic detection via \gls{acr:rl},
		\item Deadline-aware action planning and state fusion to shield agents from being overloaded,
		\item The architecture and design of a \gls{acr:vnf}- and \gls{acr:sdn}-based installation of this anti-\gls{acr:ddos} solution,
		\item Procedures and trace data for modelling and generating traffic similar to modern Opus-based \gls{acr:voip} flows,
		\item An empirical evaluation of these models against the prior state-of-the-art in \gls{acr:rl}-based \gls{acr:ddos} mitigation, and a  non-\gls{acr:ml} algorithm tailored towards \glspl{acr:lfa}.
	\end{itemize}
	\item \emph{\approachshort---the first implementation of in-network, online \gls{acr:rl} (\cref{chap:in-net-rl}, \stmtno{2})}.
	\begin{itemize}
		\item An analysis of why \gls{acr:rl} in \gls{acr:pdp} hardware is needed and best-placed to interact with the network, made feasible by classical \gls{acr:rl} methods and quantisation,
		\item An \gls{acr:rl} interaction model based on path-adjacent compute to protect carried traffic,
		\item Analysis of suitable data formats for online \gls{acr:ddn} in resource-constrained hardware,
		\item A proof that 1-step temporal-difference \gls{acr:rl} algorithms admit a parallelisable, map-reduce form with tile-coded policies,
		\item \emph{ParSa}, a wait-free, parallel, online \gls{acr:rl} algorithm to accelerate tile-coded policy inference and updates,
		\item Parallel \gls{acr:rl} strategies tailored to provide either maximum offline throughput, or optimal state-action latencies and online throughput,
		\item Work allocation algorithms and communication tailored to SmartNIC devices with an explicitly tiered memory model,
		\item In-depth evaluation of how \approachshort{} affects carried dataplane traffic, performs in latency and throughput under different policy sizes (simple/complex state), and improves on host machines,
		\item A description of how \approachshort{} would integrate with state-of-the-art \gls{acr:pdp} applications to perform fully in-\gls{acr:nic}, fast, automated \gls{acr:ddos} mitigation based on \cref{chap:ddos-rl}.
	\end{itemize}
	\item \emph{\seidr{} histograms for aggregation of precise flow telemetry (\cref{chap:seidr}, \stmtno{3})}.
	\begin{itemize}
		\item A flexible dataplane-assisted architecture and algorithm compatible with the \gls{acr:psa} that allows data aggregation in the form of histograms,
		\item A measurement study of \gls{acr:iat} microstructure between \gls{acr:tcp} \gls{acr:cca} variants, and analysis which establishes the algorithmic cause for these differences,
		\item A high-accuracy method for using the \seidr{} procedure to track \glspl{acr:iat} with nanosecond-accurate timing to tell apart timer-based (\eg, BBR) and \texttt{cwnd}-based \gls{acr:tcp} \glspl{acr:cca},
		\item An extensive evaluation of \gls{acr:tcp} congestion control classification using \gls{acr:iat} histograms using different \gls{acr:ml} models,
		\item Analysis of \seidr's scalability compression ratio relative to input sequence length.
	\end{itemize}
\end{itemize}

A contribution I can't claim to offer, but hope sincerely to have done, is to collect together enough of the literature and intuition on \gls{acr:ddn} and \glspl{acr:pdp} to serve as a comfortable introduction to a newer researcher in the field.
The topic of \gls{acr:ddn} in particular has blossomed during the course of my PhD education---and scarcely existed at the scale it does today when this work was first undertaken in 2017.
Making the case for its relevance and best practices has become much easier over the last few years alone in light of this.
I'm fortunate that the work of many others tackles the same problems as I do, which I think lends credence to the thesis statement (\stmtno{0} in particular)---in a sense, this work contributes one set of case studies among many.
I hope that this thesis can be the book I would have wanted to read (and use) as a starting point when I was setting out on this research venture.


%?? as such, I'm one case study among many?
%?? In one way, I'm fortunate that the work of many others tackles the same problems as I do which I think lends credence to the thesis statement.
%
%In addition to the main goals:
%\begin{itemize}
%	\item Procedures for modelling and generating traffic similar to modern Opus-based \gls{acr:voip} flows (\cref{adx:opus-traffic}).
%\end{itemize}

\section{Thesis outline and structure}
Broadly speaking, this thesis is presented in two halves.
The first offers in-depth background on both the fields of \gls{acr:ddn} and \gls{acr:pdp}:
\begin{description}
	\item[\Cref{chap:nets}] describes the evolution of computer networks from fixed-function devices towards increased programmability in both the control plane and dataplane---critically examining early research directions in contrast with modern successes. It then describes how modern dataplanes improve or allow new networked applications---offloading and in-network compute---before discussing the threat landscape of volumetric \gls{acr:ddos} attacks.
	\item[\Cref{chap:ddn}] provides an introduction to the new field of data-driven networking by a critically reviewing the design of many recent \gls{acr:ml} solutions to network problems and relevant function approximation and learning methods. This includes methods for networked infrastructure to improve training, and data formats needed to run \gls{acr:ml} techniques in resource-constrained environments. This concludes with some discussion on the limitations and security context of \gls{acr:ml}.
\end{description}
The second half presents novel, concrete use cases which each demonstrate a part of the thesis statement (as discussed above):
\begin{description}
	\item[\Cref{chap:ddos-rl}] investigates using multi-agent \gls{acr:rl} to automatically learn the features of attack traffic online. I explore agent designs informed by past \gls{acr:rl} approaches (and their failures) relative to the realities of Internet traffic. State spaces in particular are experimentally justified to find `per-feature' value. A system architecture as part of a larger \gls{acr:vnf} system is shown, followed by evaluation of efficacy on different traffic classes and scenarios.
	\item[\Cref{chap:in-net-rl}] takes to task the goal of enabling in-network, online \gls{acr:rl} for the first-time. I present an exploration of the design space around the interaction mechanisms, compute models, algorithm modifications, and data structures needed for \gls{acr:pdp} devices. This high-level design is named \approachshort{}. It then presents significant implementation detail for \approachshort{} on \gls{acr:nfp} SmartNIC hardware, followed by performance evaluation to show its improvements in state-action latency and to assert that its impact on traffic is minimal.
	\item[\Cref{chap:seidr}] examines how in-network data reduction to histograms can make complex, non-latency-sensitive \gls{acr:ml} decisions on host machines scalable. I motivate their use with a measurement study on \gls{acr:cca} detection from per-flow \unit{\nano\second}-level timestamps, before evaluating their general scalability and effectiveness in the target use case.
\end{description}
The thesis then concludes by summarising its main takeaways, offering closing thoughts on these fields, and outlines future work specific to the above use cases (\cref{chap:conclusion}).

Additional, supplementary details follow:
\begin{description}
	\item[\Cref{adx:caida-traffic}] describes the methodology and results of a small-scale study on the distribution of protocols in \gls{acr:caida} trace data, to establish a rough estimate of congestion-unaware traffic's presence in \gls{acr:isp} networks.
	\item[\Cref{adx:opus-traffic}] provides additional detail on the measurement process used to collect trace data for simulating \gls{acr:voip}-like traffic, as well as the software architecture for packet generation.
	\item[\Cref{adx:nfp-arch}] expands on architectural details for the \gls{acr:nfp} family of SmartNICs to offer some additional context for \approachshort{}'s design constraints.
	\item[\Cref{adx:opal-proto}] contains packet header and protocol descriptions for \approachshort{}'s in-band control protocol.
\end{description}