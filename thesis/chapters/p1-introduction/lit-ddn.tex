\chapter{Data-driven Networking}
Once we have any computer network as described above, we have a good basis for not only allowing two machines to communicate with one another, but also to do so \emph{efficiently}.
That isn't to say that there is no room to improve on how networks and operating systems provide this functionality.
Many aspects of modern networks, such as \glspl{acr:cca}, thresholds for differentiating services, and flow classification rely heavily upon hand-tuned heuristics.
As in \cref{sec:problems-in-modern-networks}, there is still vast scope to improve on communication latency and throughput, or to avoid and work around deleterious traffic patterns (such as incast communication).
As a result, research into \glspl{acr:cca}, network designs, and routing procedures is very much ongoing.
Crucially, as these operations lie at the core of network operation their solutions tend towards extremely efficient heuristic methods; they must be evaluated per-packet or react as quickly as possible to state change.
Designing new methods for network optimisation then requires deep insight into any problem, its edge cases, and the hardware/performance characteristics of the target environment.\sidenote{This affects how we design such heuristics even on commodity hosts---for instance, kernel-space \glspl{acr:cca} (i.e., as part of the TCP stack) are unable to use floating point arithmetic.}

Suppose that, as network administrators or protocol designers, we have access to a reasonable amount of information about the machines, network segments or \glspl{acr:as} under our control---measurements, observations, and statistics taken at run-time, from simulation, or by modelling.
A natural question to ask, then, is whether we can use this data to enhance and improve the operation and use of our network automatically.
Thinking further still, we might wonder whether we can outperform the general (yet useful) heuristics which are widely deployed and researched, allowing us to tailor network behaviour according to its environment and traffic patterns.
These questions are the founding principles of \gls{acr:ddn}\sidenote{Alternatively titled \emph{self-driving networks}.}, a recent field of research focussed on the automatic control and optimisation of network systems, which has sprung forth due to recent advances in \gls{acr:ml} and \gls{acr:rl}~\parencite{DBLP:conf/anrw/FeamsterR18,DBLP:journals/pieee/KellererKBBR019}.

The ideas and goals of automated network control have always existed and evolved in one form or another, particularly as computational inference and learning have grown more powerful.
Primarily, these ideas have propagated in their early forms via position papers offering a `vision of things to come'.
This was first famously formalised as the \emph{knowledge plane}~\parencite{DBLP:conf/sigcomm/ClarkPRW03}, in contrast to the \emph{data} and \emph{control} planes.
This proposal captures not only the above concepts of automation as a means for network control, but also for collaborative or commercial sharing of information between end-hosts, transit \glspl{acr:as}, and organisations to build up a global picture of the needs of the network.
In truth, over the past \num{19} years we have moved no closer to such a unified substrate, though automated inference based on the data we \emph{do} have is richly researched.
A later attempt to combine this with \gls{acr:sdn} as \emph{Knowledge}-defined networking~\parencite{DBLP:journals/corr/MestresRCBASMMB16} takes key steps in clarifying the field, through concrete problems and promising \gls{acr:ml} developments, but effectively curtails the scale of knowledge sharing.
\gls{acr:ddn} itself is named and defined by \Textcite{DBLP:conf/comsnets/JiangSSZ17}, who again expand the scope for optimisation beyond network control to include \emph{end-points}; towards application and transport layer optimisation for hosts and servers, as well as control of the underlying fabric.

Starting out with the aim of emphasising and motivating the value of \gls{acr:ddn}, I discuss and introduce some of the recent developments and applications of \gls{acr:ml} and \gls{acr:rl} techniques in computer networking (\cref{sec:use-cases}), before then moving onto to explain the `building blocks' underlying these approaches.
Specifically, I introduce relevant function approximations (\cref{sec:function-approximation}), techniques to learn these representations including \gls{acr:rl} (\cref{sec:learning-an-approximation}), networked training (\cref{sec:ddn-collaborative-training}), and representations for different target devices (\cref{sec:numerical-representations-for-embedded-ml}).
\Cref{sec:ddn-challenges} discusses additional challenges inherent to \gls{acr:ddn}, while \cref{sec:ddn-security} then presents an overview of security perspective surrounding current \gls{acr:ml} and \gls{acr:ddn} approaches.
Although this context and its challenges are rapidly evolving, an understanding of security issues is key to offering a complete picture of the viability of \gls{acr:ddn} and the care which must be taken in its research.
Sadly, full examination and further development lies beyond the scope of this thesis---it is, in fact, a thesis-worthy topic in its own right~\parencite{papernot-thesis}.

\section{Use Cases}\label{sec:use-cases}%
\input{chapters/p1-introduction/ch2-ddn/use-cases.tex}

\section{Function approximation}\label{sec:function-approximation}

?? Explain how different approximators work, I suppose?

?? Probably  define from first principles.
%
%\subsection{Accuracy Measures}
%
%?? mean-square error, bias, variance, ...

\subsection{Linear Functions}

?? Linear coding

\subsection{Neural Networks}

?? Deep learning book?~\cite{DBLP:books/daglib/0040158}

?? Neural networks

?? GNNs -- GCN, Edge-GNNs~\parencite{Mirhoseini2021}

?? graph convolution~\parencite{DBLP:conf/iclr/KipfW17}

\gls{acr:lstm}~\parencite{DBLP:journals/neco/HochreiterS97}

\section{Learning an approximation}\label{sec:learning-an-approximation}%
\input{chapters/p1-introduction/ch2-ddn/learning-approx.tex}

%?? Find some cites citing the relevance of this problem wrt. self-driving cars, robotics, etc.

%The solution we propose is to make use of the recent wave of programmable network devices to \emph{bring reinforcement learning to the dataplane}---referring again to \cref{fig:state-slip}, we would place place state measurement ($t_1$), low-cost decision-making processes ($t_2$), and controlled systems ($t_3$) as close to one another as possible.
%In networks, actions are most likely to be installed on backbone switches, bump-in-the-wire NICs or middleboxes, and in the NICs of end-hosts.
%Ideally, these functions which comprise an RL agent would all be collocated on the same chip or device, but this is easier said than done. 
%Both programmable devices and the network environment make this more difficult, as we'll examine in the sequel.

\section{Collaborative training}\label{sec:ddn-collaborative-training}
While the main focus of this thesis is to investigate how \gls{acr:ml} or data-driven methods can benefit the network, there is equal interest in asking instead \emph{how networking can benefit \gls{acr:ml}}.
Driven by the high compute-time requirements of modern \gls{acr:drl} and \gls{acr:dnn} training---in simulation, sample complexity, and raw input data---much community interest has been given to how networks can improve, accelerate, or innovate in \gls{acr:ml} model training.

I discuss briefly how networks offer distributed training, allowing models or datasets to be split over many worker nodes who all collaborate towards the training of a single \gls{acr:ml} model within a datacentre or cloud network.
This extends to brief discussion of novel ways to further optimise these processes as well as how they differ from traditional job scheduling.
Moving on, I discuss the frontier of this training task across far wider areas, network, or organisational boundaries---\acrfull{acr:fl}---alongside its limits.

%\subsection{Multiagent methods}

\subsection{Distributed training}
For various reasons, such as the high sample complexity of \glspl{acr:dnn} and \gls{acr:drl} in particular, \gls{acr:ml} model training must often scale beyond several machines.
The most practical reason for this is that we can use the parallelism we have across a computing cluster to improve training performance---for instance, spreading minibatches across many machines allows us to compute more gradient update vectors in the same amount of time.
The scale of some modern \glspl{acr:nn} requires this; either due to the sheer size of the input dataset, or due to the size of the \gls{acr:nn} model itself.
This latter case requires some explanation; high-speed training requires \glspl{acr:gpu} which are capped on a single machine by the shared \gls{acr:pcie} bus, and in turn have limited cores or \gls{acr:vram}.
Due to this, a single \gls{acr:gpu} may be unable to store the entirety of a model and related structures or data, particularly when training infrastructure is shared.
The former and simpler use case, spreading training data across distributed nodes, is known as \emph{data parallel} training.
Dividing an \gls{acr:nn}'s compute graph across either local or remote accelerators is known as \emph{model parallel} training.
These concepts may be applied independently, or hybridised as necessary.
For context, consider the level of distributed compute required to train high-profile \gls{acr:drl} works such as the Dota-playing \emph{OpenAI Five}~\parencite{openai-five}---consuming \num{128000} \gls{acr:cpu} cores and \num{100} \glspl{acr:gpu}---or \emph{AlphaGo Fan}~\parencite{DBLP:journals/nature/SilverSSAHGHBLB17}---\num{176} \glspl{acr:gpu}. 	

%Paradigm instrumental in Big RL Successes. (link to all the OpenAI Stuff here).

However, effectively exploiting this parallelism is less simple in practice than in theory.
Device heterogeneity, different batch sizes and contents, and episode lengths in the \gls{acr:rl} case can all affect how long it takes any individual compute node to complete its own set of tasks.
Network behaviour such as transient load across paths or switches, worsened by incast traffic behaviour at central \glspl{acr:ps} when all workers transmit gradient vectors at the same time, all add to latency or cause lengthy retransmits in response to losses.
Moreover, we aim to minimise the amount of time that nodes remain inactive.
In the data parallel case, this is considered from two main directions:
\begin{description}
	\item[Parameter Server] approaches designate one or more servers who are responsible for holding the current version of a trained model.
	Worker nodes send their individual gradient updates to the \gls{acr:ps}, who aggregates all inputs and broadcasts the completed update to be applied~\parencite{DBLP:conf/nips/DeanCMCDLMRSTYN12,DBLP:conf/osdi/LiAPSAJLSS14}.
	Larger models reduce contention or bottlenecks by sharding portions of $\wvec{}$ across different \glspl{acr:ps}.
	These aggregate and broadcast steps may either be synchronous (as above) or \emph{asynchronous}, where workers do not wait to receive this broadcast vector before computing further updates (at the cost of losing model convergence guarantees as updates grow `stale').
	
	\item[AllReduce] techniques have workers transmit gradient updates to one another in a structured overlay network to perform gradient aggregation without a \gls{acr:ps}~\parencite{DBLP:conf/cluster/MamidalaLP04,DBLP:conf/ipps/PatarasukY07}.
	These communication patterns may be ring structured (bandwidth-optimal, at the cost of $\mathcal{O}{\left(n\right)}$ latency) or structured as a binary tree.
\end{description}
Existing frameworks such as TensorFlow~\parencite{DBLP:journals/corr/AbadiABBCCCDDDG16} support these concepts via Horovod~\parencite{DBLP:journals/corr/abs-1802-05799}, Ray~\parencite{DBLP:conf/osdi/MoritzNWTLLEYPJ18}, BytePS~\parencite{DBLP:conf/osdi/JiangZLYCG20}, or recent designs such as Hoplite~\parencite{DBLP:conf/sigcomm/ZhuangLZWLNMS21}.\sidenote{It should be stated that gradient aggregation in general is simply the summation of all individual update vectors. This conceptually simple processing is what enables the task to be converted into a map-reduce workload so easily, but also enables many in-network approaches worth examining.}

Developing and optimising these distribution strategies is an ongoing line of work; consider that some works have seen that aggregating gradient updates consumes around \qty{83.2}{\percent} in the synchronous \gls{acr:ps} case~\parencite{DBLP:conf/isca/LiLYCSH19}, and that device heterogeneity can cause stragglers to greatly inflate latency in the AllReduce case.
In particular, different use cases or approximators (\gls{acr:rl}, \glspl{acr:gnn}) have different demands or characteristics; e.g., distributed \gls{acr:rl} policy updates are small and extremely frequent due to having an iteration count around an order of magnitude higher compared to \glspl{acr:nn} as classifiers~\parencite{DBLP:conf/isca/LiLYCSH19}.
Meanwhile, the explicit message-passing built into \glspl{acr:gnn} requires deliberate communication planning across nodes on one or many machines~\parencite{DBLP:conf/eurosys/Cai0WMCY21,DBLP:conf/eurosys/WangY0YCYYZ21}.

Distributed \gls{acr:ml} training has been found to exhibit different cluster use characteristics from other job scheduling tasks.
Work remains common on minimising \glspl{acr:jct}, wasted resource use (i.e., dollar cost to the user) and other sources of contention in the network.
These include \gls{acr:rl}-based policies such as \emph{MLFS}~\parencite{DBLP:conf/conext/0002LS20}, which optimise job and cluster performance with awareness of overfitting.
non-\gls{acr:ddn} works provide dynamic downscaling of per-task resources~\parencite{DBLP:conf/eurosys/MisraLDBKGST21}, and fair sharing and allocation between different \gls{acr:gpu} classes~\parencite{DBLP:conf/eurosys/ChaudharyRSKV20}.
More recent works include explicit prioritisation of jobs which are short or likely to maximise model accuracy increases, combined with the above class of elastic scaling~\parencite{DBLP:conf/nsdi/HwangKKSP21}.
Indeed, there have been suggestions from the research community that even the underlying transports protocols or \glspl{acr:cca}---including RoCE~\parencite{rocev2} and \gls{acr:tcp}---are at fault in bottlenecks rather than bandwidth~\parencite{DBLP:conf/sigcomm/ZhangCLWAJ20}.
Of course, this does not preclude the development of specialised optical interconnects~\parencite{DBLP:conf/sigcomm/ShirkoohiGAZGBV21} to provide extremely high-bandwidth, reliable circuit-switched communications between nodes.

Recalling the large amount of time spent centrally aggregating parameter updates in \gls{acr:ps} methods (in addition to heavy incast behaviour), it is clear they have several disadvantages over AllReduce in exchange for lower latency.
\emph{In-switch} aggregation approaches have been proposed to remedy these issues such that the update vectors are aggregated \emph{en-route} to the \gls{acr:ps}, ensuring not only that the \gls{acr:ps} itself performs less work, but also minimises its inbound packet volume.
\emph{iSwitch}~\parencite{DBLP:conf/isca/LiLYCSH19} uses NetFPGA-SUME hardware to implement special handling for RL model update packets, building floating-point adders and limited storage space into bump-in-the-wire \glspl{acr:nic} to achieve \qtyrange{3.66}{3.71}{\times} faster training.
\emph{SwitchML}~\parencite{DBLP:conf/nsdi/SapioC0NKKKMPR21} converts a programmable \gls{acr:tor} switch \emph{into a \gls{acr:ps}}, offering this as a P4 program built on fixed-point quantisation.
\emph{ATP}~\parencite{DBLP:conf/nsdi/LaoLMCWAS21} extends this to a best-effort service and custom transport protocol in front of the true \gls{acr:ps}, falling back to floating point on detection of overflows and offering better support for several dynamic jobs.
However, this family of optimisations can only be applied when two gradient packets are able to meet in the network excluding, e.g., ring AllReduce.
I discuss implementation specifics in more detail through \cref{sec:in-network-computation}, and some rationale in \cref{sec:numerical-representations-for-embedded-ml}.

%?? communication and compute can be cleverly overlapped and optimised
Techniques such as \emph{Wait-Free Backpropagation}~\parencite{DBLP:conf/usenix/ZhangZXDHLHWXX17,DBLP:conf/ppopp/AwanHHP17} have enabled tighter optimisation of when individual gradient parts can be set out and used to optimise for a compute-communication tradeoff.
This can be combined with gradient sparsification to achieve reduced bandwidth~\parencite{DBLP:conf/infocom/ShiWCLQLZ20}, and further optimised for \gls{acr:ps}~\parencite{DBLP:conf/infocom/WangLG20} and AllReduce~\parencite{DBLP:conf/infocom/BaoPCW20} communication patterns.
Moreover, \glspl{acr:gnn} have been used to dynamically optimise hybrid aggregation strategies and model/data parallelism across heterogeneous \gls{acr:gpu} clusters~\parencite{DBLP:conf/conext/0001ZLLDWZYL20}.

\subsection{Federated learning}
The key driver behind distributed training is the idea that disparate and networked resources within a single organisation can be used together to accelerate (or even make feasible) the training of complex \gls{acr:dnn} models.
A more difficult question that has since arisen is how best to achieve this between \emph{several} organisations, or even weaker devices in consumers' hands or at the edge of the Internet.

\acrfull{acr:fl}~\parencite{DBLP:journals/corr/KonecnyMRR16,DBLP:journals/corr/Konecny17,DBLP:conf/mlsys/BonawitzEGHIIKK19} allows several unaffiliated devices to independently train local \gls{acr:ml} models from the data available to them.
The main goal of \gls{acr:fl} is to train a high-quality, centralised model, potentially including local or device-specific optimisations, when input data are divided unevenly over a large number of nodes.
Crucially, we require that training remains effective when such data are non-\gls{acr:iid} across these devices.
The key intuition is that we can make use of device-local intelligence to train our model.
By having any device update its parameter set locally, a node collects its set of combined gradients, which are then sent to the parameter server; these are collected as in above \gls{acr:ps} approaches and a new canonical model may be published to all users.
This saves computing cost at the central parameter server, bandwidth consumption due to the large size and total volume of training samples, and (in principle) protects the privacy of users with regard to sensitive or personally identifying input data.

The original solution~\parencite{DBLP:journals/corr/KonecnyMRR16} has each node solves a subproblem subject to some quadratic perturbation, combining DANE~\parencite{DBLP:conf/icml/ShamirS014} and Stochastic Variance Reduced Gradient~\parencite{DBLP:conf/nips/Johnson013}.
Much of the algorithmic developments are dedicated to correcting for potential sources of bias from the edge-node results before they are combined.
This includes weighting contributions by each node's proportion of input samples and the distribution of non-zero values in these input data.
This was first adapted to more non-convex functions (i.e., \glspl{acr:dnn}) by \textcite{DBLP:conf/aistats/McMahanMRHA17}.
Notably, they \emph{only} make use of this weighted averaging step---which they claim has an inherent regularisation effect analogous to dropout---yet achieve respectable training performance.
Crucially, they raise the point that all candidate models must be at the very least be initialised from the same seed or master policy.
More recent work examines the possibility of applying \gls{acr:rl} to better handle non-\gls{acr:iid} input data by selecting the gradient vectors to include in an update round at runtime~\parencite{DBLP:conf/infocom/WangKNL20}.

\gls{acr:fl} has since been divided into two classes by some practitioners:
\begin{description}
	\item[Cross-device \gls{acr:fl}] covers the above motivating case, where contributing devices are assumed to be mobile or \gls{acr:iot} devices.
	\item[Cross-silo \gls{acr:fl}] describes this paradigm as applied to shared learning \emph{between} organisations. This formulation applies far stricter privacy guarantees: the parameter server may be unable to see the learned model or individual gradients (but is still responsible for hosting and updating it), and organisations must limit the ability to extract or infer personally identifying information from the trained model.
\end{description}

The latter case introduces additional engineering and design challenges, and is a topic of ongoing research.
\emph{Secure aggregation}~\parencite{DBLP:conf/ccs/BonawitzIKMMPRS17} has been proposed such that gradient combination is pushed down to clients who have randomised communication patterns with one another, while only the final model change is exposed to the parameter server.
Homomorphic encryption hides this from even the central server without requiring that hosts perform piecewise aggregation: arithmetic operations applied between a pair of ciphertexts behave as though applied to the clear texts.
\emph{BatchCrypt}~\parencite{DBLP:conf/usenix/ZhangLX00020} applies this to batches of integer-quantised gradients, where batching and sparsification are the key tricks needed to reduce the heavy bandwidth of homomorphic encryption.
This is then improved upon in bandwidth overheads, encryption time, and aggregation cost by \emph{FLASHE}~\parencite{DBLP:journals/corr/abs-2109-00675}, which moves to a simpler \emph{symmetric} cryptosystem that supports only \emph{addition}.
When considering whitebox attacks of the form discussed throughout \cref{sec:ddn-security} such as data reconstruction attacks~\parencite{DBLP:conf/icml/LamW0RM21}, cross-silo \gls{acr:fl} often draws on differential privacy literature~\parencite{DBLP:conf/iclr/McMahanRT018}.

%FL with Homomorphic Encryption~\parencite{DBLP:conf/usenix/ZhangLX00020,DBLP:journals/corr/abs-2109-00675} -- Check FLASHE for good cites and discussion of issues introduced by homeomorphic encryption.

%?? Explcitly describe the requirement for locally-acquired labels.

\gls{acr:fl} as a paradigm is limited in how it can be deployed to solve supervised tasks.
As each node, device or organisation must contribute gradient information that \emph{they themselves have locally sourced}, we can see that in supervised problems this mandates that these nodes must also be able to label the data themselves somehow.
To get the high volume of data required to overcome \glspl{acr:dnn}' sample complexity, input data must either be trivially user-labelled data, effectively self-labelling (i.e., known at a later time), or arise through time-series forecasting.
For instance, predictive text~\parencite{federated-learning-blog} was one of the first large-scale use cases at Google---input data are words the user had typed, and their labels are either the chosen suggestion \emph{or} what they themselves typed instead.
Naturally, there is still applicability in semi-supervised or unsupervised techniques like autoencoders~\parencite{DBLP:journals/ftml/KingmaW19} which can be cast as a gradient descent problem.
In the \gls{acr:rl} case, we require that each learner is solving its own control problem with locally available reward measurements.

%?? Issue? Only works on certain problems (explicitly unsupervised, or easy to acquire local supervised measurements).
%
%Okay, what conditions does it impose on the type of data we want to use for training (right now, at least)?
%?? Need trivially (user-)labelled data, to get the high volume we need. (label-able at the edge nodes? is unsupervised remotely worthwhile?)
%?? Ex: predictive text works (data is word the user had typed, label is what they picked from the dict OR what they typed by the end.)

%Invented by \textcite{DBLP:journals/corr/KonecnyMRR16}.
%?? (Probably look at the lead author's shiny new PhD Thesis \cite{DBLP:journals/corr/Konecny17}? Look for more of his stuff?)
%?? IDEA -- train a high-quality centralised model when data are divided unevenly over a large number of nodes (read: non-IID).
%?? Main problem is that the best-performing algorithms are very much sequential in nature. Even then, many make assumption that all learners see a representative sample.
%?? Each node solves a subproblem subject to some quadratic perturbation (as in DANE \cite{DBLP:conf/icml/ShamirS014} (not read!)) but for Stochastic Variance Reduced Gradient (SVRG).
%?? Much of the algorithm is correcting for potential sources of bias from the edge-node results before they are combined (averaged very carefully).

%\textcite{DBLP:conf/aistats/McMahanMRHA17} examine:
%?? Federated Learning to train deep neural nets.
%?? More broadly, this looks at non-convex loss functions in general.
%?? New Algo---FedAVG seems to outperform FedSVG despite having fewer terms to correct for bias. I've written that it's analogous to dropout (which itself can offer behaviour close to a Bayesian net), check again to see what exactly this refers to.
%?? Core idea---still essentially just taking (weighted) average of the policies from edge learners. Pretty important that they're all initialised from the same seed, at least then. Better convexity behaviour than expected.

\section{Numerical representations for embedded ML}\label{sec:numerical-representations-for-embedded-ml}
\gls{acr:ml} training and inference work in the domain of real numbers, and thus require a suitable data format for representation of weights, gradients, and values.
\emph{Floating-point arithmetic} allows commodity machines and accelerators to approximate real numbers in a fixed-size representation, dividing available bits among a \emph{sign}, \emph{exponent}, and \emph{mantissa}.
This captures several important properties, principally \emph{dynamic range} (as the exponent describes the magnitude of a number).
For instance, \qty{32}{\bit} floating-point numbers (FP32) use \qtylist{1; 8; 23}{\bit} to store each component, which is sometimes known as a 1-8-23 representation.

However, there are concrete reasons to consider other data formats, particularly on more resource-constrained environments.
Quantisation and alternative data formats have been suggested to make \gls{acr:ml} inference feasible on resource- and power-limited platforms, work around hardware constraints, or compute faster and more efficiently.
Although individual floating-point operations, as compared to integers, have effectively equal latency and reciprocal throughput on modern x86 hardware~\parencite{agner-x86}\sidenote{This leaves aside the performance gains offered by \gls{acr:simd} vectorisation, which is a trickier topic.}, \glspl{acr:fpu} still require additional chip area and power.
Naturally, chip designers don't want to fabricate or plan around unnecessary \glspl{acr:fu}: for instance, (programmable) network hardware and \glspl{acr:asic} require only basic integer arithmetic.
This is not the only reason to be interested in alternative data formats; by reducing the size of any individual number from \qty{32}{\bit} to \qtylist[list-pair-separator = { or }]{16; 8}{\bit}, we reduce the size of parameter sets and input vectors by \qtyrange{2}{4}{\times}.
This reduces the range of numbers we can express (in both magnitude and precision), but can reduce inference latency and memory cost for the benefit of both commodity machines and dedicated accelerators.
Luckily, the task of bringing \gls{acr:ml} models to resource-constrained environments without these capabilities is well-studied, and in general the effect on accuracy is small in spite of the introduced quantisation noise.

\subsection{Floating-point}
While there are standardised floating-point forms designed to target mobile and weaker hardware, such as \emph{half-precision} (FP16, \qty{16}{\bit}, 1-5-10) and \emph{minifloat} (FP8, \qty{8}{\bit}, 1-4-3), these fail to be effective in some \gls{acr:ml} use cases.
At the same time, a key factor in \gls{acr:fpu} chip cost is the size of the \emph{mantissa}---which has been observed to have a quadratic scaling effect on area in \gls{acr:tpu} development~\parencite{bfloat16-blog}.
Accordingly, allocating more bits to the \emph{exponent} can allow for more cores and \glspl{acr:fu} in the same area, or reduce power draw.
\emph{bfloat16}~\parencite{bfloat16-blog} is a \qty{16}{\bit} format employed in (among other devices) Google TPUs~\parencite{DBLP:journals/sigops/XieDMKVZT18} and modern Intel Xeon server \glspl{acr:cpu}~\parencite{intel-bfloat}.
It matches the dynamic range of \qty{32}{\bit} floats (1-8-7), better expressing the smaller end of the dynamic range (for e.g., gradients) while having identical failure modes (subnormal numbers, edge cases) to FP32.
\emph{hfp8}~\parencite{DBLP:conf/nips/SunCCWVSCZG19}, as an \qty{8}{\bit} format, uses different layouts for the forward (1-4-3) and backward (1-5-2) passes, applying a downward bias of \num{4} to the exponent in both cases.
This allows better expression of small values in general, and even smaller values during gradient computation, at an extra \qty{5}{\percent} hardware area cost to support both formats.
While this is an indicative summary , it must be said that there are more formats beyond the scope of this introduction~\parencite{DBLP:journals/corr/abs-2007-01530}.

\subsection{Fixed-point and binary}
While the above techniques are promising and effective, deployment on environments such as \gls{acr:pdp} hardware requires further ingenuity.
\emph{Fixed-point arithmetic} is an approach which makes it possible to represent real numbers as integers, losing dynamic range as a consequence.
$Qm.n$ expresses a real number using an $m$~\unit{\bit} integer part alongside an $n$~\unit{bit} fractional part, which allows us to evaluate and update policies using only integer arithmetic on $k=1+m+n$~\unit{\bit} numbers, assuming the presence of a sign bit.
For instance, the \qty{8}{\bit} number \mintinline{rust}|0b0010_1000| in $Q3.4$ represents the real number \num{2.5}---we can view this as two separate parts ($2 + 8\times2^{-4}$), or as one whole in the fractional base ($40\times2^{-4}$).
In practice, the entirety of the number is stored as a two's complement number in place of a sign bit, and base conversion (i.e., changing $n$) requires only bitshifts.
When $k$ is known, we can simply refer to the representation as $Qn$.
The most useful part of this scheme is that integer addition and subtraction are \emph{unchanged} for two $Q$ numbers, and conversion of a normal integer requires an $n$~\unit{\bit} left shift.
Multiplication and division \emph{by a normal integer} can be performed using the standard \gls{acr:alu} operations, while $Q$ numbers need an additional bitshift (pre-/post-op base correction) and temporary expansion into a $2k$~\unit{\bit} register.

Although these tradeoffs seem to predispose fixed-point towards \emph{only} target environments without \glspl{acr:fpu}, it has still enjoyed application in inference and training.
In particular, \textcite{DBLP:journals/corr/CourbariauxBD14} were able to train Maxout networks without substantial error with $k$ at \qty{19}{\bit}, or \qty{11}{\bit} using dynamic scaling ($m=5$).
In distributed training, fixed-point arithmetic is useful as an intermediate representation for in-NIC gradient aggregation~\parencite{DBLP:conf/nsdi/SapioC0NKKKMPR21,DBLP:conf/nsdi/LaoLMCWAS21}.
Scaling down \glspl{acr:nn} to INT8 requires careful calibration~\parencite{tensorrt-8bit}, binning the activations per layer in an FP32 network to choose optimal saturation thresholds.
Training INT8 \glspl{acr:dnn} directly on mobile/\gls{acr:iot} devices has become more possible~\parencite{DBLP:conf/usenix/Zhou0QGXZGLZ21}, though this still requires floating point hardware to compute simple compensation terms after moving all tensor operations to fixed-point.
To some extent, these can also enable in-\gls{acr:nic} \gls{acr:dnn} inference~\parencite{langlet-ml-netronome}.

Binary representations are used to great effect in \acrfullpl{acr:bnn}~\parencite{DBLP:journals/corr/MiyashitaLM16,DBLP:conf/eccv/RastegariORF16,DBLP:journals/corr/KimS16,DBLP:conf/nips/HubaraCSEB16}; by using 0 to represent to \num{-1}, and 1 to represent $+$\num{1}, we may replace Hadamard product operations between tensors with \textsc{Xnor} operations, and when combined with \textsc{Popcnt} instructions we can easily compute the dot product between vectors.
This offers bit-parallel execution compatible with almost all \glspl{acr:alu}, including \gls{acr:pdp} hardware.
This enables in-\gls{acr:nic} execution of \glspl{acr:nn}: either offloading small portions of fully-connected layers to accelerate inference~\parencite{DBLP:conf/sigcomm/SanvitoSB18} or to enable line-rate packet processing~\parencite{DBLP:journals/corr/abs-2009-02353,DBLP:journals/corr/abs-1801-05731}.
While there is vested interested in running these complex function approximators in \glspl{acr:nic} and switches, they are at present trained on commodity x86 machines using real-valued weights and gradients clamped to $\left[-1, 1\right]$ (i.e., using $\tanh$).
As such, online in-\gls{acr:nic} training remains out of reach.

\section{Challenges}\label{sec:ddn-challenges}
Leaving security aside for now, there remain key issues in the training and design of \gls{acr:ddn}-backed systems.
I briefly discuss several high-level challenges in the applicability of \gls{acr:ddn}, and initial in-roads to their solution: issues inherent to data collection and simulation in representative environments; the seeming lack of generality of \gls{acr:ml}/\gls{acr:rl} models in the systems domain; and the interpretation and verification of trained function approximators, particularly when given control over such critical infrastructure as the Internet.

\subsection{Input Data and Simulation}
A key issue in \gls{acr:ddn} is that training from trace data is inherently fraught with risk.
Consider that traces contain fixed sequences of states, and it should be apparent that through their use we cannot model or learn how the controlled system acts in response to change.
This includes the tacit assumption that the model will not change in response to the input, either due to an unforeseen operational mode, or because the users of the system actively change their behaviour.
Even given these difficulties, why do some studies still attempt to learn network control in this manner?
Network operators are, broadly speaking, unwilling to apply untested and unverified \gls{acr:ml} or \gls{acr:rl} models towards production networks; not only because they aim to prevent misconfiguration or collapse, but to uphold contractually enforced \glspl{acr:sla}.
Overcoming this requires the design and implementation of accurate simulations, which are marred by complex interactions between and across levels of the (ever-evolving) networking stack, particularly at Internet scale~\parencite{DBLP:journals/ton/X01c}.
Consider a task such as video stream \gls{acr:abr} selection: a simulation must consider at the minimum client-side and server-side behaviour (resources, contention, and demand), transport-layer protocol dynamics (handshakes, failure modes, \glspl{acr:cca}), and path characteristics (including load balancers) to name but a few.
These concerns are neither new nor limited to the field of \gls{acr:ddn}:
\begin{quotation}
\noindent
Here you can see the problem clearly.
It is not that simulations are not essential these days, and will be in the near future, but rather it is necessary for the current crop of people, who have had very little experience with reality, to realize they need to know enough so the simulations include all essential details.
How will you convince yourself you have not made a mistake somewhere in the vast amount of detail?
...The relevant accuracy and reliability of simulations are a serious problem.\\
\strut\hfill\parencite[pp. 248]{hamming-art-of-scieng} 
\end{quotation}
The main difficulty arises from the fact that it takes not only expert knowledge to model these dynamics but to \emph{consider} them, and while most of these details can be abstracted over it is harder to determine which will not lead to further surprises in deployment.

Taking advantage of trace data in a more principled way requires insights from \emph{off-policy} \gls{acr:rl}, such as the use of doubly robust estimators~\parencite{DBLP:conf/hotnets/BartulovicJBSS17} or contextual bandits~\parencite{DBLP:conf/hotnets/LecuyerLNSSS17}.
These include the derivation or learning of reward models, and importance or inverse-propensity sampling.
Even so, in the case of doubly robust estimators \citeauthor{DBLP:conf/hotnets/BartulovicJBSS17} explain that these may be invalidated by hard-to-model or highly non-linear assumptions.
In addition, different policies will invoke different state distributions, and these approaches are incompatible to some extent with non-Markovian problems or non-stationarity.

While it might be easier to cynically connect this goal to the initial wave of dataset-driven \gls{acr:ml} algorithm applications papers, trace data \emph{can be correctly handled}.
For instance, using \gls{acr:rl} to solve static problem instances derived or cast similarly to \textsf{NP}-hard optimisation problems does not require simulation or ongoing interaction with a `real' environment.
Although it can be difficult to know ahead of time, it's worth considering whether the problem is adversarial in some way; an ongoing control problem is altogether different from an offline optimisation task.
It's unlikely that in an optimised chip floorplanning, for instance, that target programs will immediately start acting differently---compare this to a network where our actions immediately induce varied modes in \glspl{acr:cca}.

\subsection{Generality}
As we can see throughout \cref{sec:use-cases}, \gls{acr:ddn} methods are applicable to a wide variety of problems.
However, to claim that these use cases require in-depth agent-environment co-design is a generous understatement---particularly applications of \gls{acr:rl}.
Consider mainly the potent effects of not only the state and action spaces which are inherently problem-specific \emph{and} require domain expertise to define and iterate on.

Recent research aims to investigate whether more general approaches are feasible, by using \gls{acr:ml} inference to convert input vectors and decisions into a performance metric~\parencite{DBLP:conf/nsdi/FuGMR21}---effectively as a black-box form of `what-if' analysis.
Ideally, there would be no case-specific design elements beyond the chosen input features, performance prediction would function for many separate applications, and perform accurately.
Looking at this task on scheduling, planning, and placement tasks\sidenote{Generally, the varied parameter which must be optimised is some form of compute allocation, e.g., the number of servers or executors given to a task set.}, they find that inbuilt optimisations add irreducible variance to the learning problem, even when made as simple as possible.
Non-deterministic behaviour (i.e., stochastic load balancing) is for instance an obvious source of noise, but threshold-based behaviours also cause significant discontinuities.
Even after diagnosis of these noise sources (removing ease of use), input-sensitive tasks still require probabilistic \gls{acr:ml} methods---which can be harder to action or reason about.

\subsection{Interpretability and Verification}
For all the effort, time and expertise required to craft them, algorithmic or heuristic solutions to network problems have a key advantage over data-driven methods.
Because they are so well-specified, it is reasonable for a network operator or expert who has observed some unintended behaviour to derive the root cause, and possibly formulate a fix for the issue.
In contrast, \gls{acr:ml} and \gls{acr:ddn} models' behaviour is governed almost entirely by an opaque set of parameters ($\wvec{}$, which can contain \numrange{e3}{e9} real numbers), which makes it harder to understand what aspects of input data are being acted on and prioritised.
\emph{Interpretability} captures whether a human can reasonably understand why an input or scenario invokes an output or set of behaviours.
As a side effect, tweaking a model to correct, modify, or improve behaviour is also rendered difficult or impossible.
\emph{Verifiability} describes our ability to prove that a \gls{acr:ddn} or heuristic solution upholds key properties through, e.g., modelling or closed-form analysis.

Interpretability in general \gls{acr:ml} has attracted attention as a research goal in its own right.
Many classical \gls{acr:ml} approaches such as \glspl{acr:svm} or decision trees offer sensible rule sets or decision boundaries~\parencite{DBLP:conf/pkdd/MolnarCB20,interpretable-ml}, \gls{acr:nn}-based function approximation presents very particular challenges.
These comprise repeated high-dimensional linear transforms of input data (e.g., by matrices containing thousands of values), interposed with non-linear activation functions.
\glspl{acr:cnn} and \glspl{acr:lstm} complicate this logic further by introducing learned convolution filters and temporal relationships, respectively.
On some data classes such as images, it is possible to visualise learned feature activations~\parencite{cnn-features-distil}, which typically manifest as shapes or patterns which make some degree of sense to a human observer.
Network state spaces, however, are far less intuitive, so feature activations in NNs are less obviously meaningful and still fail to allow `tweakability'.
Tools such as \emph{LIME}~\parencite{DBLP:conf/kdd/Ribeiro0G16} can reveal the relative importance of each feature in such cases, but it can still require in-depth testing to realise that (but not why) an agent never chooses some actions in spite of their optimality~\parencite{DBLP:conf/sigcomm/DethiseCK19}.
%?? Interpretability, particularly for more powerful fn approxes
%?? Main problem? Can'y humanly visualise the transformations on input data
%?? Why? High-dimensional transfomrations w/ millions or billions of parameters, interposed with non-linear transformations
%?? visualise decision surface
%?? Equally, hard to 'tweak' model.
%?? General cites~\parencite{DBLP:conf/pkdd/MolnarCB20,interpretable-ml}
%?? can do for CNNs on images~\parencite{cnn-features-distil} -- what about LSTMs, RNNs, ...
%?? Interpretability~\parencite{DBLP:conf/sigcomm/MengWBXMH20}
A \gls{acr:ddn}-specific remedy is to use teacher-student methods to convert (non-recurrent) \glspl{acr:nn} into simpler models~\parencite{DBLP:conf/sigcomm/MengWBXMH20}.
In particular, `local' decision-making systems (\glspl{acr:cca}, \gls{acr:abr} selection, \gls{acr:to}/\gls{acr:te}) are converted into decision trees after applying branch pruning algorithms to keep the model compact enough for human comprehension.
For global decisions (job scheduling, routing, \gls{acr:vnf} placement) they produce hypergraphs which express which decisions are critical in an optimal solution.
%?? Convert \gls{acr:ddn} \glspl{acr:dnn} to more interpretable forms using teacher-student methods: decision trees for local decisions (\glspl{acr:cca}, \gls{acr:abr} selection, \gls{acr:to}/\gls{acr:te}), for global they can show hypergraphs which express the edges which are critical to optimisation (job sched, routing, NF placement) (is this analogous to CNN feature expression?).
In addition to reducing latency and making it clear what sequence of decisions is responsible for an output, this exposes any `missing classes' in the input and output spaces quite visibly.
In response, administrators may add additional training data or modify the decision tree themselves.
Unfortunately, the hypergraph representation fails to explain or simplify the logic behind a given decision set (as opposed to the highest-value members of that set), but can allow model co-design, i.e., important features can be allowed to skip several \gls{acr:nn} layers (having a more direct effect on output).
%?? In DTs etc, can see `missing classes' and train to include or directly alter DT.
%?? Latency reductions.

Verification has a broad set of meanings in \gls{acr:ml} research, from investigating security properties (i.e., adversarial robustness) to more global guarantees of input and output properties.
I discuss verification towards adversarial example resistance in \cref{sec:ddn-security}.
One promising line of work in this area applies \gls{acr:smt} solvers to smaller \glspl{acr:dnn} for, e.g., safety and liveness constraints on inputs~\parencite{DBLP:conf/cav/KatzBDJK17,DBLP:conf/cav/KatzHIJLLSTWZDK19}.
These verification techniques are powerful in that they can guarantee a desired property is upheld (or produce a distinct counterexample where it is not), although recalling that \gls{acr:smt} solution is \textsf{NP}-Hard, this comes at a high execution cost.
Luckily, most \gls{acr:ddn} use cases considered in this thesis apply small-to-moderately sized networks, to which such verification is well-suited.
Extending this towards \gls{acr:rl} is trickier, given that we must now verify that properties hold over state trajectories of arbitrary length.
In addition, the onus now falls on system operators to design suitable state transition functions (i.e., accurate system simulations) to model how a system evolves in response to an action.
With these primitives, some degree of \gls{acr:drl} verification is possible via bounded model checking~\parencite{DBLP:conf/sigcomm/KazakBKS19,DBLP:conf/sigcomm/EliyahuKKS21} (checking run-lengths of $n$ states from a set of initial states), and $k$-induction~\parencite{drl-verification-2}.
In addition to the need to define state transition dynamics using only linear functions, these works also impose strict limits on policy non-determinism and activation functions which can be used.

%?? NOTE: \parencite{DBLP:conf/sigcomm/KazakBKS19} has cites connecting to adversarial network robustness.

%?? As in use cases, can buy some of this back by using \gls{acr:ddn} to choose params for an existing approach
It's worth noting that, as with several approaches examined in \cref{sec:use-cases}, a hybrid approach can be useful in reducing the impact of both these concerns.
By augmenting an algorithmic or heuristic approach with \gls{acr:ddn} to compute optimal parameter choices, it can be far more reasonable in practice to understand how the system on the whole will behave.
Equally, it becomes easier to bound such choices within a safe range as needed.

%\subsection{Eh??}
%
%?? Point to security as a big challenge.
%
%?? Relate these to the security \emph{problem domain}? I.e., as seen by \citeauthor{DBLP:conf/sp/SommerP10}.
%
%?? Can exhibit issues with unseen data or operation modes, inability to converge, or insane resource use as seen in \gls{acr:cca} design~\parencite{DBLP:conf/sigcomm/AbbaslooYC20}.

\section{Security}\label{sec:ddn-security}

?? Discuss attacks on ML models, techniques, paradigms here.

?? NOTE check FLASHE paper for more attacks on e.g. FL.

?? Very respectable source \cite{DBLP:conf/eurosp/PapernotMJFCS16} that they're bad I guess? (Not Read)

?? The full summary paper \cite{DBLP:conf/eurosp/PapernotMSW18}.

?? Put this somewhere: can extract portions of training data w/ carefully crafted inputs~\parencite{DBLP:journals/corr/abs-2012-07805}. Not read, though maybe relate to privacy in general? I.e., differentially private methods.

\subsection{Attacks on Data-Driven Techniques}\label{sec:attacks-on-data-driven-techniques}

?? Older taxonomy, includes what we now call poisoning, evasion~\cite{DBLP:conf/ccs/BarrenoNSJT06}. Is there an updated?

?? White-box attacks -- evasion, adversarial examples, etc.

\paragraph{Poisoning Attacks}
?? Attacker wants to (permanently) alter the behaviour of a system which is still training in some way.
The key intuition is that an attacker wishes to affect which data points are used in training, and in turn affect the decision boundaries to do... something bad?

?? For systems which assume a stable (stationary) form of normality, it takes an exponential amount of packets with respect to how far the mean must move.
Yet in systems which use a finite number of data points (i.e. modelling non-stationarity) an attacker requires only a linear amount of data if they control a sufficient fraction of the network throughput.
Further findings are more optimistic: if the attacker controls insufficient traffic, then they cannot succeed in appreciably shifting the mean with even an infinite amount of traffic.
It isn't made clear how these findings relate to more complex systems or models, but it will remain an important consideration. \cite{DBLP:journals/jmlr/KloftL10}

?? More modern cite?

\paragraph{Evasion Attacks/Adversarial Examples}

?? Be clear---this is still evolving today!

?? Yeah these are a thing (note: haven't actually read most of these aside from the wonderful turtle-rifle 3d printer one).
?? The one, the only, the original \parencite{DBLP:journals/corr/SzegedyZSBEGF13}, the application \parencite{DBLP:journals/corr/GoodfellowSS14}.
?? Second one here suggests that part of the weakness is that models fall back on their heavily linear components -- verify this.

?? Most recently summed up in the SoK paper by \textcite{DBLP:conf/eurosp/PapernotMSW18}.

?? Where are we now? Transform invariant (i.e., photographs \cite{DBLP:journals/corr/KurakinGB16}, 3D model \& 2D transforms \cite{DBLP:journals/corr/AthalyeS17}): what does this mean with regards to the transforms we apply to our input data?

A more recent formalisation and strengthening of attacks based on raw input data was recently presented by \textcite{DBLP:conf/sp/Carlini017}.
Around the time of publication, distillation \cite{DBLP:conf/sp/PapernotM0JS16} was proposed as a form of hardening for neural networks expected to perform in adversarial settings where evasion attacks might be common.
This work reveals that existing approaches for generating adversarial examples \emph{weren't strong enough} and, accordingly, approaches like defensive distillation are shown to be ineffective.
Some future works refer to the methods they propose as CW-$L_{\{0, 2, \infty\}}$ attacks, according to the target data distance metric to be minimised (respectively, the Hamming, Euclidean and Chebyshev metrics).
Their attacks exceed existing work based on these three well-understood metrics by a more in-depth analysis of the construction of cost functions, a reworked box constraint built around $\tanh(\cdot)$ (as in HDR image tone mapping), and a more nuanced treatment of the effects of discretisation error.
By introducing a \emph{confidence factor} $\kappa$, they are able to explicitly design attacks which are \emph{transferable} between one classifier and its distilled form, or a network derived from the original by black box inference.
All attacks are computed by way of a general stochastic optimiser, such as \emph{Adam} \cite{DBLP:journals/corr/KingmaB14}.
Their work currently establishes the benchmark for future mitigation techniques.

While it is feasible that an attacker could start with a \emph{white-box} understanding of your model to aid in evasion, a more feasible situation is the case where they do not.
\textcite{DBLP:conf/uss/TramerZJRR16} have shown that attackers can infer many classes of learned models from observation (\emph{model extraction attacks}), allowing evasion attacks, model cloning or discovery of confidential training data characteristics.
They are able to demonstrate model extraction on logistic regressors, SVMs, MLPs and NNs with and without confidence values.
Furthermore, from their retrained models they are able to extract images of faces which are ``visually indistinguishable'' from the real training data of a facial recognition classifier.
As far as defences go, they recommend: hiding or rounding confidence values (to limited success); applying differential privacy to mode parameters (an open question); and ensemble methods (which may still be beaten by evasion attacks).

?? RL algorithms based on NNs are just as vulnerable \cite{DBLP:journals/corr/HuangPGDA17}!

?? New work \parencite{DBLP:journals/corr/abs-2002-04599} covering the balancing attack between sensitivity/invariance-based attacks. Sensitivity is what we usually think of (small change which doesn't impact the true label), invariance is a change which \emph{would} change the true label, but is performed in such a way that the model still outputs the old label. Implication: defending against one makes you more sensitive to the other (specifically, attacks within some $\ell_p$ norm ball)---even more sensitive than an undefended network.
?? Even modern provably robust systems based on $\ell_\infty$ \parencite{DBLP:conf/iclr/ZhangCXGSLBH20} are vuln.

\subsection{Defences on above}

?? DIstillation, other papers I read ages ago.

?? Mine the relevant conferences for more...

\paragraph{Poisoning}
?? Poisoning defences. Auror \cite{DBLP:conf/acsac/ShenTS16} relies upon the fact that masked features (i.e. gradient updates) submitted by users tend to have a known distribution.
By performing 2-means cluster detection on \emph{indicative features}, users whose updates consistently fall outside of the benign distribution may be detected and blacklisted.

\paragraph{Evasion}

?? \textcite{DBLP:conf/sp/PapernotM0JS16}---distillation---not read.

In ensemble classification, if many of the individual classifiers disagree then this can represent a high degree of uncertainty about the observed data.
\textcite{DBLP:conf/ndss/SmutzS16} realise that this uncertainty can act as a powerful indicator of an evasion attack in progress, and propose \emph{mutual agreement analysis} as a defence.
When an insufficient amount of the constituent classifiers return the same result, the result returned is that the sample is `uncertain'---suggesting either a new class of data or evidence of attempted evasion.
The approach naturally suits ensemble methods such as \emph{random forests}, but an extension to SVMs is proposed (an ensemble of feature-bagged SVMs); both are shown to be more effective than standard learning in the face of evasion, mimicry and reverse mimicry attacks.
Moreover, the addition of the `uncertain' classification acts as a useful metric for continuous training and evolution.

Adversarial examples typically occur very close to the decision hyperplane; applying too much noise can either accidentally `push' the data into a classification the attacker did not desire, or it may become humanly perceptible.
This principle is exploited by \textcite{DBLP:conf/acsac/CaoG17}, who propose ensemble classification of potentially adversarial data by sampling from the local hypercube---\emph{region-based} classification, rather than standard \emph{point-based} classification.
This differs from the previous approach; we consider here an ensemble of \emph{classifications} around one data point, rather than an ensemble of \emph{classifiers}.
This approach is remarkable because it may rely on any existing classifier $\mathcal{C}$.
This draws from the observation that most of the volume of the surrounding hypercube (of length $r$) lies within the true class region, even for adversarial examples.
To learn the true class of an example, we must then choose the class which admits the largest volume of overlap with the sample region: we may approximate this by drawing samples uniformly from this hypercube (e.g., \num{10000} points), labelling them with the chosen $\mathcal{C}$ and observing the output histogram.
$r$ is chosen such that classifier performance on benign examples does not degrade below that of $\mathcal{C}$.
Given that this design is non-differentiable, an attacker cannot compose an adversarial attack directly even if they know $r$, the sample count and $\mathcal{C}$ exactly---they must craft examples against $\mathcal{C}$ (which \emph{is} differentiable) and use these.
Detection of the standard set of attacks \cite{DBLP:conf/sp/Carlini017} is shown by very convincing results, meaning that attackers must amplify the applied noise, again risking an incorrect target class or very perceptible distortion.
An interesting area that isn't discussed is how different sampling distributions might affect robustness in the face of even this.

?? Newest on these?~\parencite{DBLP:journals/corr/abs-1902-06705}. Attempts to offer concrete methodology in response to very variable analysis/testing and slow research on defences. Most defences posited in response to the rapid development of attacks are shown to be incorrectly or incompletely evaluated. ?? Human and machine classification/decision-making performance are tied in efficacy, while there remains a huge gap between sensitivity to adversarial examples. The norm is to assume attackers have white-box access, because this includes black-box defence.

\section{Summary}
Eh.
