\chapter{Data-driven Networking}

?? Firstly, what is data-driven networking? Probably: making use of measurements, observations, and statistics (run-time, simulation, pen-and-paper) to enhance and improve the operation of a computer network.

?? enhance in what way? what metrics?

\emph{Data-driven networking} is a recent field of research focussed on the automatic control and optimisation of network systems, which has sprung forth due to recent advances in RL and ML.
Many aspects of modern networks, such as congestion control algorithms, or thresholds for differentiating services and flow classification rely heavily upon hand-tuned heuristics.

?? Likely started with lofty goal of ``knowledge plane''~\cite{DBLP:conf/sigcomm/ClarkPRW03}

?? DDN risks~\cite{DBLP:conf/hotnets/BartulovicJBSS17}.

?? DDN paper~\cite{DBLP:conf/comsnets/JiangSSZ17}

?? ``Knowledge''-defined networking~\cite{DBLP:journals/corr/MestresRCBASMMB16}

\section{Use Cases}

?? Optimisation

?? Design

?? Detection / telemetry / inference

?? Refer back to the computer networks chapter: topologies, routing, defence, ... all present problems who are often served and solved by the use of heuristics.

\section{Function approximation}

?? Explain how different approximators work, I suppose?

?? Probably  define from first principles.

\subsection{Accuracy Measures}

?? mean-square error, bias, variance, ...

\subsection{Linear Functions}

?? Linear coding

\subsection{Neural Networks}

?? Deep learning book?~\cite{DBLP:books/daglib/0040158}

?? Neural networks

\section{Learning an approximation}

?? How do we learn a nice policy?

\subsection{Gradient Descent}

?? The standard. Probably space here to reference a good many techniques.

\subsection{Federated Learning}

?? Describe it here

?? Issue? Only works on certain problems (explicitly unsupervised, or easy to acquire local supervised measurements).

?? Core concept: push out most of the work involved in training the network to users/edge node learners, and then combine their models to train a new centralised model (rather than sending all data points over the network (bandwidth and privacy concerns))

Okay, what conditions does it impose on the type of data we want to use for training (right now, at least)?
?? Need trivially (user-)labelled data, to get the high volume we need. (label-able at the edge nodes? is unsupervised remotely worthwhile?)
?? Ex: predictive text works (data is word the user had typed, label is what they picked from the dict OR what they typed by the end.)

Invented by \textcite{DBLP:journals/corr/KonecnyMRR16}.
?? (Probably look at the lead author's shiny new PhD Thesis \cite{DBLP:journals/corr/Konecny17}? Look for more of his stuff?)
?? IDEA -- train a high-quality centralised model when data are divided unevenly over a large number of nodes (read: non-IID).
?? Main problem is that the best-performing algorithms are very much sequential in nature. Even then, many make assumption that all learners see a representative sample.
?? Each node solves a subproblem subject to some quadratic perturbation (as in DANE \cite{DBLP:conf/icml/ShamirS014} (not read!)) but for Stochastic Variance Reduced Gradient (SVRG).
?? Much of the algorithm is correcting for potential sources of bias from the edge-node results before they are combined (averaged very carefully).

\textcite{DBLP:conf/aistats/McMahanMRHA17} examine:
?? Federated Learning to train deep neural nets.
?? More broadly, this looks at non-convex loss functions in general.
?? New Algo---FedAVG seems to outperform FedSVG despite having fewer terms to correct for bias. I've written that it's analogous to dropout (which itself can offer behaviour close to a Bayesian net), check again to see what exactly this refers to.
?? Core idea---still essentially just taking (weighted) average of the policies from edge learners. Pretty important that they're all initialised from the same seed, at least then. Better convexity behaviour than expected.

\subsection{Reinforcement Learning}

?? RL works in tandem with other mathematical training approaches: the key insight is that the structure of an MDP allows external information and model(-free) observation to strengthen or weaken different function responses.

?? The reference~\cite{DBLP:books/lib/SuttonB18}

Automatic, adaptive network control and optimisation requires accurate, recent state to make optimal decisions and to act in a timely manner.
Action execution, computation and training have real costs, which have been shown to negatively affect the performance of asynchronous RL systems~\cite{DBLP:journals/firai/TravnikMSP18}---hence, data-driven networking applications are profoundly affected.
As it stands, state measurement and policy execution require additional hardware and infrastructure, increasing delays and costing rack-space.
Placing learning algorithms, policy execution, and measurement into the network fabric will increase performance, the accuracy of system state and simplify network architectures which use data-driven concepts.

?? Can we relate this to the broader ML notion of concept drift?

\section{Hardware considerations}

?? Put in all the stuff about quantisation here?

\section{Security}

?? Discuss attacks on ML models, techniques, paradigms here.

?? Very respectable source \cite{DBLP:conf/eurosp/PapernotMJFCS16} that they're bad I guess? (Not Read)

?? The full summary paper \cite{DBLP:conf/eurosp/PapernotMSW18}.

\subsection{Attacks on Data-Driven Techniques}

?? Older taxonomy, includes what we now call poisoning, evasion~\cite{DBLP:conf/ccs/BarrenoNSJT06}. Is there an updated?

?? White-box attacks -- evasion, adversarial examples, etc.

\paragraph{Poisoning Attacks}
?? Attacker wants to (permanently) alter the behaviour of a system which is still training in some way.
The key intuition is that an attacker wishes to affect which data points are used in training, and in turn affect the decision boundaries to do... something bad?

?? For systems which assume a stable (stationary) form of normality, it takes an exponential amount of packets with respect to how far the mean must move.
Yet in systems which use a finite number of data points (i.e. modelling non-stationarity) an attacker requires only a linear amount of data if they control a sufficient fraction of the network throughput.
Further findings are more optimistic: if the attacker controls insufficient traffic, then they cannot succeed in appreciably shifting the mean with even an infinite amount of traffic.
It isn't made clear how these findings relate to more complex systems or models, but it will remain an important consideration. \cite{DBLP:journals/jmlr/KloftL10}

?? More modern cite?

\paragraph{Evasion Attacks/Adversarial Examples}

?? Yeah these are a thing (note: haven't actually read most of these aside from the wonderful turtle-rifle 3d printer one).
?? The one, the only, the original \cite{DBLP:journals/corr/SzegedyZSBEGF13}, the application \cite{DBLP:journals/corr/GoodfellowSS14}.

?? Most recently summed up in the SoK paper by \textcite{DBLP:conf/eurosp/PapernotMSW18}.

?? Where are we now? Transform invariant (i.e., photographs \cite{DBLP:journals/corr/KurakinGB16}, 3D model \& 2D transforms \cite{DBLP:journals/corr/AthalyeS17}): what does this mean with regards to the transforms we apply to our input data?

A more recent formalisation and strengthening of attacks based on raw input data was recently presented by \textcite{DBLP:conf/sp/Carlini017}.
Around the time of publication, distillation \cite{DBLP:conf/sp/PapernotM0JS16} was proposed as a form of hardening for neural networks expected to perform in adversarial settings where evasion attacks might be common.
This work reveals that existing approaches for generating adversarial examples \emph{weren't strong enough} and, accordingly, approaches like defensive distillation are shown to be ineffective.
Some future works refer to the methods they propose as CW-$L_{\{0, 2, \infty\}}$ attacks, according to the target data distance metric to be minimised (respectively, the Hamming, Euclidean and Chebyshev metrics).
Their attacks exceed existing work based on these three well-understood metrics by a more in-depth analysis of the construction of cost functions, a reworked box constraint built around $\tanh(\cdot)$ (as in HDR image tone mapping), and a more nuanced treatment of the effects of discretisation error.
By introducing a \emph{confidence factor} $\kappa$, they are able to explicitly design attacks which are \emph{transferable} between one classifier and its distilled form, or a network derived from the original by black box inference.
All attacks are computed by way of a general stochastic optimiser, such as \emph{Adam} \cite{DBLP:journals/corr/KingmaB14}.
Their work currently establishes the benchmark for future mitigation techniques.

While it is feasible that an attacker could start with a \emph{white-box} understanding of your model to aid in evasion, a more feasible situation is the case where they do not.
\textcite{DBLP:conf/uss/TramerZJRR16} have shown that attackers can infer many classes of learned models from observation (\emph{model extraction attacks}), allowing evasion attacks, model cloning or discovery of confidential training data characteristics.
They are able to demonstrate model extraction on logistic regressors, SVMs, MLPs and NNs with and without confidence values.
Furthermore, from their retrained models they are able to extract images of faces which are ``visually indistinguishable'' from the real training data of a facial recognition classifier.
As far as defences go, they recommend: hiding or rounding confidence values (to limited success); applying differential privacy to mode parameters (an open question); and ensemble methods (which may still be beaten by evasion attacks).

?? RL algorithms based on NNs are just as vulnerable \cite{DBLP:journals/corr/HuangPGDA17}!

\subsection{Defences on above}

?? DIstillation, other papers I read ages ago.

?? Mine the relevant conferences for more...

\paragraph{Poisoning}
?? Poisoning defences. Auror \cite{DBLP:conf/acsac/ShenTS16} relies upon the fact that masked features (i.e. gradient updates) submitted by users tend to have a known distribution.
By performing 2-means cluster detection on \emph{indicative features}, users whose updates consistently fall outside of the benign distribution may be detected and blacklisted.

\paragraph{Evasion}

?? \textcite{DBLP:conf/sp/PapernotM0JS16}---distillation---not read.

In ensemble classification, if many of the individual classifiers disagree then this can represent a high degree of uncertainty about the observed data.
\textcite{DBLP:conf/ndss/SmutzS16} realise that this uncertainty can act as a powerful indicator of an evasion attack in progress, and propose \emph{mutual agreement analysis} as a defence.
When an insufficient amount of the constituent classifiers return the same result, the result returned is that the sample is `uncertain'---suggesting either a new class of data or evidence of attempted evasion.
The approach naturally suits ensemble methods such as \emph{random forests}, but an extension to SVMs is proposed (an ensemble of feature-bagged SVMs); both are shown to be more effective than standard learning in the face of evasion, mimicry and reverse mimicry attacks.
Moreover, the addition of the `uncertain' classification acts as a useful metric for continuous training and evolution.

Adversarial examples typically occur very close to the decision hyperplane; applying too much noise can either accidentally `push' the data into a classification the attacker did not desire, or it may become humanly perceptible.
This principle is exploited by \textcite{DBLP:conf/acsac/CaoG17}, who propose ensemble classification of potentially adversarial data by sampling from the local hypercube---\emph{region-based} classification, rather than standard \emph{point-based} classification.
This differs from the previous approach; we consider here an ensemble of \emph{classifications} around one data point, rather than an ensemble of \emph{classifiers}.
This approach is remarkable because it may rely on any existing classifier $\mathcal{C}$.
This draws from the observation that most of the volume of the surrounding hypercube (of length $r$) lies within the true class region, even for adversarial examples.
To learn the true class of an example, we must then choose the class which admits the largest volume of overlap with the sample region: we may approximate this by drawing samples uniformly from this hypercube (e.g., \num{10000} points), labelling them with the chosen $\mathcal{C}$ and observing the output histogram.
$r$ is chosen such that classifier performance on benign examples does not degrade below that of $\mathcal{C}$.
Given that this design is non-differentiable, an attacker cannot compose an adversarial attack directly even if they know $r$, the sample count and $\mathcal{C}$ exactly---they must craft examples against $\mathcal{C}$ (which \emph{is} differentiable) and use these.
Detection of the standard set of attacks \cite{DBLP:conf/sp/Carlini017} is shown by very convincing results, meaning that attackers must amplify the applied noise, again risking an incorrect target class or very perceptible distortion.
An interesting area that isn't discussed is how different sampling distributions might affect robustness in the face of even this.

?? Newest on these?~\parencite{DBLP:journals/corr/abs-1902-06705}. Attempts to offer concrete methodology in response to very variable analysis/testing and slow research on defences. Most defences posited in response to the rapid development of attacks are shown to be incorrectly or incompletely evaluated. ?? Human and machine classification/decision-making performance are tied in efficacy, while there remains a huge gap between sensitivity to adversarial examples. The norm is to assume attackers have white-box access, because this includes black-box defence.

\section{Summary}
Eh.
