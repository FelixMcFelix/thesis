\chapter{Data-driven Networking}
Once we have any computer network as described above, we have a good basis for not only allowing two machines to communicate with one another, but also to do so \emph{efficiently}.
That isn't to say that there is no room to improve on how networks and operating systems provide this functionality.
Many aspects of modern networks, such as \glspl{acr:cca}, thresholds for differentiating services, and flow classification rely heavily upon hand-tuned heuristics.
As in \cref{sec:problems-in-modern-networks}, there is still vast scope to improve on communication latency and throughput, or to avoid and work around deleterious traffic patterns (such as incast communication).
As a result, research into \glspl{acr:cca}, network designs, and routing procedures is very much ongoing.
Crucially, as these operations lie at the core of network operation their solutions tend towards extremely efficient heuristic methods; they must be evaluated per-packet or react as quickly as possible to state change.
Designing new methods for network optimisation then requires deep insight into any problem, its edge cases, and the hardware/performance characteristics of the target environment.\sidenote{This affects how we design such heuristics even on commodity hosts---kernel-space \glspl{acr:cca} (i.e., as part of the TCP stack) are unable to use floating point arithmetic.}

Suppose that, as network administrators or protocol designers, we have access to a reasonable amount of information about the machines, network segments or \glspl{acr:as} under our control---measurements, observations, and statistics taken at run-time, from simulation, or by modelling.
A natural question to ask, then, is whether we can use this data to enhance and improve the operation and use of our network automatically.
Thinking further still, we might wonder whether we can outperform the general (yet useful) heuristics which are widely deployed and researched, tailoring network behaviour according to its environment and traffic patterns.
These questions are the founding principles of \gls{acr:ddn}\sidenote{Alternatively titled \emph{self-driving networks}.}, a recent field of research focussed on the automatic control and optimisation of network systems, which has sprung forth due to recent advances in \gls{acr:ml} and \gls{acr:rl}~\parencite{DBLP:conf/anrw/FeamsterR18,DBLP:journals/pieee/KellererKBBR019}.

The ideas and goals of automated network control have always existed and evolved in one form or another, particularly as computational inference and learning have grown more powerful.
Primarily, these ideas have propagated via position papers offering a `vision of things to come'.
This was first famously formalised as the \emph{knowledge plane}~\parencite{DBLP:conf/sigcomm/ClarkPRW03}, in contrast to the \emph{data} and \emph{control} planes.
This proposal captures not only the above concepts of automation as a means for network control, but also for collaborative or commercial sharing of information between end-hosts, transit \glspl{acr:as}, and organisations to build up a global picture of the needs of the network.
As it turns out, over the past \num{18} years we have moved no closer to such a unified substrate, though automated inference based on the data we \emph{do} have is richly researched.
A later attempt to combine this with \gls{acr:sdn} as \emph{Knowledge}-defined networking~\parencite{DBLP:journals/corr/MestresRCBASMMB16} takes key steps in clarifying the field, through concrete problems and promising \gls{acr:ml} developments, but effectively curtails the scale of knowledge sharing.
\gls{acr:ddn} itself is named and defined by \Textcite{DBLP:conf/comsnets/JiangSSZ17}, who again expand the scope for optimisation beyond network control to include \emph{end-points}; towards application and transport layer optimisation for hosts and servers, as well as control of the underlying fabric.

Starting out with the aim of emphasising and motivating the value of \gls{acr:ddn}, I discuss and introduce some of the recent developments and applications of \gls{acr:ml} and \gls{acr:rl} techniques in computer networking (\cref{sec:use-cases}), before then moving onto to explain the `building blocks' underlying these approaches (\crefrange{sec:function-approximation}{sec:numerical-representations-for-embedded-ml}).
\Cref{sec:ddn-security} then presents an overview of security perspective surrounding current \gls{acr:ml} and \gls{acr:ddn} approaches.
Although this context and its challenges are rapidly evolving, an understanding of security issues is key to offering a complete picture of the viability of \gls{acr:ddn} and the care which must be taken in its research.
Sadly, full examination and further development lies beyond the scope of this thesis---it is, in fact, a thesis-worthy topic in its own right~\parencite{papernot-thesis}.

\section{Use Cases}\label{sec:use-cases}
\input{chapters/p1-introduction/ch2-ddn/use-cases.tex}

\section{Function approximation}\label{sec:function-approximation}

?? Explain how different approximators work, I suppose?

?? Probably  define from first principles.

\subsection{Accuracy Measures}

?? mean-square error, bias, variance, ...

\subsection{Linear Functions}

?? Linear coding

\subsection{Neural Networks}

?? Deep learning book?~\cite{DBLP:books/daglib/0040158}

?? Neural networks

?? GNNs -- GCN, Edge-GNNs from Mirhoseini

\section{Learning an approximation}\label{sec:learning-an-approximation}

?? How do we learn a nice policy?

\subsection{Gradient Descent}

?? The standard. Probably space here to reference a good many techniques.

\subsection{Reinforcement Learning}

?? 2021 --- need to emphasise that we're using a goodness metric to learn a state-action map without an explicit model of the system we're learning to control.

?? Core use-cases -- sparity and delayedness of rewards.

?? Need for high training of DRL is known as \emph{sample complexity}.

?? Should explain the theoretical basis in MDPs, including the Bellman Eqns (which go to values through dynamic programming). Explain that RL is learning w/o knowledge of a state-transition function.

?? Space from direct policy search (ES, Policy gradients) towards value-based \url{https://icml.cc/2015/tutorials/PolicySearch.pdf}

?? Beware: asynchronous in A3C sense means in terms of training from indep traces; not in the sense of ``the world keeps ticking''.

DDPG~\parencite{DBLP:journals/corr/LillicrapHPHETS15}, DPG~\parencite{DBLP:conf/icml/SilverLHDWR14}

Note -- are actor-critic methods needed for continuous action spaces? Might be worth mentioning whjat it takes to learn both discrete and continuous. Need to use different methods to explore, i.e. Ornstein-Uhlenbeck process.

?? Also want to spend some time discussing various action-selection strategies, that the output can be

?? RL works in tandem with other mathematical training approaches: the key insight is that the structure of an MDP allows external information and model(-free) observation to strengthen or weaken different function responses.

?? The reference~\parencite{RL2E}

Automatic, adaptive network control and optimisation requires accurate, recent state to make optimal decisions and to act in a timely manner.
Action execution, computation and training have real costs, which have been shown to negatively affect the performance of asynchronous RL systems~\cite{DBLP:journals/firai/TravnikMSP18}---hence, data-driven networking applications are profoundly affected.
As it stands, state measurement and policy execution require additional hardware and infrastructure, increasing delays and costing rack-space.
Placing learning algorithms, policy execution, and measurement into the network fabric will increase performance, the accuracy of system state and simplify network architectures which use data-driven concepts.

?? Can we relate this to the broader ML notion of concept drift?

\emph{Reinforcement learning} (RL) algorithms are methods of training an \emph{agent} to choose an optimal sequence of actions in pursuit of a given task \cite{RL2E}.
Like most machine learning methods, an RL algorithm uses gradient information to update the parameters used to approximate a function.
In RL contexts, this is a state$\rightarrow$action function known as a \emph{policy}.
When training, agents are given \emph{reward measurements} and a learned policy acts to maximise the \emph{expected discounted reward} received.
When a pre-trained policy is deployed, this signal is not required.

It's useful to consider how these algorithms differ from other ML use cases, such as classifiers.
The main differences lie in how this gradient information is used, and combined with \emph{reward measurements} received from the environment.
Rather than adapting the learned parameters along the gradient using an error value from a target value, because the optimal actions aren't known they adjust values using a \emph{Markov Decision Process} (MDP)---capturing state trajectories to adjust value based on past and future decisions.

Consider the single-step, semi-gradient Sarsa algorithm~\cite[pp. \numrange{217}{221}]{RL2E}:
% \begin{subequations}
% 	\begin{gather}
% 		\delta_t = R_{t+1} + \gamma \acval{S_{t+1}}{A_{t+1}}{\wvec{t}} - \acval{S_t}{A_t}{\wvec{t}},\\
% 		\bm{w}_{t+1} = \bm{w}_{t} + \alpha \delta_t \nabla{\acval{S_t}{A_t}{\wvec{t}}},
% 	\end{gather}%
% 	\label{eqn:sg-sarsa}%
% 	where $\delta_t$ is known as the \emph{temporal-difference} (TD) error, $\acval{S}{A}{\wvec{}}$ denotes the \emph{value} of an action $A$ taken in state $S$ according to the policy $\wvec{}$, and the vector gradient $\nabla$ is taken with respect to $\wvec{}$. $\gamma,\alpha\in[0,1]$ are the discount factor and learning rate (governing the degree of forward planning and policy stability, respectively).
% \end{subequations}
In essence, at each timestep the policy parameters ($\wvec{}$) are increased along the gradient ($\nabla{\acvalblank}$) using a fixed learning rate ($\alpha$) and a computed adjustment ($\delta_t$).
This adjustment is equal to the difference between the chosen action $A$'s value in state $S$ and the reward received ($R_{t+1} - \acval{S_t}{A_t}{\wvec{}}$), \emph{plus} some part of the \emph{next} action's value ($\gamma\acval{S_{t+1}}{A_{t+1}}{\wvec{}}$).
To give some context on the design space here, other algorithms may employ separate state value approximations, use the entirety of an agent's trace, or be tailored to characteristics of the policy approximator (e.g., how neural networks benefit from batching).

\fakepara{RL in asynchronous environments}
There remains some degree of divergence between the theory and implementation of RL agents.
Consider \cref{fig:state-slip}: the traditional formulation of a Markov decision process assumes that an agent receives a new view of the world's state at fixed time intervals, and then decides upon and executes an action instantly.
The reality is that state information takes time to traverse the network, service times are offset by how quickly hosts respond to interrupts and deserialise requests, and action preference lists are often computed via expensive policy approximations.
Action installation also incurs costs in fields such as network administration, initially to contact the controller and then for those actions to be installed via the control plane.

These delays (and variance thereof) add noise to the state-action mapping being learned, which has a potent reduction to learning rate and final accuracy, even for simple grid world tasks according to \textcite{DBLP:journals/firai/TravnikMSP18}.
They in turn show that reordering algorithmic steps can reduce these costs for online 1-step algorithms, but that reducing this further requires detailed agent-environment co-design.
This principle has influenced the design of real network use cases, such RL-based congestion-control algorithms~\parencite{DBLP:journals/corr/abs-1910-04054}, showing that asynchrony is necessary for high-speed applications.
Achieving this often requires that state measurements are combined or coalesced~\parencite{DBLP:journals/corr/abs-1910-04054,DBLP:journals/tnsm/SimpsonRP20} while expensive computations are ongoing.
`Stopping the world' in the algorithmic sense causes significant performance degradation, as inference takes up to \SI{30}{\milli\second} in the above work on congestion control, or any time-sensitive control problems.

\begin{figure}
	\begin{subfigure}{0.45\linewidth}
		\resizebox{\linewidth}{!}{
			\centering
			\begin{tikzpicture}
				\node[circle, draw] (state) {$S$};
				\node[circle, draw] (state') at ($(state) + (0, -2)$) {$S'$};
				
				\node (agent) at ($(2, 0) + (state)$) {Agent};
				\node[below of=agent] (action) {Action $A$};
				
				\node[right of=state'] {$+ R$};
				
				\draw[->] (state) -- (state') node[midway, right] {$A$};
				
				\draw[dotted, ->, bend left = 30] (state) -- (agent);
				\draw[->] (agent) -- (action);
				\draw[dotted, ->] (action) -- (state);
		\end{tikzpicture}}
		\caption{Theory: state measurement, action computation, and learning are zero-cost.}
	\end{subfigure}
	\hspace{0.05\linewidth}
	\begin{subfigure}{0.45\linewidth}
		\resizebox{0.75\linewidth}{!}{
			\centering
			\begin{tikzpicture}
				\node[circle, draw] (state) {$S$};
				\node[circle, draw] (state') at ($(state) + (0, -1.5)$) {$S'$};
				\node[circle, draw] (state'') at ($(state') + (0, -1.5)$) {$S''$};
				
				\node (agent) at ($(2, 0) + (state)$) {Agent};
				\node[below of=agent] (action) {Action $A$};
				
				\node[right of=state''] {$+ R$};
				
				\draw[->] (state) -- (state') node[midway, right] {$\varnothing$};
				\draw[->] (state') -- (state'') node[midway, right] {$A$};
				
				\draw[dotted, ->, bend left = 30] (state) -- (agent) node[midway, above] {$t_1$};
				\draw[->] (agent) -- (action) node[midway, right] {$t_2$};
				\draw[dotted, ->] (action) -- (state') node[midway, below] {$t_3$};
		\end{tikzpicture}}
		\caption{Reality: costs of measurement and action lead to \emph{state drift}---over a time delay $t_1+t_2+t_3$, inaction transforms state $S$ into $S'$.}
	\end{subfigure}
	\caption{Asynchronous RL delays and state slippage (policy updates omitted).\label{fig:state-slip}}
\end{figure}

%?? Find some cites citing the relevance of this problem wrt. self-driving cars, robotics, etc.

The solution we propose is to make use of the recent wave of programmable network devices to \emph{bring reinforcement learning to the dataplane}---referring again to \cref{fig:state-slip}, we would place place state measurement ($t_1$), low-cost decision-making processes ($t_2$), and controlled systems ($t_3$) as close to one another as possible.
In networks, actions are most likely to be installed on backbone switches, bump-in-the-wire NICs or middleboxes, and in the NICs of end-hosts.
Ideally, these functions which comprise an RL agent would all be collocated on the same chip or device, but this is easier said than done. 
Both programmable devices and the network environment make this more difficult, as we'll examine in the sequel.

\section{Collaborative training}
As something of a counterpoint to the main focus of this thesis, much community interest has been given to how networks can improve, accelerate, or innovate in \gls{acr:ml} model training.

?? This is how the network can, in turn, benefit the development of ML/RL appls.

\subsection{Multiagent methods}

\subsection{Distributed training}

\subsection{Federated Learning}

?? Describe it here

?? Issue? Only works on certain problems (explicitly unsupervised, or easy to acquire local supervised measurements).

?? Core concept: push out most of the work involved in training the network to users/edge node learners, and then combine their models to train a new centralised model (rather than sending all data points over the network (bandwidth and privacy concerns))

Okay, what conditions does it impose on the type of data we want to use for training (right now, at least)?
?? Need trivially (user-)labelled data, to get the high volume we need. (label-able at the edge nodes? is unsupervised remotely worthwhile?)
?? Ex: predictive text works (data is word the user had typed, label is what they picked from the dict OR what they typed by the end.)

Invented by \textcite{DBLP:journals/corr/KonecnyMRR16}.
?? (Probably look at the lead author's shiny new PhD Thesis \cite{DBLP:journals/corr/Konecny17}? Look for more of his stuff?)
?? IDEA -- train a high-quality centralised model when data are divided unevenly over a large number of nodes (read: non-IID).
?? Main problem is that the best-performing algorithms are very much sequential in nature. Even then, many make assumption that all learners see a representative sample.
?? Each node solves a subproblem subject to some quadratic perturbation (as in DANE \cite{DBLP:conf/icml/ShamirS014} (not read!)) but for Stochastic Variance Reduced Gradient (SVRG).
?? Much of the algorithm is correcting for potential sources of bias from the edge-node results before they are combined (averaged very carefully).

\textcite{DBLP:conf/aistats/McMahanMRHA17} examine:
?? Federated Learning to train deep neural nets.
?? More broadly, this looks at non-convex loss functions in general.
?? New Algo---FedAVG seems to outperform FedSVG despite having fewer terms to correct for bias. I've written that it's analogous to dropout (which itself can offer behaviour close to a Bayesian net), check again to see what exactly this refers to.
?? Core idea---still essentially just taking (weighted) average of the policies from edge learners. Pretty important that they're all initialised from the same seed, at least then. Better convexity behaviour than expected.

\section{Numerical representations for embedded ML}\label{sec:numerical-representations-for-embedded-ml}
A key feature universally lacking from the target environments is floating-point arithmetic support.
Luckily, the task of bringing machine learning models to resource-constrained environments without these capabilities is well-studied.
Quantisation and alternative data formats have been suggested to make ML inference feasible on resource- and power-limited platforms, work around hardware constraints, or compute faster and more efficiently.
Lower bit depths reduce memory footprints, and improve throughput in designs such as \emph{bfloat16}~\parencite{bfloat16-blog} in Google TPUs~\parencite{DBLP:journals/sigops/XieDMKVZT18}, \emph{hfp8}~\parencite{DBLP:conf/nips/SunCCWVSCZG19}, and other floating point formats~\parencite{DBLP:journals/corr/abs-2007-01530}.
In many cases, accuracy losses for doing so are negligible.
Much of this work goes further still towards integer~\parencite{tensorrt-8bit} or binarised~\parencite{DBLP:journals/corr/MiyashitaLM16,DBLP:conf/eccv/RastegariORF16,DBLP:journals/corr/KimS16,DBLP:conf/nips/HubaraCSEB16} representations, sacrificing dynamic range for simpler arithmetic operations.
The works mentions above use similar techniques to make inference tractable on network hardware, i.e., to bring neural networks to the dataplane~\parencite{DBLP:journals/corr/abs-2009-02353,DBLP:conf/sigcomm/SanvitoSB18,DBLP:journals/corr/abs-1801-05731}.

To make RL workloads feasible under such constraints, we propose the use of quantised, fixed-point representations such as $Qm.n$ (i.e., $m$\si{bit} signed integers with an $n$\si{bit} fractional part), which allow us to evaluate and update policies using only integer arithmetic.
This is not only essential in performing this work, but also serves as a mechanism for reducing the processing and memory costs of function approximation ($t_2$, \cref{fig:state-slip}).

When combined with the above P4-driven techniques for in-network flow/device measurement, we at last have the mechanisms to collocate the key processes of an RL agent.
Moreover, the base P4 dataplane can be used to simplify parsing logic and offer runtime control over which flows/packets are monitored, alongside other packet actions.
This again fits our goals of integrating RL techniques directly within bump-in-the-wire installations and at end-hosts.

\section{Challenges}\label{sec:ddn-challenges}

?? Include some good, high-level discussion of sensitivity to training environment (i.e., many authors willing to use traces) \textcite{DBLP:conf/hotnets/BartulovicJBSS17}, yet there's still interesting use-cases in being able to do so \textcite{DBLP:conf/hotnets/LecuyerLNSSS17}.
?? Main problem with traces: not modelling/learning how the controlled system acts in response to change.
?? Opinion: this is a holdover from the initial wave of people throwing ML algos at datasets and getting papers out. Even the correct handling of this (i.e., using RL to solve static problem instances derived like NP-optimisation problems) limits the field in my opinion.
?? "includes the tacit assumption that the model will not change in response to the input", which is as true in science as it is in Human systems (a view I am not alone in sharing (cite Hamming book))
?? although it can be difficult to know ahead of time, it's worth considering whether the problem ``bites back'' or is adversarial in some way; an ongoing control problem is altogether different from an offline optimisation task. It's unlikely that in an optimised chip allocation, programs will immediately start acting differently (as compared to a network where our actions induce various modes in \glspl{acr:cca})

?? Seems to always require dedicated co-design? (https://www.usenix.org/conference/nsdi21/presentation/fu)

?? Interpretability (https://dl.acm.org/doi/10.1145/3387514.3405859),
* https://dl.acm.org/doi/10.1145/3341216.3342218 -- Verifying Deep-RL-Driven Systems
* https://dl.acm.org/doi/10.1145/3341216.3342210 -- Cracking Open the Black Box: What Observations Can Tell Us About Reinforcement Learning Agents
* Verification nowadays~\parencite{DBLP:conf/sigcomm/EliyahuKKS21,drl-verification-2}

?? Point to security as a big challenge.

?? Relate these to the security \emph{problem domain}? I.e., as seen by \citeauthor{DBLP:conf/sp/SommerP10}.

?? Can exhibit issues with unseen data or operation modes, inability to converge, or insane resource use as seen in \gls{acr:cca} design~\parencite{DBLP:conf/sigcomm/AbbaslooYC20}.

\section{Security}\label{sec:ddn-security}

?? Discuss attacks on ML models, techniques, paradigms here.

?? Very respectable source \cite{DBLP:conf/eurosp/PapernotMJFCS16} that they're bad I guess? (Not Read)

?? The full summary paper \cite{DBLP:conf/eurosp/PapernotMSW18}.

?? Put this somewhere: can extract portions of training data w/ carefully crafted inputs~\parencite{DBLP:journals/corr/abs-2012-07805}. Not read, though maybe relate to privacy in general? I.e., differentially private methods.

\subsection{Attacks on Data-Driven Techniques}\label{sec:attacks-on-data-driven-techniques}

?? Older taxonomy, includes what we now call poisoning, evasion~\cite{DBLP:conf/ccs/BarrenoNSJT06}. Is there an updated?

?? White-box attacks -- evasion, adversarial examples, etc.

\paragraph{Poisoning Attacks}
?? Attacker wants to (permanently) alter the behaviour of a system which is still training in some way.
The key intuition is that an attacker wishes to affect which data points are used in training, and in turn affect the decision boundaries to do... something bad?

?? For systems which assume a stable (stationary) form of normality, it takes an exponential amount of packets with respect to how far the mean must move.
Yet in systems which use a finite number of data points (i.e. modelling non-stationarity) an attacker requires only a linear amount of data if they control a sufficient fraction of the network throughput.
Further findings are more optimistic: if the attacker controls insufficient traffic, then they cannot succeed in appreciably shifting the mean with even an infinite amount of traffic.
It isn't made clear how these findings relate to more complex systems or models, but it will remain an important consideration. \cite{DBLP:journals/jmlr/KloftL10}

?? More modern cite?

\paragraph{Evasion Attacks/Adversarial Examples}

?? Be clear---this is still evolving today!

?? Yeah these are a thing (note: haven't actually read most of these aside from the wonderful turtle-rifle 3d printer one).
?? The one, the only, the original \parencite{DBLP:journals/corr/SzegedyZSBEGF13}, the application \parencite{DBLP:journals/corr/GoodfellowSS14}.
?? Second one here suggests that part of the weakness is that models fall back on their heavily linear components -- verify this.

?? Most recently summed up in the SoK paper by \textcite{DBLP:conf/eurosp/PapernotMSW18}.

?? Where are we now? Transform invariant (i.e., photographs \cite{DBLP:journals/corr/KurakinGB16}, 3D model \& 2D transforms \cite{DBLP:journals/corr/AthalyeS17}): what does this mean with regards to the transforms we apply to our input data?

A more recent formalisation and strengthening of attacks based on raw input data was recently presented by \textcite{DBLP:conf/sp/Carlini017}.
Around the time of publication, distillation \cite{DBLP:conf/sp/PapernotM0JS16} was proposed as a form of hardening for neural networks expected to perform in adversarial settings where evasion attacks might be common.
This work reveals that existing approaches for generating adversarial examples \emph{weren't strong enough} and, accordingly, approaches like defensive distillation are shown to be ineffective.
Some future works refer to the methods they propose as CW-$L_{\{0, 2, \infty\}}$ attacks, according to the target data distance metric to be minimised (respectively, the Hamming, Euclidean and Chebyshev metrics).
Their attacks exceed existing work based on these three well-understood metrics by a more in-depth analysis of the construction of cost functions, a reworked box constraint built around $\tanh(\cdot)$ (as in HDR image tone mapping), and a more nuanced treatment of the effects of discretisation error.
By introducing a \emph{confidence factor} $\kappa$, they are able to explicitly design attacks which are \emph{transferable} between one classifier and its distilled form, or a network derived from the original by black box inference.
All attacks are computed by way of a general stochastic optimiser, such as \emph{Adam} \cite{DBLP:journals/corr/KingmaB14}.
Their work currently establishes the benchmark for future mitigation techniques.

While it is feasible that an attacker could start with a \emph{white-box} understanding of your model to aid in evasion, a more feasible situation is the case where they do not.
\textcite{DBLP:conf/uss/TramerZJRR16} have shown that attackers can infer many classes of learned models from observation (\emph{model extraction attacks}), allowing evasion attacks, model cloning or discovery of confidential training data characteristics.
They are able to demonstrate model extraction on logistic regressors, SVMs, MLPs and NNs with and without confidence values.
Furthermore, from their retrained models they are able to extract images of faces which are ``visually indistinguishable'' from the real training data of a facial recognition classifier.
As far as defences go, they recommend: hiding or rounding confidence values (to limited success); applying differential privacy to mode parameters (an open question); and ensemble methods (which may still be beaten by evasion attacks).

?? RL algorithms based on NNs are just as vulnerable \cite{DBLP:journals/corr/HuangPGDA17}!

?? New work \parencite{DBLP:journals/corr/abs-2002-04599} covering the balancing attack between sensitivity/invariance-based attacks. Sensitivity is what we usually think of (small change which doesn't impact the true label), invariance is a change which \emph{would} change the true label, but is performed in such a way that the model still outputs the old label. Implication: defending against one makes you more sensitive to the other (specifically, attacks within some $\ell_p$ norm ball)---even more sensitive than an undefended network.
?? Even modern provably robust systems based on $\ell_\infty$ \parencite{DBLP:conf/iclr/ZhangCXGSLBH20} are vuln.

\subsection{Defences on above}

?? DIstillation, other papers I read ages ago.

?? Mine the relevant conferences for more...

\paragraph{Poisoning}
?? Poisoning defences. Auror \cite{DBLP:conf/acsac/ShenTS16} relies upon the fact that masked features (i.e. gradient updates) submitted by users tend to have a known distribution.
By performing 2-means cluster detection on \emph{indicative features}, users whose updates consistently fall outside of the benign distribution may be detected and blacklisted.

\paragraph{Evasion}

?? \textcite{DBLP:conf/sp/PapernotM0JS16}---distillation---not read.

In ensemble classification, if many of the individual classifiers disagree then this can represent a high degree of uncertainty about the observed data.
\textcite{DBLP:conf/ndss/SmutzS16} realise that this uncertainty can act as a powerful indicator of an evasion attack in progress, and propose \emph{mutual agreement analysis} as a defence.
When an insufficient amount of the constituent classifiers return the same result, the result returned is that the sample is `uncertain'---suggesting either a new class of data or evidence of attempted evasion.
The approach naturally suits ensemble methods such as \emph{random forests}, but an extension to SVMs is proposed (an ensemble of feature-bagged SVMs); both are shown to be more effective than standard learning in the face of evasion, mimicry and reverse mimicry attacks.
Moreover, the addition of the `uncertain' classification acts as a useful metric for continuous training and evolution.

Adversarial examples typically occur very close to the decision hyperplane; applying too much noise can either accidentally `push' the data into a classification the attacker did not desire, or it may become humanly perceptible.
This principle is exploited by \textcite{DBLP:conf/acsac/CaoG17}, who propose ensemble classification of potentially adversarial data by sampling from the local hypercube---\emph{region-based} classification, rather than standard \emph{point-based} classification.
This differs from the previous approach; we consider here an ensemble of \emph{classifications} around one data point, rather than an ensemble of \emph{classifiers}.
This approach is remarkable because it may rely on any existing classifier $\mathcal{C}$.
This draws from the observation that most of the volume of the surrounding hypercube (of length $r$) lies within the true class region, even for adversarial examples.
To learn the true class of an example, we must then choose the class which admits the largest volume of overlap with the sample region: we may approximate this by drawing samples uniformly from this hypercube (e.g., \num{10000} points), labelling them with the chosen $\mathcal{C}$ and observing the output histogram.
$r$ is chosen such that classifier performance on benign examples does not degrade below that of $\mathcal{C}$.
Given that this design is non-differentiable, an attacker cannot compose an adversarial attack directly even if they know $r$, the sample count and $\mathcal{C}$ exactly---they must craft examples against $\mathcal{C}$ (which \emph{is} differentiable) and use these.
Detection of the standard set of attacks \cite{DBLP:conf/sp/Carlini017} is shown by very convincing results, meaning that attackers must amplify the applied noise, again risking an incorrect target class or very perceptible distortion.
An interesting area that isn't discussed is how different sampling distributions might affect robustness in the face of even this.

?? Newest on these?~\parencite{DBLP:journals/corr/abs-1902-06705}. Attempts to offer concrete methodology in response to very variable analysis/testing and slow research on defences. Most defences posited in response to the rapid development of attacks are shown to be incorrectly or incompletely evaluated. ?? Human and machine classification/decision-making performance are tied in efficacy, while there remains a huge gap between sensitivity to adversarial examples. The norm is to assume attackers have white-box access, because this includes black-box defence.

\section{Summary}
Eh.

\subsection{Efficient policy approximation}
Some problems either evolve over time in unpredictable ways, or cannot be easily modelled.
This can then make \emph{online} learning of the task an attractive prospect, using a single available stream of experience.
\emph{Deep neural networks} (DNNs), particularly when used as the basis for an RL agent's policy, require vast amounts of experience to converge on an accurate parameter set.
In many problem domains, this equates to training from compute-years worth of distributed offline simulations, which is at odds with the need to adapt to changes in the underlying problem \emph{as they happen}.
Achieving stable learning requires sizeable batches of gradients to be computed, potentially using the entire experience replay from each simulation.
Although there has been a vested interest in efficiently \emph{executing} more complex function approximators such as DNNs in-NIC~\parencite{DBLP:journals/corr/abs-2002-08987,DBLP:journals/corr/abs-2009-02353,DBLP:conf/sigcomm/SanvitoSB18,DBLP:journals/corr/abs-1801-05731,langlet-ml-netronome}, the computational cost of gradient computation via backpropagation remains too significant on embedded hardware.

%?? Probably want some cites on the need for batching in NN methods, even though this is understood. DL book?

We then return again to what classical RL methods can offer us; in particular, the simple linear function approximation given by \emph{tile coding} \cite[pp.\ \numrange{217}{221}]{RL2E}.
The idea is simple: a policy is represented by sets of tiles, each covering one or more dimensions of the input state with several overlapping tilings (offset stepwise to provide generalisation).
A state vector produces a single `hit' in each tiling, which provides both the set of action preference lists to sum during inference \emph{and} the set of tiles to update at the next timestep.
Crucially, this internal representation ensures that there are no data dependencies between any tile hit calculations for an input state vector.
This has important benefits for parallelisation.
To select an action, we can produce a task for each tiling---retrieving the list of action values contributed by the tile activated by the input state.
An aggregate step sums up all action preference lists, and the action with the highest value is then selected.
Updating the policy (with a new $\delta_t$ value) has no aggregate step, and as tile indices map to disjoint memory regions no locks are required.

For instance, a policy with $m$ sets of tiles (each having its own set of input dimensions), each containing $n$ overlapping tilings then creates $m \times n$ separate tasks.\sidenote{Retrieving each individual action's value can be considered a work item (giving $m \times n \times a$ tasks when there are $a$ actions). However, this requires the tile coding operation to repeated $a$ times per tile, which is likely extremely wasteful outside of extreme degrees of parallelism.}
As an example, a real-world policy size which we examine later (\cref{sec:experimental-setup}) with one bias tile and \num{16} sets containing \num{8} tilings creates \num{129} work items.

Computing a tile hit requires one division per dimension, which is an expensive addition to the ALU operations we require from a platform.
However, if the width of tiles are fixed at powers-of-two, then these may be replaced with right bitshifts.
As we shall see shortly, this aligns with our choice of numerical representation.
