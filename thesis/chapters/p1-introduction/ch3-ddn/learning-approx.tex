Having introduced function approximation schemes of relevance to this thesis (and the most prominent in \gls{acr:ddn} at present), we now turn our attention to how these approximators are trained in practice.
Due to its relevance in \gls{acr:dnn} and \gls{acr:drl}, this comprises a brief discussion of gradient descent techniques and stochastic optimisers, followed by a more in-depth introduction to and overview of the field of \gls{acr:rl}.
This allows us to comment on these methods' (and key variations') suitability to network control problems and different execution environments (primarily \gls{acr:pdp} hardware) as required by \cref{chap:in-net-rl}.
Tying these back to the realities of \gls{acr:ddn}, many of these techniques and advances are likely feasible in a \gls{acr:pdp} context with small datasets and no minibatches, such as \gls{acr:sgd} and its variants, but their use is left to future work.
Finally, this allows us to consider various problems in deployment and system design where we must expand or diverge from baseline algorithms---(a)synchrony,  exploration, and dynamic learning---as needed by applications (\cref{chap:ddos-rl,chap:seidr}) or \gls{acr:pdp} co-designed algorithms (\cref{chap:in-net-rl}).

We can begin by assuming that our approximate function is backed by some \emph{parameter vector} $\wvec{}$, such that $\hat{\symbfit{y}} = f{\left(\symbfit{x}~|~\wvec{}\right)}$---an input vector $\symbfit{x}$ produces some output $\hat{\symbfit{y}}$, and ideally after training $\hat{\symbfit{y}}\eqsim\symbfit{y}$ across all input $\symbfit{x}$.
For all intents and purposes, we can understand $\wvec{}$ as a large block of real numbers residing in \gls{acr:ram}, and that we update this block over time.
We may not have access to a ground-truth $\symbfit{y}$ (i.e., in the \gls{acr:rl} case), but we generally assume that there does exist a best-fitting or optimal output.
In case we do have the true $\symbfit{y}$ values, we may refer to the complete set of training inputs and ground-truth outputs as $\symbfit{X}$.
Crucially, we require that $f$ is differentiable with respect to $\wvec{}$: if we define some scalar performance metric $J$ which assesses the quality of an output $\hat{\symbfit{y}}$, then $\nabla_{\wvec{}}{J}$ offers a direction in $\wvec{}$.\sidenote{In the definition of $J$, we naturally expand occurrences of $\hat{\symbfit{y}}$ in terms of $f$, and thus $\wvec{}$. Additionally, $J$ may be defined over any subset (or all) of the training set $\symbfit{X}$.}
This output gradient is simply a vector which represents the direction in parameter-space which  would have the largest \emph{increase} in the value of $J$ if followed.
The approaches presented in \cref{sec:function-approximation} are differentiable as required.

\subsection{Gradient descent}
%Supervised and unsupervised methods alike typically offer clear ways to map from the model parameters we have at this moment, and our performance on a training dataset, into a \emph{loss function}.
Most supervised \gls{acr:ml} problems are defined in terms of a \emph{loss function} $L$, such that differences between our output $\hat{\symbfit{y}}$ and $\symbfit{y}$ values are penalised.
Some examples in use today include the mean absolute and mean squared errors ($\ell_1$ and $\ell_2$ loss) in regression tasks, or (categorical) cross-entropy loss in classification.
Naturally, these are differentiable, but as we aim to \emph{minimise} loss we then \emph{subtract} the gradient from $\wvec{}$; this concept underlies \emph{gradient descent}.
Gradient descent is the iterative process of optimising our parameter vector $\wvec{}$, using all input data and labels $\symbfit{X}$ at each iteration:
\begin{equation}
	\wvec{t+1} = \wvec{t} - \alpha\nabla_{\wvec{t}}{L\left(\symbfit{X}~|~\wvec{t}\right)}
\end{equation}
Every iteration, we move the parameter vector a small step in the direction which would optimise its overall performance; this is the learning rate hyperparameter $\alpha\in\left(0,1\right)$.
However, re-evaluating the loss function over all input data becomes intractable in the case that either the function is expensive to compute on a moderately-sized dataset, or the dataset itself is monstrously large.
Both are often the case in \gls{acr:dnn} training.
It is for this reason that \gls{acr:sgd} and related algorithms are typically employed in \gls{acr:ml} model training, particularly for \glspl{acr:dnn}.
\gls{acr:sgd} modifies the above formula such that individual, randomly chosen samples (or larger minibatches) are used as the input for the loss function rather than the entire dataset.
This only approximates the loss gradient at $\wvec{}$ on the input data, but in practice this is key for the training of modern \gls{acr:ml} techniques---$\alpha$ may be reduced over time and the dataset reshuffled as necessary until convergence.

While \gls{acr:sgd} provides the theoretical underpinning for more efficient \gls{acr:ml} training, in practice the difficulty of choosing $\alpha$ with regard to different parameter sensitivity has led to more adaptive methods.
These also introduce the notion of per-parameter learning rates.
\emph{AdaGrad}~\parencite{DBLP:journals/jmlr/DuchiHS11} gives each parameter its own learning rate $\alpha$, divided by the square root of the sum of squared gradient values (i.e., large derivatives in a parameter lead to a greater reduction in learning rate).
\emph{RMSProp}~\parencite{rmsprop} converts this accumulation into an exponentially-weighted moving average, better handling the non-convex loss behaviour seen in \gls{acr:dnn} training.
\emph{Adam}~\parencite{DBLP:journals/corr/KingmaB14} includes momentum terms (to carry a general direction in gradient across several steps) and additional bias correction, giving advantages in the early stages of training.

It should be noted that the above discussion is a very cursory treatment of the subject, primarily intended to contrast the in-depth discussion of \gls{acr:rl} methods in the remainder of the section.
Similarly, learning-centric modifications to loss functions such as regularisation terms or function-specific regularisation strategies (to mitigate overfitting) are also out of scope.
For more details, readers should refer to more specialised texts such as \Textcite{DBLP:journals/siamrev/BottouCN18, DBLP:books/daglib/0040158}.

\begin{marginfigure}
	\centering
	\resizebox{0.9\linewidth}{!}{\input{diagrams/rl/toy-forward-plan}}
	\caption[A motivating example for MDPs in handling delayed rewards.]{A toy example showing how the \emph{delayed rewards} which an \gls{acr:rl} agent learns to maximise. Consider a traveller journeying towards a castle in the south, with $R$ being some aggregate of food, water and rest. Learning only to optimise \emph{immediate} rewards attached to actions would lead our traveller to choose an overall worse path---going thirsty in the desert!}\label{fig:toy-rl-motiv}
\end{marginfigure}

\subsection{Reinforcement learning}
When we aim to optimise for decisions and estimations in classification, clustering, and regression, it suffices to apply gradient descent and similar optimisers to minimise this loss function alone.
How might things differ when we want to design some \emph{agent}---an actor who sees and acts on a system for many timesteps---to interact with the world?
Consider a toy example in \cref{fig:toy-rl-motiv}: an oracle with global knowledge knows that a hypothetical traveller would best be served by going through the swamp, even if it appears the worse of two choices.
Yet if we applied simple input classification to choose our path at each turn, we would always act poorly without some way of propagating knowledge of later choices' value.
It becomes more complex to collect training data once we realise that our actions move the world forward and change it (thus we may not be able to rewind a simulation to explore alternative choices), and that we might need to explore apparently suboptimal choices for some time to be better off in the long-term.

\glsxtrfull{acr:rl} algorithms are methods of training such an agent to choose an optimal sequence of actions in pursuit of a given task~\parencite{RL2E}, and neatly encode these training- and run-time notions of \emph{exploration} and \emph{exploitation}.
An agent is typically defined by its \emph{policy} $\pi$, which chooses an action in response to an input state.
These powerful techniques effectively use a \emph{reward} metric to learn a state-action mapping without any explicit model of the system they're learning to control---even when reward signals are sparse or come with some delay after an initial choice as \cref{fig:toy-rl-motiv} exemplifies.

To achieve the goals described above, we first make the assumption that the task we're attempting to solve is structured as an \gls{acr:mdp}.
In an \gls{acr:mdp}, we break the world or target problem down into a set of \emph{states} ($\mathcal{S}$), \emph{actions} ($\mathcal{A}$), and \emph{reward measurements} ($\mathbb{R}$).
To relate this to computer networks, an example state space would be a vector of buffer occupancies in a switch, an action space would be the priority to assign some new flow which has arrived, and the reward might be the proportion of finished flows which achieved comfortably low \glspl{acr:fct}.
We then consider our sequence of interactions in discrete \emph{timesteps}---at the present time $t$, an agent observes $s_t \in \mathcal{S}$, chooses some $a_t \in \mathcal{A}$, and then observes their change in the world as a new state $s_{t+1} \in \mathcal{S}$ and a reward measure $r_{t+1} \in \mathbb{R}$.
As required, we can qualify these further, e.g., the set of actions we can take from a state $s$ may be limited to $A_s \subseteq \mathcal{A}$.
Acting optimally then consists of maximising the sum of rewards over some time horizon.
This is captured up to an infinite horizon by the concept of the \emph{expected discounted return}:
\begin{equation}
	\mathbb{E}\left[\sum_{t=0}^{\infty}\gamma^{t}R_t\right]
\end{equation}
where the \emph{discount} $\gamma \in \left[0,1\right]$ controls how ``forward-planning'' an agent may be\sidenote{Setting $\gamma=0$ defines a `myopic' agent, where no intermediate loss of reward is allowed. Choosing $\gamma=1$ will prioritise future rewards, with no concern over how long it may take to achieve those rewards. This hyperparameter features in many \gls{acr:rl} algorithms; in practice, we choose values closer to \num{1}.}, always choosing $a_t \sim \pi\left(a_t~|~s_t,\wvec{t}\right)$.
While dense, this formalism essentially captures and describes the performance of our agent over all possible run-lengths from all starting states (i.e., over all \emph{episodes}), and in general this is what \gls{acr:rl} algorithms are designed to maximise.

\glspl{acr:mdp} assume that there are stationary, well-defined (though stochastic) transitions between these states.
For any $s, s' \in \mathcal{S}, a \in A_s$, an \gls{acr:mdp} is defined by a state transition function, $\operatorname{P_a}\left(s,s'\right)$ which returns the probability that we will land in $s'$ after taking action $a$ in state $s$, and a function returning the \emph{expected reward} $\operatorname{R_a}\left(s,s'\right)$ for each valid transition.
If we have all this information, then we can apply the Bellman equation~\parencite{10.2307/24900506}, a dynamic programming algorithm, to solve for an optimal policy.
This allows us to assign a \emph{value} $v_\pi{\left(s\right)}$ to each state (the expected return over all choices in the current state), and $q_\pi{\left(s,a\right)}$ to each action, and then choose maximal-value actions at each timestep.
In real-world scenarios however, we usually lack this knowledge; this either requires involved modelling, or is rendered intractable by a continuous or combinatorially large state space.

Like most \gls{acr:ml} methods, modern \gls{acr:rl} algorithms use gradient information to update the parameters used to approximate a function.
How \gls{acr:rl} differs is that it aims to learn an optimal policy without any knowledge of the \gls{acr:mdp} apart from its state and action space---\emph{model-free}.
Knowledge of the \emph{trajectories} followed by agents (i.e., state-action-reward tuples at all timesteps) is then used to compute target values and adjustments for the parameter set $\wvec{}$ for any function approximator.
By requiring that our policy approximation $\pi\left(a~|~s, \wvec{} \right)$ is differentiable, \gls{acr:rl} works in tandem with any of the function approximation approaches described in \cref{sec:function-approximation}.\sidenote{Note the policy $\pi$ does not require any knowledge of rewards, except when training. This allows for \gls{acr:rl} agents to be trained using simulated systems or data, and then deployed in an environment where they cannot access this knowledge due to sampling cost. If these measures are present in deployment, then \gls{acr:rl} can learn online.}

%\gls{acr:rl} works in tandem with other mathematical training approaches: the key insight is that the structure of an MDP allows external information and model(-free) observation to strengthen or weaken different function responses.

\subsection{Demonstrating RL: Sarsa}\label{sec:demo-rl-sarsa}
While many \gls{acr:rl} algorithms have been developed (each of which making quite different choices on how to apply trajectory data), it is likely most helpful to start with a concrete point in the design space \emph{before} listing their full variety.
Doubly so here, as this particular algorithm---single-step, semi-gradient Sarsa~\parencite[pp. \numrange{217}{221}]{RL2E}---underpins \cref{chap:ddos-rl,chap:in-net-rl}.
The Sarsa algorithm considers only one transition at a time: a pair of states $s_t,s_{t+1}$, their accompanying actions $a_t,a_{t+1}$, and the reward $r_{t+1}$ accompanying $s_{t+1}$.
This is why it is defined as \emph{single-step}, and as such it does not require a completed trajectory for learning.

%Assume first that $\wvec{}$ is a large block of real numbers residing in \gls{acr:ram}, and that we update this block through time.
As Sarsa is a \emph{value-based} method, an agent operates by defining an approximate \emph{value} function $\acval{s}{a}{\wvec{}}$ to each action $a$ it can take from $s$, typically choosing the action with the highest value.
Note that $\acvalblank$ is completely arbitrary, and may be any differentiable approximator.
We can then update $\wvec{}$ over time as follows:
\begin{subequations}
	\begin{gather}
		\delta_t = r_{t+1} + \gamma \acval{s_{t+1}}{a_{t+1}}{\wvec{t}} - \acval{s_t}{a_t}{\wvec{t}},\label{eqn:sg-sarsa-td}\\
		\wvec{t+1} = \wvec{t} + \alpha \delta_t \nabla{\acval{s_t}{a_t}{\wvec{t}}},\label{eqn:sg-sarsa-update}
	\end{gather}%
	\label{eqn:sg-sarsa}%
	where $\delta_t$ is known as the \gls{acr:td} error. $\alpha\in\left(0,1\right)$ defines the learning rate (governing the policy stability), with $\gamma$ defined as in the \gls{acr:mdp} formulation.
\end{subequations}

In essence, at each timestep the policy parameters ($\wvec{}$) are increased along the gradient ($\nabla{\acvalblank}$) using a fixed learning rate ($\alpha$) and a computed adjustment ($\delta_t$).
This adjustment is equal to the difference between the chosen action $a$'s value in state $s$ and the reward received ($r_{t+1} - \acval{s_t}{a_t}{\wvec{}}$), plus some part of the \emph{next} action's value ($\gamma\acval{s_{t+1}}{a_{t+1}}{\wvec{}}$).
Naturally, as state transitions are visited and revisited during training, the value of later choices can propagate down the tree of states, as shown by \cref{fig:sarsa-intuition}.
To see how this combines with a simple linear function approximation, consider \cref{fig:sarsa-tilecode,fig:sarsa-tilecode-update}.

\begin{marginfigure}
	\resizebox{0.9\linewidth}{!}{\input{diagrams/rl/td-learn}}
	\caption[An illustration of how value adjustments in single-step RL propagate backwards through a state trajectory.]{A simplified view of how action values propagate back through visited states: because a state's value includes some portion $\gamma$ of its children's values, at convergence it includes $\gamma^2$ of its grandchildren, and so on. A limitation of the single-step family is that they must visit the same transitions several times for earlier states to learn about their successors.}\label{fig:sarsa-intuition}
\end{marginfigure}

\begin{figure}
	\centering
	\begin{subfigure}{0.6\linewidth}
		\resizebox{\linewidth}{!}{\input{diagrams/rl/tile-coding-worked}}
		\caption{Tile Coding\label{fig:tilecode-code}}
	\end{subfigure}

	\begin{subfigure}{0.6\linewidth}
		\resizebox{\linewidth}{!}{\input{diagrams/rl/tile-coding-acsel}}
		\caption{\centering Value Estimation and Action Selection\label{fig:tilecode-select}}
	\end{subfigure}
	
	\caption[An end-to-end example of Sarsa selecting an action using a tile-coded policy.]{
		An example of tile coding for 2-dimensional state and 4 actions, using 2 tilings, 3 tiles per dimension, and a bias tile.
		All components of $s$ are clamped to $[0,1]$.
		For simplicity, I denote $\symbfit{x}(s,\cdot)$ as a list of indices and represent the values of all actions at each tile with a vector.
		(a) The state $s$ activates the bias tile and exactly one tile in each tiling.
		(b) The action values of active tiles are summed to produce the current value estimate for each action available in $s$---for this state, local knowledge ensures that action 4 is chosen by the greedy policy despite typically being a poor choice elsewhere.
		\label{fig:sarsa-tilecode}
	}
\resizebox{0.6\linewidth}{!}{\input{diagrams/rl/tile-coding-update}}
\caption[An end-to-end example of Sarsa updating the values for a chosen action using a tile-coded policy.]{
	The update step for \cref{fig:sarsa-tilecode}, given an observed \gls{acr:td} error $\delta_t=$ \num{-0.2} (indicating a lower observed reward than the expected long-term value of \num{0.7}) and $\alpha=$ \num{0.5}.
	Action 4's value is thus reduced in the tiles associated with state $s$, but remains the most likely choice; the negative $\delta_t$ may have arisen from noise in the reward signal.
	For illustrative purposes, I choose an unrealistically high $\alpha$ (typically, $\alpha\le$ \num{0.05} is a more reasonable choice).
	\label{fig:sarsa-tilecode-update}
}
\end{figure}

%\begin{figure}
%	\centering
%	\resizebox{0.6\linewidth}{!}{\input{diagrams/rl/tile-coding-update}}
%	\caption[An end-to-end example of Sarsa updating the values for a chosen action using a tile-coded policy.]{
%		The update step for \cref{fig:sarsa-tilecode}, given an observed \gls{acr:td} error $\delta_t=-0.2$ (indicating a lower observed reward than the expected long-term value of \num{0.7}) and $\alpha=0.5$.
%		Action 4's value is thus reduced in the tiles associated with state $s$, but remains the most likely choice; the negative $\delta_t$ may have arisen from noise in the reward signal.
%		For illustrative purposes, I choose an unrealistically high $\alpha$ (typically, $\alpha\le0.05$ is a more reasonable choice).
%		\label{fig:sarsa-tilecode-update}
%	}
%\end{figure}

\subsection{The RL algorithm design space}
While the above introduction to Sarsa is a clear way to intuit the key ideas underpinning \gls{acr:rl}, it is a very specific point within a remarkable design space.
For context, other algorithms may employ separate state-value approximations, use the entirety of an agent's trace, or be tailored to characteristics of the policy approximator (e.g., how neural networks benefit from batching).
Algorithms tend to differ in some key ways:

\begin{description}
	\item[Trace length.] Single-step methods like the above can be generalised to \emph{$n$-step methods} by including further discounted reward measurements (at increased runtime memory cost), as in \emph{A3C}~\parencite{DBLP:conf/icml/MnihBMGLHSK16}. \emph{Monte Carlo methods} such as \emph{REINFORCE}~\parencite{DBLP:journals/ml/Williams92} carry this to its logical extreme, updating every transition in a trace using the known return for an episode. This solves the issue of `repeat visits' hinted at in \cref{fig:sarsa-intuition}, at the cost of storing entire trajectories. Moreover, these can be tricky to delineate into clear episodes in some online tasks. Consider also \emph{eligibility trace} methods such as TD($\lambda$)~\parencite{DBLP:journals/cacm/Tesauro95}, which propagate value backwards through the \gls{acr:mdp} by including some portion $\lambda\in\left[0,1\right]$ of the last timestep's gradient alongside the current (i.e., having some decaying part of \emph{every} prior state's gradient).
	
	\item[The role of values in a policy.] Sarsa (and many other algorithms) follow a \emph{value-based} approach: the role of the function approximator is to compute and learn the value for each action, and then action selection chooses based on these values. This design, however, prevents continuous control (i.e., $a_t\in\mathbb{R}$). The dominating paradigm of late has been \emph{policy gradient methods}, which impose no requirement on the policy's output format---given that a differentiable performance metric in $\wvec{}$ is known. This allows easier expression of many system designs, such as having mixed real-valued and discrete action components. The development of \emph{DPG}~\parencite{DBLP:conf/icml/SilverLHDWR14} has been a key driver in ensuring their suitability for continuous problems. \emph{Actor-critic} algorithms are a considerable subset of this which also learn separate a value estimate for the current state to provide this performance measure. Going further still, \emph{direct policy search} approaches such as those inspired by or using \gls{acr:es} eschew gradient computation to apply randomised modifications directly to the policy.
\end{description}

While the high-level conceptual directions and differences between these algorithms are interesting, we should return to what they imply for online deployment in \gls{acr:pdp} hardware.
Additional trace length means that we must dedicate extra runtime memory \emph{per trace} for either state-action-reward tuples and values---high-speed memory of course being in short supply in this environment.
However, even if we don't cache gradient values themselves\sidenote{This is the case in the \emph{ParSa} algorithm introduced in \cref{chap:in-net-rl}, where recomputing the gradient is faster. We only require one gradient measure per update still---in an $n$-step method, this is the gradient for the state-action pair $n$ steps ago.} the computational cost does not substantially increase beyond including additional discounted value pairs, meaning that there may be an acceptable tradeoff here.
Policy gradient methods like actor-critic algorithms may prove trickier even with discrete actions, as they require additional compute and storage to maintain \emph{two or more} parameter sets which may overlap or be disjoint.
\gls{acr:es} methods~\parencite{DBLP:journals/corr/SalimansHCS17,DBLP:journals/corr/abs-1802-08842} may instead be a comfortable fit for policies with fewer parameters.
By adding a small amount of random noise over $\wvec{}$ and aggregating successful policies, these algorithms are devoid of \emph{any} gradient computation and admit efficient communication schemes.

%---which require fewer noise values to add and track over $\wvec{}$ for all live traces.
%
%
%\glsxtrfull{acr:es} algorithms, while not strictly belonging to the \gls{acr:rl} family, have become relevant in the same \gls{acr:mdp}-like control problems we are interested in.
%Hybrid \gls{acr:rl}-\gls{acr:es} works~\parencite{DBLP:journals/corr/SalimansHCS17} inject Gaussian noise into the policy parameters themselves, and then use the Monte Carlo return values of trajectories to perform a weighted combination of noise updates between distributed actors.
%This can be considered as an approximation of policy gradients, \emph{without} executing any gradient function (and so removing the cost of backpropagation).
%This is primarily justified by systems-level optimisations---deterministic noise generation means that agents need not send full policy updates to one another, merely their final reward values (saving on communications overhead).
%More canonical \gls{acr:es} algorithms~\parencite{DBLP:journals/corr/abs-1802-08842} begin again with distributed actors using perturbed versions of a starting policy, but keep only the top-$k$ policies.
%The mean of their noise vectors is then taken to be the `true' policy update.

%\subsection{Modern RL algorithms}
%\emph{Q-learning}~\parencite{WatkinsThesisQLearning}, similar to Sarsa (though \emph{off-policy}), has had a long and continuing influence on the design of \gls{acr:rl} algorithms.
%\emph{DQN}~\parencite{DBLP:journals/corr/MnihKSGAWR13} introduced \gls{acr:drl} by making the additional modifications required to keep Q-learning stable and feasible using \glspl{acr:dnn}, primarily by including an \emph{experience replay buffer}.
%This buffer contains a history of individual state transitions, and once this is full enough the model is updated using a randomly chosen subset (minibatch) of such transitions at each timestep, drawing influence from \gls{acr:sgd}.
%\emph{DDQN}~\parencite{DBLP:conf/aaai/HasseltGS16} addresses value over-estimation in this family of algorithms by using two parameter sets $\wvec{}$ and $\wvec{}'$, which alternate between the roles of computing action-values for selection and the \gls{acr:td} update $\delta_t$.
%
%%?? Space from direct policy search (ES, Policy gradients) towards value-based \url{https://icml.cc/2015/tutorials/PolicySearch.pdf}
%
%The flexibility of \glspl{acr:dnn} in representing a wide variety of policies, including combining continuous actions with discrete ones, has led to a rich base of work in actor-critic algorithms to exploit this functionality.
%\emph{DDPG}~\parencite{DBLP:journals/corr/LillicrapHPHETS15} acts as a Q-learning-style actor-critic algorithm, building on the theoretical basis of DPG with the inclusion of DQN-style minibatches.
%\emph{A3C}\sidenote{A3C is `asynchronous' in the sense that it is trained from parallel agents, which is different from the below discussion on \emph{environmental} asynchrony. When this is excluded, the algorithm is known as A2C.}~\parencite{DBLP:conf/icml/MnihBMGLHSK16} learns from $n$-step returns (applying the critic value as a baseline to reduce variance\sidenote{This is known as an \emph{advantage function}: a reasonable baseline is typically the state-value function, giving $A_\pi{\left(s,a\right)} = q_\pi{\left(s,a\right)} - v_\pi{\left(s\right)}$.}), but makes the key choice to replace experience replay with parallel agents controlling simulations on the same device (increasing sample diversity across input states).
%\emph{TD3}~\parencite{DBLP:conf/icml/FujimotoHM18} builds on DDPG, but draws on the development of DDQN to attempt to reduce the upward value bias in the critic function by having one actor network and two policy networks: both critics are trained based on the smaller estimate output by either.
%The actor's update steps are also delayed and occur more infrequently, to accelerate its learning using a (slightly) more trained critic.
%\emph{D4PG}~\parencite{DBLP:conf/iclr/Barth-MaronHBDH18} introduces to DDPG a distributional critic~\parencite{DBLP:conf/icml/BellemareDM17} (as opposed to modelling the expected value for a state), improving its performance further by allowing learning to be aware of noise inherent in the value signal due to the environment and critic itself.
%
%Policy gradients methods closer to direct policy search, such as \emph{TRPO}, have equally seen key successes.
%TRPO~\parencite{DBLP:conf/icml/SchulmanLAJM15} recasts the learning problem as a constrained optimisation problem to be solved by \gls{acr:sgd} or another stochastic optimiser.
%The policy parameters are changed to maximise the expected return from Monte Carlo trajectories, given the constraint that the new policy's expected outputs must fall within a Kullback-Leibler error bound (i.e., must not differ too greatly between $\wvec{t}$ and $\wvec{t+1}$).
%\emph{PPO}~\parencite{DBLP:journals/corr/SchulmanWDRK17} remains a Monte Carlo algorithm over independent agent traces but clips the contribution of an action's probability ratio, leading to a simpler implementation and increased performance as this has the side-effect of \emph{implicitly} limiting the size of policy changes.

%\glsxtrfull{acr:es} algorithms, while not strictly belonging to the \gls{acr:rl} family, have become relevant in the same \gls{acr:mdp}-like control problems we are interested in.
%Hybrid \gls{acr:rl}-\gls{acr:es} works~\parencite{DBLP:journals/corr/SalimansHCS17} inject Gaussian noise into the policy parameters themselves, and then use the Monte Carlo return values of trajectories to perform a weighted combination of noise updates between distributed actors.
%This can be considered as an approximation of policy gradients, \emph{without} executing any gradient function (and so removing the cost of backpropagation).
%This is primarily justified by systems-level optimisations---deterministic noise generation means that agents need not send full policy updates to one another, merely their final reward values (saving on communications overhead).
%More canonical \gls{acr:es} algorithms~\parencite{DBLP:journals/corr/abs-1802-08842} begin again with distributed actors using perturbed versions of a starting policy, but keep only the top-$k$ policies.
%The mean of their noise vectors is then taken to be the `true' policy update.

%
%Note -- are actor-critic methods needed for continuous action spaces? Might be worth mentioning whjat it takes to learn both discrete and continuous. Need to use different methods to explore, i.e. Ornstein-Uhlenbeck process.
%
%?? Also want to spend some time discussing various action-selection strategies, that the output can be
%
%?? The reference~\parencite{RL2E}

%?? Can we relate this to the broader ML notion of concept drift?
\subsection{RL use considerations}\label{sec:ddn-rl-design-considerations}
\paragraph{Exploration vs. exploitation in practice}
Consider again the dilemma presented in \cref{fig:toy-rl-motiv}: discovery of an optimal policy relies on either global knowledge, or \emph{exploration} of a suboptimal portion of the state-space.
\gls{acr:rl} agents are typically initialised with either zeroed or random policy parameters, but we cannot expect that for larger state spaces this produces \emph{meaningful} exploration.
It may well be the case that following the current optimal policy up to some uncertain state and then exploring can provided targeted and useful samples, whereas randomised parameters are something more akin to a random walk for all timesteps spent in early episodes.

To encourage exploration in discrete action spaces, we typically introduce some randomness into action selection.
\emph{$\epsilon$-greedy methods} pick another action at random with probability $\epsilon$---typically annealing the value of $\epsilon$ towards zero over some number of timesteps or training episodes.
Meanwhile, the simplest way to achieve this in many \glspl{acr:nn} is to use the outputs of a softmax~\parencite{luce-softmax} layer as a probability distribution over actions, particularly if starting from randomly initialised parameters.
Boltzmann and Max-Boltzmann action selection~\parencite[p. 73]{WieringMThesisRLExploration} constitute variations of these schemes which control the concentration of action probabilities.
Active estimation of the degree of exploration has also attracted healthy degrees of interest (particularly with regard to evolving or non-stationary problems): VDBE~\parencite{DBLP:conf/ki/Tokic10,DBLP:conf/ki/TokicP11} uses changes in the \gls{acr:td} error to control $\epsilon$ over time, while \textcite{DBLP:conf/annpr/TokicP12} train a continuous \gls{acr:nn}-backed agent to control exploration parameters.

In the case of continuous \gls{acr:rl} action spaces, an initial consensus in the wake of the DDPG algorithm~\parencite{DBLP:journals/corr/LillicrapHPHETS15} was to make use of Ornstein-Uhlenbeck processes~\parencite{PhysRev.36.823} to directly inject noise in the action space.
However, more recent ablation studies have shown that this offers no tangible benefits over Gaussian noise~\parencite{DBLP:conf/icml/FujimotoHM18,DBLP:conf/iclr/Barth-MaronHBDH18}.

\paragraph{RL in asynchronous data-driven networking}
Automatic, adaptive control requires accurate, recent state to make optimal decisions and to act in a timely manner.
Action execution, computation and training have real costs, which have been shown to negatively affect the performance of asynchronous \gls{acr:rl} systems~\parencite{DBLP:journals/firai/TravnikMSP18}.
Hence, \gls{acr:ddn} applications are profoundly affected, as they must often reckon with inference times which are significant compared to the systems they control.
As it stands, state measurement and policy execution require additional hardware and infrastructure, increasing delays and costing rack-space in the data centres or networks where we aim to deploy \gls{acr:ddn}.
%Placing learning algorithms, policy execution, and measurement into the network fabric will increase performance, the accuracy of system state and simplify network architectures which use data-driven concepts.

There remains some degree of divergence between the theory and implementation of \gls{acr:rl} agents when it comes to accounting for the above costs.
Consider \cref{fig:state-slip}: the traditional formulation of an \gls{acr:mdp} assumes that an agent receives a new view of the world's state at fixed time intervals, and then decides upon and executes an action instantly.
The reality is that state information takes time to traverse the network, service times are offset by how quickly hosts respond to interrupts and deserialise requests, and action preference lists are often computed via expensive policy approximations.
Action installation also incurs costs in fields such as network administration, initially to contact the controller and then for those actions to be installed via the control plane.

These delays (and variance thereof) add noise to the state-action mapping being learnt, which has a potent reduction to learning rate and final accuracy, even for simple grid world tasks according to \textcite{DBLP:journals/firai/TravnikMSP18}.
They in turn show that reordering algorithmic steps can reduce these costs for online single-step algorithms, but that reducing this further requires detailed agent-environment co-design.
%This principle has influenced the design of real network use cases, such RL-based congestion-control algorithms~\parencite{DBLP:journals/corr/abs-1910-04054}, showing that asynchrony is necessary for high-speed applications.
Achieving this often requires that state measurements are combined or coalesced~\parencite{DBLP:journals/corr/abs-1910-04054,DBLP:journals/tnsm/SimpsonRP20} while expensive computations are ongoing.
For instance, `stopping the world' in the algorithmic sense causes significant performance degradation, as inference takes up to \SI{30}{\milli\second} for \citeauthor{DBLP:journals/corr/abs-1910-04054}, or any time-sensitive control problems.
In the \gls{acr:pdp} case, this can be important for a wide variety of reasons; we might be interested in capturing statistics of a controlled system over longer timescales, or we might need to explicitly rate limit requests at switch-scale or line-rate to prevent an agent (and its parent \gls{acr:nic} or switch) from being overloaded.

\begin{figure}
	\begin{subfigure}{0.45\linewidth}
		\centering
		\resizebox{0.975\linewidth}{!}{\input{diagrams/rl/sa-slip1}}
		\caption{Theory: state measurement, action computation, and learning are zero-cost.}
	\end{subfigure}
	\hspace{0.05\linewidth}
	\begin{subfigure}{0.45\linewidth}
		\centering
		\resizebox{0.75\linewidth}{!}{\input{diagrams/rl/sa-slip2}}
		\caption{Reality: costs of measurement and action lead to \emph{state drift}---over a time delay $t_1+t_2+t_3$, inaction transforms state $S$ into $S'$.}
	\end{subfigure}
	\caption[Illustrating state slippage in an asynchronous RL agent.]{One issue which arises when we aim to introduce \gls{acr:rl} into temporally fine-grained environments is that the \gls{acr:mdp} formulation can fail to capture how state drifts during computation. Input states may take some time to acquire and transmit to an agent over the network ($t_1$), the function approximator itself has an associated computational cost for inference ($t_2$), and enacting said action can involve network latency \emph{and} expensive table preprocessing in the case of hardware P4 implementations ($t_3$). As a result, the system actually undergoes the transition $(S', A)\rightarrow(S'')$, introducing noise or variance into the value function being learnt. \label{fig:state-slip}}
\end{figure}

%\paragraph{Tooling}
%The growth in interest around \gls{acr:rl} research has generated strong development tools for the community.
%OpenAI Gym~\parencite{DBLP:journals/corr/BrockmanCPSSTZ16} presents a consistent middleware for integrating new algorithms and new target problems to be solved.
%Equally, the high sample complexity (coupled with vested interest from hypergiant networks) has led to significant advancements in distributed training---of these, Ray RLLib~\parencite{DBLP:conf/osdi/MoritzNWTLLEYPJ18} offers distributed integration with OpenAI Gym.