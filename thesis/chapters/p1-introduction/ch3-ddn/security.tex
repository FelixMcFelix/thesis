\gls{acr:ml} models, introduce particular security issues in their training, use, and how we expose them or their decisions to users.
Recall that, in general, their operation is \emph{entirely governed by their parameter set $\wvec{}$}, and that we currently face great difficulty in understanding exactly what transformations or logic they encode.
What additional concerns might arise from this?
The most obvious challenge is that an attacker might construct input samples which appear to a human to have one label, but produce a strong response in a \gls{acr:ddn} model for \emph{another label}.
We call such inputs \emph{adversarial examples} or \emph{evasion attacks}.
Changing focus, the idea of online or \emph{active learning}~\parencite{active-learning-report} can seem like a powerful capability to have in the administration of a network for saving operator time.
In introducing this, we now need to ask how an attacker might aim to covertly modify our model's behaviour, either to change the label for a handful of samples (e.g., ensure a malware sample always evades inspection), adjust the entire decision surface (e.g., to incur a \gls{acr:dos} or performance degradation by incorrectly handling \emph{all} flows), or to encode an input pattern which always triggers a given output.
These behaviours fall under the umbrella of \emph{poisoning attacks}.
In tandem, we must also ask whether attackers are able to reverse engineer our model parameters from queries or environmental observation, and the privacy implications of a parameter vector $\wvec{}$ being leaked or extracted---\emph{data extraction} attacks.
These classes of attack, interestingly, mostly mirror those that have historically threatened classifiers in the security domain~\parencite{DBLP:conf/ccs/BarrenoNSJT06}.

%?? valid concerns in is the model behaving as intended? Can an attacker change our model's behaviour? Can sensitive data be extracted from an ML model?

I introduce in this section the techniques and procedures for undertaking these classes of attack, in addition to defences and present suggestions on how and why they work.
%Finally, I ...
It must be stated that the attack and defence surface of \gls{acr:ml} models is very much subject to the same game of cat-and-mouse as any other security domain, i.e. malware or \gls{acr:ddos} design and detection.
This field in particular moves very quickly due to the larger reach and impact of \glspl{acr:dnn} in society as a whole, motivating constant scrutiny by the security community.
As a result, any defences listed are certain to have been invalidated by the time this thesis is read; I hope this section at least provides an illustration of the \emph{classes} of input, output and model transforms that have held some promise.

%?? Discuss attacks on ML models, techniques, paradigms here.

\subsection{Evasion attacks and adversarial examples}\label{sec:evasion-attacks}
\emph{Adversarial examples} are input data which have been subtly modified to trick a machine learning model into producing an incorrect output~\parencite{DBLP:conf/eurosp/PapernotMSW18,DBLP:conf/eurosp/PapernotMJFCS16}.
This problem has been known to security experts for a much longer time under the moniker of \emph{evasion attacks}~\parencite{DBLP:conf/ccs/BarrenoNSJT06}.
The context for these evasions includes cases as simple as spam filter avoidance, and as complex as self-modifying and virtualisation-aware malware~\parencite{DBLP:conf/acsac/CoptyDEEMZ18}.
The term does not purely cover \gls{acr:ml}-based approaches in this context, though there are similarities in the sense that the transformed output must maintain a key property (i.e., it remains a functioning malware payload).

For instance, assume we have in input vector $\mathbf{x}$ with a ground truth label $w$ that the classifier correctly outputs.
An attacker wishes to add some \emph{perturbation} $\symbfit{\delta}$ such that the adversarial example $\mathbf{x}+\symbfit{\delta}$ produces a new output $w'$ from a classifier but still appears to belong to $w$ according to a human observer.
They may require that $w'$ is a specific label, or simply that $w \ne w'$.
These attacks typically assume a white-box attacker (i.e., one who has direct read access to the \gls{acr:ml} model's parameters), who is able to use their knowledge of $\wvec{}$ to compute this $\symbfit{\delta}$.
The data extraction techniques discussed shortly (\cref{sec:data-extraction-and-privacy}) offer more concrete tools for mounting a black-box evasion attack.
Typically, this is then formalised as an optimisation problem in terms of the underlying model, which can be solved via a stochastic optimiser like \emph{Adam}~\parencite{DBLP:journals/corr/KingmaB14}.
To ensure that these alterations are subtle enough to be unnoticeable to a human operator, the constraint to be minimised is some distance metric in $\ell_{\{0,1,2,...,\infty\}}$\sidenote{These specific metrics are the \emph{Hamming} metric $\ell_0$ (number of altered elements in $\mathbf{x}$), \emph{Manhattan} metric $\ell_1$, \emph{Euclidean} metric $\ell_2$, and the \emph{Chebyshev} metric $\ell_\infty$ (the largest change to any element).} between the altered data and its original.
For instance, in a pixel image a bounded $\ell_0$ limits the number of pixels that may be changed, while $\ell_2$ limits the overall strength of noise added.
These adversarial examples typically occur very close to the decision hyperplane; applying too much noise can either accidentally `push' the data into a classification the attacker did not desire, or it may become humanly perceptible.
Since their inception~\parencite{DBLP:journals/corr/SzegedyZSBEGF13}, they have been shown to generalise between models and input vectors~\parencite{DBLP:journals/corr/GoodfellowSS14}.
In the image domain, they have been made transform-resilient~\parencite{DBLP:journals/corr/KurakinGB16}, to transfer to textural information on 3D-printed objects~\parencite{DBLP:journals/corr/AthalyeS17}, and to persist through projection onto surfaces~\parencite{DBLP:journals/corr/abs-2108-06247}.

%?? Indeed, others~\parencite{DBLP:conf/sp/PierazziPCC20} are noticing that most research remains in \emph{feature-space} (i.e., techniques and mathematics), rather than \emph{problem-space}---and most of these are in malware.

A more recent formalisation and strengthening of attacks based on raw input data was recently presented by \textcite{DBLP:conf/sp/Carlini017}.
Around the time of publication, distillation~\parencite{DBLP:conf/sp/PapernotM0JS16} was proposed as a form of hardening for neural networks expected to perform in adversarial settings where evasion attacks might be common.
This work reveals that existing approaches for generating adversarial examples \emph{weren't strong enough} and, accordingly, approaches like defensive distillation are shown to be ineffective.
Some future works refer to the methods they propose as CW-$\ell_{\{0, 2, \infty\}}$ attacks.
Their attacks exceed existing work based on these three well-understood metrics by a more in-depth analysis of the construction of cost functions, a reworked box constraint built around $\tanh(\cdot)$ (as in HDR image tone mapping), and a more nuanced treatment of the effects of discretisation error.
By introducing a \emph{confidence factor} $\kappa$, they are able to explicitly design attacks which are \emph{transferable} between one classifier and its distilled form, or a network derived from the original by black box inference.
%Their work currently establishes the benchmark for future mitigation techniques.

%?? Yeah these are a thing (note: haven't actually read most of these aside from the wonderful turtle-rifle 3d printer one).
%?? The one, the only, the original \parencite{DBLP:journals/corr/SzegedyZSBEGF13}, the application \parencite{DBLP:journals/corr/GoodfellowSS14}.
%?? Second one here suggests that part of the weakness is that models fall back on their heavily linear components -- verify this.

%?? Where are we now? Transform invariant (i.e., photographs \cite{DBLP:journals/corr/KurakinGB16}, 3D model \& 2D transforms \cite{DBLP:journals/corr/AthalyeS17}): what does this mean with regards to the transforms we apply to our input data?

In practice, inputs to \gls{acr:ml} classifiers are often heavily pre-processed or undergo some statistical transformation; either to achieve a fixed-size and compact representation or to increase accuracy.
In this sense we can refer to the `true' inputs as belonging to the \emph{problem space}, while the transformed input given to the \gls{acr:dnn}/\gls{acr:ml} classifier belongs to the \emph{feature space}\sidenote{Image and audio processing are something of an exception to this, where the feature space \emph{is} the problem space. As a result, most adversarial \gls{acr:ml} research targets these domains for simplicity.}.
A malware detector would not, for instance, take an executable's binary as its input, and would instead process behavioural features extracted by static and dynamic analysis tools. 
Of course, these transforms are non-invertible and often non-differentiable, the need to maintain input functionality (in, e.g., malware), and when combined with input validity checks this makes it difficult to create adversarial examples.
A recent frontier on enabling such attacks is a framework for expressing input validity and transformation constraints~\parencite{DBLP:conf/sp/PierazziPCC20}; if feature transformations are approximately differentiable then, otherwise falling back on genetic algorithms and Monte Carlo tree search.

\gls{acr:drl} algorithms are equally vulnerable to this class of attacks, despite the fact that their stochastic nature greatly influences the trajectories gathered during training.
The meaning of an attacker manipulating the environment is, again, problem space dependent, and most work focuses to some extent on reducing agent performance rather than invoking specific actions.
\Textcite{DBLP:journals/corr/HuangPGDA17} have shown that this vulnerability to adversarial inputs extends between \gls{acr:rl} algorithms in white-box settings, while perturbations acquired in a black-box setting on the same \gls{acr:nn} architecture require greater error bounds to invoke the same loss of reward.
An alternative strategy is to directly modify the PPO algorithm, training agents to choose actions with the highest likelihood of making another victim agent perform suboptimally~\parencite{DBLP:conf/uss/Wu0WX21}---i.e., through this adversarial agent's effect on shared environment state via valid actions.

%?? RL algorithms based on NNs are just as vulnerable~\parencite{DBLP:journals/corr/HuangPGDA17}, now~\parencite{DBLP:conf/uss/Wu0WX21}!

%?? Very respectable source \cite{DBLP:conf/eurosp/PapernotMJFCS16} that they're bad I guess? (Not Read)
%
%?? The full summary paper \cite{DBLP:conf/eurosp/PapernotMSW18}.

\paragraph{Defences}
While there is a great deal of churn in what defences will still be considered `valid' at any time, there are concrete guidelines on the evaluation of defences~\parencite{DBLP:journals/corr/abs-1902-06705} to aid in their development (particularly as earlier attempts at defence were not always considered as rigorously as they should be).
For instance, an earlier proposed (though now defeated) defence was defensive distillation~\parencite{DBLP:conf/sp/PapernotM0JS16}.
Ordinarily, distillation~\parencite{DBLP:journals/corr/HintonVD15} takes the softmax probability scores output by a given model, and applies these as the soft labels to be learned by a more compact \gls{acr:nn} architecture.
For early evasion attacks, it sufficed to perform this without changing the target model's structure (selecting a higher softmax temperature) such that smoother decision boundaries would be learned, though this was invalidated by the above CW attacks.

%?? Newest on these?~\parencite{DBLP:journals/corr/abs-1902-06705}. Attempts to offer concrete methodology in response to very variable analysis/testing and slow research on defences. Most defences posited in response to the rapid development of attacks are shown to be incorrectly or incompletely evaluated. ?? Human and machine classification/decision-making performance are tied in efficacy, while there remains a huge gap between sensitivity to adversarial examples. The norm is to assume attackers have white-box access, because this includes black-box defence.

Adversarial training methods have borne some degree of interest.
These approaches integrate generated evasion attacks into the training set in some way; either by explicitly adding generated adversarial examples into the dataset alongside their true labels~\parencite{DBLP:journals/corr/abs-1712-09196}, or by the direct inclusion of the attack generation method in the loss function~\parencite{DBLP:conf/iclr/MadryMSTV18}.
However, they exhibit some vulnerability to universal black-box attacks and as such should not be used as the sole defence~\parencite{DBLP:conf/iclr/TramerKPGBM18}.

%?? \textcite{DBLP:conf/sp/PapernotM0JS16}---distillation---not read, and defeated by CW attacks.

In ensemble classification, if many of the individual classifiers disagree then this can represent a high degree of uncertainty about the observed data.
\textcite{DBLP:conf/ndss/SmutzS16} realise that this can act as a powerful indicator of an evasion attack in progress, and propose \emph{mutual agreement analysis} as a defence on top of the PDFrate (PDF malware) and Drebin (Android executables) malware detection systems.
Both of these platforms had well-established adversarial attacks~\parencite{DBLP:conf/ccs/MaiorcaCG13,DBLP:conf/sp/SrndicL14}, built around the constraint that core exploit functionality must be preserved (i.e., problem-space constraints).
When an insufficient amount of the constituent classifiers return the same result, the result returned is that the sample is `uncertain'---suggesting either a new class of data or evidence of attempted evasion.
The approach naturally suits ensemble methods such as \emph{random forests}, but an extension to \glspl{acr:svm} is proposed.
Moreover, the addition of the `uncertain' classification acts as a useful metric for continuous training and evolution.

Ensembles of \emph{classifications} around one data point, rather than \emph{classifiers} (i.e., without changing the classifier).
Adversarial examples typically occur very close to the decision hyperplane; applying too much noise can either accidentally `push' the data into a classification the attacker did not desire, or it may become humanly perceptible.
This principle is exploited by \textcite{DBLP:conf/acsac/CaoG17}, who propose ensemble classification of potentially adversarial data by sampling from the local hypercube---\emph{region-based} classification, rather than standard \emph{point-based} classification.
%This approach is remarkable because it may rely on any existing classifier $\mathcal{C}$.
This draws from the observation that most of the volume of the surrounding hypercube lies within the true class region, even for adversarial examples, with the size of this noise chosen to minimise accuracy loss.
To learn the true class of an example, we must then choose the class which admits the largest volume of overlap with the sample region: we may approximate this by drawing samples uniformly from this hypercube (e.g., \num{10000} points), and observing the output histogram of labels.
%$r$ is chosen such that classifier performance on benign examples does not degrade below that of $\mathcal{C}$.
Nowadays this ties in heavily to the concept of certified defence~\parencite{DBLP:conf/iclr/RaghunathanSL18} and provable robustness~\parencite{DBLP:conf/iclr/ZhangCXGSLBH20}, which effectively guarantee through training that no perturbation with norm $|\symbfit{\delta}| \le \epsilon$ can alter the output label.
\emph{PixelDP}~\parencite{DBLP:conf/sp/LecuyerAG0J19} connects this notion to differential privacy schemes' formalisation, observing that adding analytically-derived randomness \emph{within} the model can provide certified robustness.
This noise may be injected at a hidden layer (i.e., the extracted latent representation), or may be applied directly to the input by training an auto-encoder to generate a noise vector which would map to bounded-strength noise at the target depth.
Output classifications are then the expectation of \gls{acr:nn} softmax outputs computed over around \num{300} draws (\qty{42}{\times} runtime overhead), with some additional fine-tuning during training to account for noisier inputs.
As noise strength increases, overall robustness improves while clean performance degrades.

%Given that this design is non-differentiable, an attacker cannot compose an adversarial attack directly even if they know $r$, the sample count and $\mathcal{C}$ exactly---they must craft examples against $\mathcal{C}$ (which \emph{is} differentiable) and use these.
%Detection of the standard set of attacks \cite{DBLP:conf/sp/Carlini017} is shown by very convincing results, meaning that attackers must amplify the applied noise, again risking an incorrect target class or very perceptible distortion.
%An interesting area that isn't discussed is how different sampling distributions might affect robustness in the face of even this.

%?? With DiffPriv?~\parencite{DBLP:conf/sp/LecuyerAG0J19}

%?? NN structure analysis~\parencite{DBLP:conf/eurosp/SperlKCLB20}
Recent work suggests that neuron coverage (the pattern of neuron activations throughout an \gls{acr:nn} triggered by an input) exhibits significant differences between benign and adversarial inputs~\parencite{DBLP:conf/eurosp/SperlKCLB20}.
To perform evasion attack detection, they train a model as normal and generate a perturbation from each input to every other label, measuring the neuron activations for every standard and adversarial input.
They then train a fully-connected network to detect adversarial patterns of activation.
This works reasonably well, though in a white-box setting this is still attackable at the cost of more visible input noise.

%\Textcite{DBLP:conf/ndss/SmutzS16} examined an ensemble-based defence on top of the PDFrate (PDF malware) and Drebin (Android executables) malware detection systems.
%Both of these platforms had well-established adversarial attacks~\parencite{DBLP:conf/ccs/MaiorcaCG13,DBLP:conf/sp/SrndicL14}, built around the constraint that core exploit functionality must be preserved.

Fully convolutional neural networks, particularly as applied to images, are vulnerable to adversarial `patches' applied to sub-sections or regions of the input vector.
Robust masking~\parencite{DBLP:conf/uss/0001BSM21} tackles this by reducing the size of \gls{acr:cnn} receptive fields (via ensemble methods or smaller convolution kernels).
This forces adversarially triggered features to contribute larger activations than in the undefended case, making their presence detectable.
However, this adds a non-negligible clean accuracy cost and adds \qtyrange{10}{50}{\percent} execution time (large--small \glspl{acr:nn}).

\Textcite{DBLP:journals/corr/abs-2002-04599} suggest an inherent balancing act between sensitivity and invariance-based attacks---in that defence against one creates a vulnerability against the other.
Sensitivity attacks are what we usually consider in this family (a small change which doesn't impact the input's true label), while invariance attacks use a change which \emph{would} change the true label, but is performed in such a way that the model still outputs the old label.
The defence in question would be against attacks within some $\ell_p$ norm ball (i.e., similar pixel/state similarity)---with the findings suggesting that a `robust' neural network is even more sensitive than an undefended one.

%?? New work~\parencite{DBLP:journals/corr/abs-2002-04599} covering the balancing attack between sensitivity/invariance-based attacks. Sensitivity is what we usually think of (small change which doesn't impact the true label), invariance is a change which \emph{would} change the true label, but is performed in such a way that the model still outputs the old label. Implication: defending against one makes you more sensitive to the other (specifically, attacks within some $\ell_p$ norm ball)---even more sensitive than an undefended network.
%?? Even modern provably robust systems based on $\ell_\infty$~\parencite{DBLP:conf/iclr/ZhangCXGSLBH20} are vuln.

\subsection{Poisoning attacks}
\emph{Poisoning attacks} are undertaken by an attacker who wants to permanently alter the behaviour of a system which is still training in some way.
The key intuition is that an attacker wishes to either choose data points used in training or modify gradient and parameter vectors to affect the model's decision boundaries in some way.
For instance, they might aim to subtly adjust model parameters such that a single (chosen) malware sample escapes detection, or to effectively create a \gls{acr:dos} by reducing the function of decision-making RL agents.
It is crucial to note that this does \emph{not} require white-box access---control over a handful of input samples may be sufficient, as are any collaborative (e.g., \gls{acr:fl}) or online-learning systems like \gls{acr:rl} or active learning~\parencite{active-learning-report}.
Semi-supervised approaches are also vulnerable due to their inclusion of a large unchecked and unlabelled dataset~\parencite{DBLP:conf/uss/Carlini21}.
In this case, an attacker can construct a path in feature space from a labelled point towards a target value (comprising at most \qtyrange{0.1}{1}{\percent} of the dataset).

\emph{Clean-label} poison attacks~\parencite{DBLP:journals/corr/abs-2005-00191} combine the insights of \gls{acr:dnn} adversarial examples to force a classifier to misbehave on a target instance, while all training data appears correct to a human observer.
Perturbed data points are selected, forming a convex hull around the target point in feature space.
This causes the target to be mapped to the same class as the hull, though takes around \qty{3}{\hour} to launch a successful attack.

%?? Backdooring/Trojan~\parencite{DBLP:journals/corr/abs-1712-05526,DBLP:conf/eurosp/TanS20}
\emph{Trojan} and \emph{backdoor} attacks are a stronger and recent variant of model poisoning~\parencite{DBLP:journals/corr/abs-1712-05526}.
They differ from their precursors in that an attacker aims to keep the victim model's performance unchanged for all typical inputs, while an input of their choice maps to their desired output.
An attacker may also have a model learn a `trigger' in the input (i.e., a relative sequence of state values, or a pair of glasses in an image) which immediately produces their desired label if it is present.
These attacks are possible in black-box scenarios with little effort: consider that \citeauthor{DBLP:journals/corr/abs-1712-05526} are able to achieve both classes of attack in \numrange{5}{50} (versus a training set of size \num{600000}) without awareness of either the model or training data.
White-box scenarios can make these learned triggers less obvious to an analyst, by including additional terms in the loss function to penalise cases where their feature activations are easy to discriminate from benign inputs~\parencite{DBLP:conf/eurosp/TanS20}.
Concrete attacks have also been proposed, which target \gls{acr:fl}~\parencite{DBLP:conf/aistats/BagdasaryanVHES20} and \glspl{acr:gnn}~\parencite{DBLP:conf/uss/XiPJ021}.

%In \gls{acr:ddn}...

\paragraph{Defences}
Earlier work on centroid-distance anomaly detection~\parencite{DBLP:journals/jmlr/KloftL10} indicated that online learning systems which assume stationary normality require exponential amount of poison samples with respect to how far the mean must move.
If non-stationarity is modelled via a bounded memory of points then an attacker requires only a linear amount of samples if they control a sufficient fraction of the network throughput.
However, this analysis has limited applicability to modern function approximation which encodes far more complex decision surface behaviour---particularly when bounded memories are not kept.
%Yet in systems which use a finite number of data points (i.e. modelling non-stationarity) an attacker requires only a linear amount of data if they control a sufficient fraction of the network throughput.
%Further findings are more optimistic: if the attacker controls insufficient traffic, then they cannot succeed in appreciably shifting the mean with even an infinite amount of traffic.
%It isn't made clear how these findings relate to more complex systems or models, but it will remain an important consideration. \cite{DBLP:journals/jmlr/KloftL10}

\emph{Auror}~\parencite{DBLP:conf/acsac/ShenTS16} attempts to prevent model poisoning in collaborative model training scenarios, and relies upon the observation that gradient updates submitted by users tend to have a known distribution.
By performing 2-means cluster detection on \emph{indicative features}, users whose updates consistently fall outside of the benign distribution may be detected and blacklisted.

\Textcite{DBLP:conf/sp/WangYSLVZZ19} present a pipeline for detecting, identifying and mitigating backdoors in pre-trained \gls{acr:dnn} models.
They observe that the existence of a trigger (mapping into a target class $t$) makes it easier to adversarially perturb \emph{all other classes} toward $t$ as compared to any other target label.
Since the trigger is shared, it may be reverse-engineered by clustering over all derived perturbation vectors, and thus removed by either model unlearning techniques or input preprocessing.
\emph{Februus}~\parencite{DBLP:conf/acsac/DoanAR20} explicitly pre-processes \gls{acr:nn} inputs, but observes that past work is limited in accuracy loss when removing trojan patches.
In the image domain, the authors apply work on \gls{acr:cnn} interpretability to identify which image regions are responsible for the output class---with trojan regions domineering compared to the rest of the image content.
These pixels are then removed and replaced using an image inpainting algorithm.
Backdoors may also be detected by analysing the \gls{acr:nn} parameters directly.
\Textcite{DBLP:conf/sp/XuWLBGL21} train a mixture of smaller `shadow' models using the same architecture---both clean and trojaned---and train a binary classifier to make this distinction in a white-box setting.
In black-box scenarios, they explore the classification of input-output pairs selected to maximise model information, rather than parameter vectors.

\subsection{Data extraction and privacy}\label{sec:data-extraction-and-privacy}
A key consideration of many \gls{acr:ml} models is that their dense parameter sets encode an accurate, specialised, and \emph{monetarily expensive} logical transform---even if it can't be directly humanly interpreted.
This expense arises through the processes required to collect and label input datasets, as well as the vast compute cost associated with training and hyperparameter optimisation in energy and hardware procurement.
As a result, learned models themselves have high monetary value as intellectual property.
\emph{Model extraction attacks} allow an attacker to either directly steal knowledge of the architecture and parameter set, or to train a functionally equivalent model using input and output pairs (analogous to distillation).
An attacker typically aims to derive a new model with similar or greater accuracy on the same workloads at minimal effort.
Furthermore, they make for an excellent precursor to the above evasion and poisoning attacks as a means to elevate a black-box adversary to a white-box one---in such scenarios it is preferable to pursue model fidelity (i.e., matching correct decisions and mistakes alike).

In general, these attacks are launched through acquiring soft labels (i.e., class probabilities) for a representative set of input points from a victim model~\parencite{DBLP:conf/uss/TramerZJRR16}, and are also applicable to more models than \glspl{acr:nn}.
The presence of soft labels essentially provides strong knowledge about the class and decision boundaries which have been learned by the victim model, simplifying the training of a new cloned model.
\Textcite{DBLP:conf/uss/JagielskiCBKP20} examine the extraction of high-fidelity models in greater depth, finding that this distillation-like approach is somewhat limited by non-determinism in the \gls{acr:sgd} procedure and random initialisation of $\wvec{}$ (i.e., peak \qty{93}{\percent} fidelity).
Their analytic extraction of truly functionally equivalent models based on ReLU behaviour is limited to 2-layer models at this stage.
Exact model theft is most possible when directly monitoring \gls{acr:pcie} bus traffic, i.e., as performed by a compromised or outright adversarial cloud host provider.
Even when running application code on \glspl{acr:tee} to prevent external inspection, access to \glspl{acr:gpu} or \glspl{acr:tpu} still requires communication over the insecure \gls{acr:pcie} bus.\sidenote{While confidential models could in theory be run on the \gls{acr:cpu} using \glspl{acr:tee}, most capable Intel \glspl{acr:cpu} have an enclave memory limit of around \qty{128}{\mebi\byte}. More recent offering are beginning to extend this into $\ge$\qty{8}{\gibi\byte}~\parencite{tee-memlimit}, though this still limits the achievable throughput one can attain versus a dedicated accelerator.}
Although additional \gls{acr:pcie} traffic, re-ordering, and batching complicate this task, by using locally known models as a reference point it is possible to extract the sent compute kernel in spite of proprietary driver behaviour.

%?? Just look at PCIe lol~\parencite{DBLP:conf/uss/ZhuCZL21}

%?? What to do if no notion of data?
The question of which input data should be used to launch such an attack is also worth consideration; extracting a model in fewer samples means that not only can a model be cloned in less time, but in fewer discrete interactions with the victim (lowering the chance of detection).
Assuming a similar (labelled) training set to the target, \textcite{DBLP:conf/uss/HeM0HH21} use mutual information analysis to aid in this process.
They reduce the input dataset to learn what the most valuable points in their own dataset to query from a victim's model are.
If such labels are not known, semi-supervised learning techniques can aid in choosing a pared down set of queries~\parencite{DBLP:conf/uss/JagielskiCBKP20}.
Construction of a viable query dataset with no knowledge of the victim model's training distribution is also possible~\parencite{DBLP:conf/cvpr/TruongMWP21}---generative models may be used to create input samples which maximise disagreement between the clone and victim models.

%?? NOTE check FLASHE paper for more attacks on e.g. FL.

%Furthermore, it should be noted that the \emph{white-box} requirement can be discarded in network-facing or observable models.
%\textcite{DBLP:conf/uss/TramerZJRR16,DBLP:conf/uss/JagielskiCBKP20} have shown that visibility of input-output pairs can allow neural network parameters to be reverse-engineered, and that attacks computed on these surrogates transfer to the target.

%While it is feasible that an attacker could start with a \emph{white-box} understanding of your model to aid in evasion, a more feasible situation is the case where they do not.
%\textcite{DBLP:conf/uss/TramerZJRR16} have shown that attackers can infer many classes of learned models from observation (\emph{model extraction attacks}), allowing evasion attacks, model cloning or discovery of confidential training data characteristics.
%They are able to demonstrate model extraction on logistic regressors, SVMs, MLPs and NNs with and without confidence values.
%Furthermore, from their retrained models they are able to extract images of faces which are ``visually indistinguishable'' from the real training data of a facial recognition classifier.

%?? Put this somewhere: can extract portions of training data w/ carefully crafted inputs~\parencite{DBLP:journals/corr/abs-2012-07805}. Not read, though maybe relate to privacy in general? I.e., differentially private methods.
%
%?? member inference attacks? Find CItes?~\parencite{DBLP:conf/sp/ShokriSSS17}

The opacity of \gls{acr:ml} models has raised the question of how much information they implicitly contain about the training data they are based upon, i.e., whether training data may be extracted or inferred.
\emph{Membership inference} attacks take a given model and input $\mathbf{x}$, and ask whether $\mathbf{x}$ was used in the training of this model---one of the main risks being that sensitive data may be inferred using a speculative query.
Models can be expected to have higher confidence about samples they were trained upon; this can be exploited to train a classifier on input/soft label pairs using smaller `shadow' classifiers~\parencite{DBLP:conf/sp/ShokriSSS17}.
While this was originally thought to arise from overfitting, analysis of language models such as GPT-2 suggests this is not the case~\parencite{DBLP:journals/corr/abs-2012-07805}.
This same analysis is able to propose and generate prompts to extract such memorized data (including personal data), however accuracy of generated `members' is limited to around \qty{33.5}{\percent}.
Collaborative learning settings are vulnerable to membership inference on a per-participant level from their gradient updates~\parencite{DBLP:conf/sp/MelisSCS19}, allowing properties of subsets of classes to be detected according to learners' (non-\gls{acr:iid}) training data.
\emph{Model inversion} attacks focus on extracting information about an input or the space of inputs which would map to a given output class~\parencite{DBLP:conf/ccs/FredriksonJR15}, e.g., extracting the face of an individual from a facial recognition classifier.
These attacks present a real threat in concert with model extraction, for instance \textcite{DBLP:conf/uss/TramerZJRR16} are able to extract images of faces which are ``visually indistinguishable'' from the real training data of a facial recognition classifier.
\emph{Attribute inference} attacks, where an attacker has most of the information of a member and seeks to derive the remainder, requires a stronger adversary who can tell apart true inferred members from nearby data points~\parencite{DBLP:journals/corr/abs-2103-07101}.

%In \gls{acr:ddn}...

\paragraph{Defences}
Given that model extraction attacks are enabled by information sharing, the simplest defences involve limiting the amount of output information given by a classifier.
For instance, rounding confidence values and soft labels or outright removing many output probabilities can offer some degree of defence~\parencite{DBLP:conf/uss/TramerZJRR16,DBLP:conf/uss/ChandrasekaranC20}.
Sampling a model's parameters (potentially as part of an ensemble) may also serve to make the decision boundary harder to learn~\parencite{DBLP:conf/uss/ChandrasekaranC20}.
Attacks designed to maximise query information exhibit characteristic patterns, differing in aggregate information per query versus normal users.
This principle has been applied to detect extraction attacks by monitoring the distribution of information content of input-output pairs~\parencite{DBLP:conf/eurosp/JuutiSMA19,DBLP:conf/acsac/KesarwaniMAM18}.

%Link some diffpriv here?~\parencite{DBLP:conf/ccs/AbadiCGMMT016}, reduces accuracy~\parencite{DBLP:conf/uss/Jayaraman019}
\emph{Differential privacy}~\parencite{DBLP:conf/ccs/AbadiCGMMT016} is a powerful formalism for bounding the amount by which any individual training datum can affect the overall behaviour of the model.
Typically, this is achieved by injecting noise with magnitude below an analytically derived bound to the objective/loss, gradients, or output vectors.
By construction, this prevents model and attribute inference attacks, while limiting the amount of information which can be gleaned in collaborative learning settings.
However, a more secure privacy budget is directly at odds with final model accuracy, and relaxations to the differential privacy formalism invite additional vulnerability to model inference attacks~\parencite{DBLP:conf/uss/Jayaraman019}.
In addition, recent work on model unlearning (by dataset sharding and slicing) can be used to truly remove training members from the model by partial retraining~\parencite{DBLP:conf/sp/BourtouleCCJTZL21}.