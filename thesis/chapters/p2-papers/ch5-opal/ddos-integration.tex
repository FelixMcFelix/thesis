To show the applicability and use of \approachshort{}, we propose an ideal integration which would benefit from in-\gls{acr:nic} \gls{acr:rl}; online \gls{acr:ddos} attack mitigation.
I support this using other state-of-the-art P4/PDP developments, and discuss how best to balance the concerns of online training (\Coopfw{} agents) with throughput (\Indfw{} agents) in widespread deployment.
%Additionally, we discuss how network administrators may combine offline (\Indfw) and online (\Coopfw) agents to achieve online learning while maximising action throughput in the majority of the network.

%?? How do we move this intuition to the start? ELI5 as dimitris says, to get the full end-to-end pipeline across to the reader.

\subsection{In-network DDoS defence}\label{sec:integ-1}
Classical \gls{acr:rl} can enable real-time, adaptive DDoS mitigation, as I discuss through \cref{chap:ddos-rl}.
The \emph{Guarded} agent design uses a mixture of global network state and local, per-flow state to monitor how flows respond to bandwidth changes and packet drop---applying the observations made by SPIFFY~\parencite{DBLP:conf/ndss/KangGS16} (observing how flow behaviour reacts to a change in rate limits) with more allowance for congestion-unaware protocols.
Actions then move flows up or down in punishment levels according to a finite state machine.

\paragraph{Why in-NIC?}
This approach relies on co-hosting traffic measurement and analysis alongside OpenFlow-compatible switches at the edge nodes of an \gls{acr:as}.
However, packet mirroring and moving \gls{acr:rl} computation to a host (potentially over layers of virtualisation) are all sources of additional, consistent state-action latency.
Both traffic statistics collection \emph{and} data-driven learning must be executed on such hosts/network functions.
Unless running these stages in a dedicated pipeline (adding further processing latency), resource contention between these processes will further impact tail latencies.
Naturally, this requires high-performance hardware to be stacked at network egress points, potentially beyond reasonable space, power, or ventilation constraints.
The solution to implement and improve upon this work using \approachshort{} is to place its \gls{acr:rl} agents on SmartNICs at \gls{acr:as} edge nodes---a bump-in-the-wire deployment.

%?? Any way to move some of this sooner? I.e., this is ``why move DDoS prevention here'', Stefanos suggested backporting some of this to justify ``why \emph{RL} in-NIC''?

\paragraph{Inputs}
To collate the required inputs and state, let us examine the recent innovations of the community.
Low-latency, pure-P4 solutions to extract and record per-flow \gls{acr:tcp} state directly in the dataplane such as Dapper~\parencite{DBLP:conf/sosr/GhasemiBR17} and Sonata~\parencite{DBLP:conf/sigcomm/GuptaHCFRW18} are well-studied.
In fact, the statistics offered by Dapper are a super-set of the local input state values employed by \emph{Guarded} agents, offering an opportunity to further improve their efficacy. 
I propose placing such monitors in the P4 dataplane, existing on-chip alongside the \approachshort{} agent.
The required global state (load measures from network paths) must still come from elsewhere in the network; this is now the element at highest risk of becoming stale, but the least likely to vary significantly in response to individual actions.

The \emph{Guarded} design as I present it uses theoretical ``ground truth'' rewards, whose correct implementation and designs were left as an open challenge. 
I posit that INDDoS~\parencite{DBLP:journals/tnsm/DingSPCS21}, which uses Count-min Sketches to estimate \gls{acr:ddos} victim cardinality, could be an effective reward function source---i.e., using the number of detected victims as a loss value.

\paragraph{Integrating \approachshort}
Before each monitoring action, we require that the table hosting this hybrid solution polls \approachshort{}'s \outring{} ring for any generated actions.
As noted in \cref{sec:opal-data-format}, these actions would be placed into a small hash table and simultaneously exported to the attached controller to be inserted as P4 rules in batches.
Afterwards, packet ingress timestamps would be used to emulate the \gls{acr:trs} scheduler used by the anti-\gls{acr:ddos} agents for rate-controlled work, where state vectors would be selected and passed into \approachshort.
By design, active flows which are not \emph{judged} after a configurable time are discarded to prune the work set and allow new flows to be seen by the agent.
The tight bounds on execution time known \emph{a priori} make it easy to calculate the maximum number of decisions which can be made per deadline.
Reward values would then be separately inserted by a modified INDDoS table.

%?? Can I have a P4 code snippet here?

Reducing state-action latency (i.e., with \Coopfw) is useful for minimising the noise inherent in learning.
However, an agent is limited by the fact that it can take at least one \gls{acr:rtt} for meaningful changes to occur in a flow's behaviour ($\mathcal{O}{\left(\si{\milli\second}\right)}$ in a transit \gls{acr:as} or \gls{acr:isp}).
Accordingly, this use-case benefits most from an increase in \emph{throughput} using \Indfw{}.
In this context, higher throughput means that network flows are more likely to be judged in every timestep, even when flow cardinality is high---making it more likely that changes in flow behaviour will be observed, acted on, and learnt from.

Note that the \gls{acr:trs} scheduler is designed to handle large numbers of attack flows, combining seen state vectors over time while the asynchronous agent is itself busy.
By design, a number of (attack) flows beyond the maximum throughput simply makes it take longer in expectation for a flow to be reassessed.
As shown in \cref{sec:opal-evaluation}, \approachshort{} far exceeds the throughput of host offloading.
The control plane can then use wildcards or specific matches to narrow down or expand the set of flows to be controlled dynamically, though explicit (\gls{acr:trs}) scheduling is still key in such adversarial environments.

%\subsection{Something host-specific?}\label{sec:integ-2}
%Accelerate something at an end-host... Flow control? Something datacentre-y? A novel CCA? What?

\subsection{Network deployment considerations}
The two compute models discussed above, \coopfw{} and \indfw{}, needn't be homogeneously deployed in a distributed installation.
In a networked deployment, a subset of \approachshort{} nodes could be \coopfw{} agents, training online, while most other nodes run \indfw{} designs to meet throughput guarantees.
The control plane would then combine, downsample, and distribute these improved policies between offline agents.
This can be taken further still, using policy deltas or execution traces to enable out-of-path transfer learning for more complex models such as \glspl{acr:nn}.