%?? Here is where I would put my ramblings about system architecture...
To demonstrate how we would measure state throughout the network, coalesce it, and act upon flows in an effective way, I present the design of a system which supports the effective real-world deployment of the above \gls{acr:rl} agents.
\Cref{fig:sys-arch} displays this, separating system elements which are local to each agent from those which reside elsewhere in the network.
Each agent here operates as a \gls{acr:vnf} adjacent to a software-defined switch, to which it acts as an extra controller for installing install actions, forwarding rules, and logic into a switch at runtime.
%Moreover, networks of this kind more naturally enable the future relocation and easy installation of learners.
Agent \glspl{acr:vnf} communicate with these co-hosted switches to install probabilistic packet-drop rules.
I describe the main purpose and operation of each module within an agent's \gls{acr:vnf}, and discuss techniques to make deployment more efficient using existing technologies.

\begin{figure}
	\centering
	\resizebox{\linewidth}{!}{\input{diagrams/marl-ddos/sys-arch}}
	\caption[VNF and OpenFlow-based system architecture for an RL-driven DDoS mitigation system.]{
		\gls{acr:vnf} and OpenFlow-based system architecture for an \gls{acr:rl}-driven \gls{acr:ddos} mitigation system.
		Each edge node of the network runs a single agent, using an OpenFlow switch to probabilistically filter traffic, with a co-located host machine used to perform \gls{acr:rl} inference and learning.
		Estimators using, e.g., \gls{acr:dots} information to estimate legitimate traffic communicate with these edge switches, which also share \gls{acr:rl} trace data with one another to learn collaboratively.
		Most switches in the network need only to report their load statistics to a shared controller.
		As this is a common capability, this architecture can serve a network of mostly legacy switches.
		\label{fig:sys-arch}
	}
\end{figure}

\subsection{Core and RL executor}\label{sec:core-and-rl-executor}
The core module is the main loop in an agent's architecture.
At each timestep, the core receives information about which flows have arrived and should be acted upon from the \emph{\gls{acr:trs} Scheduler}.
The core then retrieves the current and previous state vector associated with each flow from the \emph{Flowstate Database}, passing them into the \gls{acr:rl} algorithm alongside the last action chosen for that flow (if available).

The \gls{acr:rl} algorithm then infers and returns an action.
Each action is passed to the database, which computes and returns a packet drop rate according to the agent model (\emph{Instant/Guarded}) while updating flow state.
This is then converted into an OpenFlow message carrying packet drop rules; these are batched to the agent's switch using the same groupings produced by the scheduler.
Finally, timing information is passed back into the scheduler to refine its estimates about how much work should be scheduled in the next timestep.

State space sizes guarantee that an \emph{Instant} policy remains under \qty{520}{\kibi\byte}, though the sparse representation admitted by tile-coding typically leads to far smaller policies of around \qty{17.8}{\kibi\byte} from my experiments.
\emph{Guarded} policies are \qty{30}{\percent} of this size.
As described earlier, action updates require a constant number of floating point operations, and the vast majority of these operations can be vectorised.
Action computation for \emph{Guarded} agents is cheaper still, on account of having fewer actions.

\subsection{Stats API and collectors}
Agents require information from the network and one another to be effective.
These agents can act either independently, having no agent-to-agent communication, or cooperatively.
In the latter case agents transfer, when possible, \emph{experience} to one another---lists of state-action-state transitions with associated rewards.
It's noted that a transition may be high-value or surprising to one agent, while well-known to another, causing each to produce different policy updates from the same unit of experience.
For this reason I do not transfer policy deltas between agents, causing each to learn its own policy.
Determining which scheme achieves better performance is, however, left to future work.

%?? How do stats get from switcehs to the agent?
Load collectors and estimators periodically push observations to each active agent \gls{acr:vnf}.
In the current implementation, load statistics are gathered via \glspl{acr:vnf} active at each network switch, though we expect that OpenFlow stats requests, NetFlow or SNMP data may be used to derive these cheaply.
Transferring state to agents and experience sharing can both be made more efficient through effective use of broadcast addressing in a target network.
Depending on the capabilities of switches in the network, the estimator can either send benign traffic estimates or parameters for use in a reward function.

Gathering and transmission of load/flow statistics would be difficult to perform quite as often as an emulated environment allows.
However, the measurements acquired in such a scenario are likely to be less noisy (by being collected over longer periods of time), which could aid effective training.

\subsection{Flowstate database}
For each flow 5-tuple, we hold two state vectors containing the features described in \cref{sec:feature-space}---the current state, and the state which induced the last action.
To ensure that flow control actions are made with recent information, we combine state vectors for unvisited flows in the current work set.
State vector combination is done by coalescing and combining state vectors as described in \cref{sec:systems-considerations}.
For flows outside of the current work list, we simply replace or insert the new state vector.

%\subsection{TRS Scheduler}
%Acting on an unbounded set of flows in each timestep introduces potential issues: the inability to respond to unexpected changes in flow state, delayed service of new flows, and the risk that flow states become outdated.
%At their worst, these risks present additional attack surface to an adversary.
%To tackle these problems, we make use of \emph{Timed Random Sequential} updates.
%
%The scheduler begins with a shuffled work list of active flows.
%When requested, the scheduler estimates the cost of an action computation using the timing information received from the core, proceeding down the list to send a set of 5-tuples to the core which can be handled in a set time limit.
%The scheduler continues until the list is empty, at which point it is repopulated and reshuffled with active flows.
%
%Following \citeauthor{DBLP:conf/sigcomm/ChenL0L18}'s observations on handling short flows \cite{DBLP:conf/sigcomm/ChenL0L18}, we maintain a deadline of \SI{1}{\milli\second}---an agent is typically able to process around 3 flows in this time.
%We expect deadlines should be tuned based on the frequency at which statistics arrive.
%The amount of processed flows per deadline depends on the agent design (FLOP count, policy size), but also on the amount of flow telemetry data needing processed---our current implementation is written in python, restricting this handling to a single thread.
%An implementation in a systems language such as Rust or C++ would allow faster concurrent processing.
%
%There is a risk that so much work can be queued up that an agent is never able to act on some attack flows.
%A solution is to impose an upper bound on the amount of action computations/policy updates that can be performed before the work list is regenerated.
%This removes the guarantee that all flows will be visited often, but if updates occur regularly then this sampling may be sufficient to achieve good performance.

\subsection{Agent switches}
%?? How do we do stat collection?
Agent switches operate a modified version of \gls{acr:ovs}, implementing an action which requests that each matched packet be dropped with a certain probability.
Ideally, this host-based solution would be replaced with a bespoke \gls{acr:asic} running OpenFlow with this extension.
To get around the lack of floating-point support in many environments, such as \gls{acr:pdp} environments or \gls{acr:ovs}'s kernel datapath, I represent this probability using a 32-bit unsigned integer (scaling \num{1.0} to $2^{32}-1$).
On commodity hardware without explicit packet drop support, I believe that a similar effect can be achieved using OpenFlow meters (at the expense of these being stateful measures).

OpenFlow groups are used to simplify control messages: premade tables with permitted levels of packet drop.
This saves some overhead compared to using experimenter/extension headers.
Flows are automatically given a group with the default level of packet drop (according to the chosen agent design), meaning that switches don't need to refer to a controller or the agent \gls{acr:vnf}.

%?? What do our modifications to openflow look like? How do we get around the lack of floating point math?