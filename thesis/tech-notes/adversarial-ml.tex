% !TeX document-id = {29d93272-0c70-47c0-a2c9-f98f7fffc7db}
% !TeX program = xelatex
% !BIB program = biber

\documentclass[11pt,a4paper]{article}
%\documentclass[11pt,paper=b5,footinclude,headinclude]{scrbook}
%\usepackage{classicthesis}

% ^^ draft shows overfilled boxes.
% ^^ replace "oneside" with "twoside" to set the gutter correctly for
%    two-sided printing.
% ^^ add "nogutter" option for digital copy (without binding offsets),
%    if printed copy is twoside, use [twoside,nogutter] for digital copy.

\usepackage[T1]{fontenc}

\usepackage[UKenglish]{babel}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{url}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{mcaption}
\usepackage[font=scriptsize]{subfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bookmark}
\usepackage[acronym, toc]{glossaries}
\usepackage[notbib,nottoc,chapter]{tocbibind}
\usepackage{blindtext}
\usepackage[binary-units, per-mode=symbol]{siunitx}

%\usepackage{Alegreya}
\usepackage{AlegreyaSans}
%\usepackage[sfdefault]{AlegreyaSans}

%\usepackage{tgpagella}
\usepackage{libertine}
%\usepackage{FiraSans}

\usepackage{color}
\usepackage{changebar}
\usepackage{soul}

\hyphenation{Internet analysis analyse}

\newcommand\TLSins[1]{
  \cbstart{}
  {
    \color{blue}
    \ul{#1}
  }
  \cbend{}
}

\newcommand\TLSdel[1]{
  \cbdelete{}
  {
    \color{red}
    \st{#1}
  }
}
\usepackage{fancyhdr}                    % Fancy Header and Footer
\pagestyle{fancy}                       % Sets fancy header and footer
\fancyfoot{}                            % Delete current footer settings
\renewcommand{\headrulewidth}{0.2pt}
\fancyhead[LE,RO]{\bfseries\thepage}    % Page number (boldface): left-even, right-odd
\fancyhead[RE]{\sffamily\nouppercase{\leftmark}}      % Chapter in the right on even pages
\fancyhead[LO]{\sffamily\nouppercase{\rightmark}}     % Section in the left on odd pages


\setlength{\headheight}{15pt}        % adjust for fancyhdr warning
% \pagestyle{fancy}                    % clear all header and footer fields
% \fancyhf{}
% \fancyhead[L]{\slshape \rightmark}   % put section heading left
% \fancyhead[R]{\thepage}              % put page number right

% % redefine "plain" to fix page numbering (on first page of chapters)
\fancypagestyle{plain} { %
   \fancyhf{}                           % clear all header and footer fields
   \fancyhead[L]{\slshape \rightmark}  % no section heading for these pages
   \fancyhead[R]{\bfseries\thepage}
   \renewcommand{\headrulewidth}{0pt}   % no headrule for these pages
   \renewcommand{\footrulewidth}{0pt}
}


\newenvironment{codelisting}
{\begin{list}{}{\setlength{\leftmargin}{1em}}\item\scriptsize\bfseries}
{\end{list}}

\newcommand{\note}[1]{\emph{\textbf{Note:}[#1]}}

\usepackage[sortcites
%,style=numeric-comp
,style=apa
%,style=authoryear
%,style=ext-authoryear-comp
%,style=ext-alphabetic
,maxcitenames=2
,maxbibnames=99
,backend=biber
]{biblatex}
\addbibresource{../bibliography/ml/papers.bib}
\addbibresource{../bibliography/ml/confs.bib}
\addbibresource{../bibliography/ml/misc.bib}
\addbibresource{../bibliography/ml/books.bib}
\addbibresource{../bibliography/network/papers.bib}
\addbibresource{../bibliography/network/confs.bib}
\addbibresource{../bibliography/network/misc.bib}
\addbibresource{../bibliography/network/rfcs.bib}
\DefineBibliographyStrings{english}{%
	andothers = {\emph{et al}\adddot}
}

\newcommand{\mytitle}{Technical Note: Adversarial ML}
\newcommand{\myname}{Kyle A. Simpson}

\hypersetup{
%	colorlinks,
	%draft,
%	citecolor=black,
%	filecolor=black,
%	linkcolor=black,
%	urlcolor=black,
	pdftitle={\mytitle{}},
	pdfauthor={\myname{}}
}

% =============================================================================
\begin{document}

\title{\mytitle{}}
\author{\myname{}}
%\date{December 2021}

\maketitle

\noindent
\emph{Adversarial machine learning} is a family of techniques and approaches which aim to trick a machine learning model to produce an incorrect output for a label~\parencite{DBLP:conf/eurosp/PapernotMSW18}.
These attacks typically assume a white-box attacker (i.e., one who has direct read access to the ML model), who is able to use their knowledge of the ML model to subtly alter an input to produce this mislabelling.
Typically, an attack is formalised as an optimisation problem in terms of the underlying model (which can be solved) via a stochastic optimiser like \emph{Adam}~\parencite{DBLP:journals/corr/KingmaB14}.
The constraint to be minimised is some distance metric in $\ell_{\{0,1,2,...,\infty\}}$ between the altered data and its original.

An attacker's goal, however, may be either to induce \emph{any} incorrect label or a specific one; where the former can usually be achieved with smaller change to the input.
Typically, these alterations aim to be so subtle as to be unnoticeable to a human operator.
These adversarial examples typically occur very close to the decision hyperplane; applying too much noise can either accidentally `push' the data into a classification the attacker did not desire, or it may become humanly perceptible.

In reality, this problem has been known to security experts for a much longer time under the moniker of \emph{evasion attacks}~\parencite{DBLP:conf/ccs/BarrenoNSJT06}.
The context for these evasions includes cases as simple as spam filter avoidance, and as complex as self-modifying and virtualisation-aware malware~\parencite{DBLP:conf/acsac/CoptyDEEMZ18}.
The term does not purely cover ML-based approaches in this context.

\paragraph{Defence}
Defences against these techniques are regularly proposed~\parencite{DBLP:conf/sp/PapernotM0JS16,DBLP:conf/ndss/SmutzS16,DBLP:conf/acsac/CaoG17,DBLP:conf/iclr/ZhangCXGSLBH20}, then defeated in short order~\parencite{DBLP:conf/sp/Carlini017,DBLP:journals/corr/abs-2002-04599}.
Broadly speaking, these have included ensembles of classifiers, ensembles of examples (e.g., by adding noise to inputs), and distilling vulnerable models into smaller representations.
The manner in which the model is used or learns doesn't offer any defence, as \emph{reinforcement learning} models remain vulnerable~\parencite{DBLP:journals/corr/HuangPGDA17}.
Most of this research concerns neural network-based approaches, but historical works have examined classical ML under the same lens.

Recent work~\parencite{DBLP:journals/corr/abs-2002-04599} suggests an inherent balancing act between sensitivity/invariance-based attacks---in that defence against one creates a vulnerability against the other.
Sensitivity attacks are what we usually consider in this family (a small change which doesn't impact the input's true label), while invariance attacks use a change which \emph{would} change the true label, but is performed in such a way that the model still outputs the old label.
The defence in question would be against attacks within some $\ell_p$ norm ball (i.e., similar pixel/state similarity)---with the findings suggesting that a `robust' neural network is even more sensitive than an undefended one.

\paragraph{Examples in networks}
\Textcite{DBLP:conf/ndss/SmutzS16} examined an ensemble-based defence on top of the PDFrate (PDF malware) and Drebin (Android executables) malware detection systems.
Both of these platforms had well-established adversarial attacks~\parencite{DBLP:conf/ccs/MaiorcaCG13,DBLP:conf/sp/SrndicL14}, built around the constraint that core exploit functionality must be preserved.

Furthermore, it should be noted that the \emph{white-box} requirement can be discarded in network-facing or observable models.
\textcite{DBLP:conf/uss/TramerZJRR16,DBLP:conf/uss/JagielskiCBKP20} have shown that visibility of input-output pairs can allow neural network parameters to be reverse-engineered, and that attacks computed on these surrogates transfer to the target.

I examined some recent work~\parencite{DBLP:journals/corr/abs-2004-11898} which claims to use \emph{genetic algorithms}, \emph{Monte Carlo methods}, and \emph{generative adversarial networks} to help samples evade a NIDS.
Sadly, it seems to perturb the output of other feature extractors, which isn't particularly interesting.
I list it here purely for completeness.

At this point, I'm having trouble finding anything targetting data-driven network techniques--I feel like there should be a rich baseline here?
For instance, you'd think it might be possible to trick ML-driven AQM to punish legitimate flows or similar (i.e., novel DDoS rather than just ML evasion).
Indeed, others~\parencite{DBLP:conf/sp/PierazziPCC20} are noticing that most research remains in \emph{feature-space} (i.e., techniques and mathematics), rather than \emph{problem-space}---and most of these are in malware.

\printbibliography[title={References}]

\end{document}

